{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import model.train_schedule\n",
    "import model.model_defination\n",
    "import torch\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "from typing import Union,List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# img = Image.open(path).convert('L')\n",
    "# img = np.array(img, dtype=np.float32)\n",
    "# print('图片大小：', img.shape)\n",
    "# print(img)\n",
    "# img = np.expand_dims(img, 0)\n",
    "\n",
    "\n",
    "def loadSingleImg(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    载入单张图片，形状为NCHW\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert('L')\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    # img = torch.from_numpy(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def loadImages(path_to_folder: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    载入给定文件夹中的所有图片，形状为NCHW\n",
    "\n",
    "    返回np.ndarray\n",
    "    \"\"\"\n",
    "    pics = os.listdir(path_to_folder)\n",
    "    # 首先载入第一张图片\n",
    "    imgs = loadSingleImg(os.path.join(path_to_folder, pics[0]))\n",
    "    for i in range(1, len(pics)):\n",
    "        img = loadSingleImg(os.path.join(path_to_folder, pics[i]))\n",
    "        imgs = np.concatenate((imgs, img), axis=0)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def arrayToPic(array: np.ndarray) -> Image.Image:\n",
    "    \"\"\"将数组转换为图片\"\"\"\n",
    "    return Image.fromarray(255 * array.astype('int')).convert('L')\n",
    "\n",
    "\n",
    "def drawImageArray(img: np.ndarray):\n",
    "    for i in range(img.shape[0]):\n",
    "        plt.figure(figsize=(7.07, 7.07))\n",
    "        plt.imshow(img[i], cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "def savefig(array: np.ndarray, path: str, preffix='pic'):\n",
    "    \"\"\"保存数组为图片\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        需要保存的数组，形状为NHW\n",
    "    path : str\n",
    "        路径\n",
    "    \"\"\"\n",
    "    for i in range(array.shape[0]):\n",
    "        Image.fromarray(array[i].astype('int')).convert('L').save(\n",
    "            os.path.join(path, preffix + '{}.png'.format(i)))\n",
    "\n",
    "\n",
    "def drawImageArrays(*arrays):\n",
    "    column = len(arrays)\n",
    "    row = arrays[0].shape[0]\n",
    "    for array in arrays:\n",
    "        if row != array.shape[0]:\n",
    "            raise ValueError(\n",
    "                'Shape of all array expected to be the same.Expected {} got {}'\n",
    "                .format(row, array.shape[0]))\n",
    "        if array.ndim != 3:\n",
    "            raise ValueError(\n",
    "                'Array dimension expected to be 3 , got {}'.format(array.ndim))\n",
    "    plt.figure(figsize=(3.6 * column, 4 * row))\n",
    "    for i in range(row):\n",
    "        for j in range(column):\n",
    "            plt.subplot(row, column, i * column + j + 1)\n",
    "            plt.title(str(i), fontsize=10, color='white')\n",
    "            plt.imshow(arrays[j][i], cmap='gray')\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 预处理步骤\n",
    "首先，0.5级的LASCO C2数据文件会处理为1级数据。所有的(1024,1024)分辨率大小的图片会被下采样至(512,512)。然后，会经过噪声滤波以压制部分尖锐噪声。文中使用3*3正则化正方滤波器.该滤波器是基本的线性滤波器，计算邻近像素的平均值，然后再生成差分图。\n",
    "##### 图像分类\n",
    "所有的(1024,1024)差分图会被下采样(112,112)大小，作为神经网络的输入。经过卷积层1，输出为(20,108,108)，经过池化层1，输出为(20,54,54)，经过卷积层2，输出为(50,50,50)，经过池化层2，输出为(50,25,25).然后经过两全连接层，得到最终的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ho(h, f, padding, stride):\n",
    "    return (h - f + 2 * padding) / stride + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文中：\n",
      "108.0\n",
      "54.0\n",
      "50.0\n",
      "25.0\n",
      "我的工作：\n",
      "220.0\n",
      "110.0\n",
      "106.0\n",
      "53.0\n"
     ]
    }
   ],
   "source": [
    "# 原文中:input N*112*112\n",
    "print('原文中：')\n",
    "print(ho(112, 5, 0, 1)) # 卷积后N*108*108\n",
    "print(ho(108, 2, 0, 2)) # 池化后N*54*54\n",
    "print(ho(54, 5, 0, 1)) # 卷积后N*50*50\n",
    "print(ho(50, 2, 0, 2)) # 池化后N*25*25\n",
    "\n",
    "#我的工作中采用input为 N*224*224\n",
    "print('我的工作：')\n",
    "print(ho(224, 5, 0, 1))  # 卷积后N*220*220\n",
    "print(ho(220, 2, 0, 2))  # 池化后N*110*110\n",
    "print(ho(110, 5, 0, 1))  # 卷积后N*106*106\n",
    "print(ho(106, 2, 0, 2))  # 池化后N*53*53\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### CME探测\n",
    "首先利用register_forward_hook函数得到卷积层输出的activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# net = model.model_defination.LeNet5()\n",
    "# img_path = r'D:\\Programming\\CME_data\\CME\\Halo\\20130830_032405_lasc2rdf_aia193rdf.png'\n",
    "# parameter_path = r'D:\\Programming\\codetest\\CMEclassfication\\log\\2022_04_13_20_00_25\\parameters.pkl'\n",
    "# map_location = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# net.load_state_dict(torch.load(parameter_path,map_location=map_location))\n",
    "\n",
    "def getActivation(net: torch.nn.Module, x:Union[torch.Tensor,np.ndarray]) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    获得最后一层卷积层的输出\n",
    "    \n",
    "    输出结果的维度为N*h*w*d\n",
    "    \"\"\"\n",
    "    if isinstance(x,np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    net.eval()\n",
    "    activation = []\n",
    "\n",
    "    def forward_hook(modeul, data_input, data_output):\n",
    "        activation.append(data_output.detach().permute(0, 2, 3, 1).numpy())\n",
    "\n",
    "    net.conv2.register_forward_hook(forward_hook)\n",
    "    out = net(x)\n",
    "    return activation[0]\n",
    "\n",
    "\n",
    "# img = loadSingleImg(img_path)\n",
    "# activat = getActivation(net, img)\n",
    "# out = net(torch.from_numpy(img))\n",
    "# print('act:', activat.shape)\n",
    "# print('out:', torch.argmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### CME Co-localiaztion\n",
    "从卷积层的输出特征图中提取信息。利用DDT手段进行图像共定位(Co-locolization)。共定位就是在一系列的图像中找到相关物体的位置。对于一张H*W大小的图片，其activation(卷积层的输出)就是一个形状为h*w*d的三维张量。该张量可被认为有h*w个cell，每个cell包含一个d维的DD(deep desciptor)向量。  \n",
    "首先，有N张图片构成的序列，这N个图片包含着同一类别的目标。这N张图片生成N个activation，每一个activation都是k*w*d维的张量。计算这N个activation的平均张量，然后获得协方差矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getMeanVector(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    获得所有descriptor的平均向量\n",
    "    \n",
    "    x为N*h*w*d维的np.array\n",
    "    \"\"\"\n",
    "    return np.mean(x, axis=(0, 1, 2))\n",
    "\n",
    "\n",
    "def cov(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    获得协方差矩阵\n",
    "    \n",
    "    x为N*h*w*d维的np.ndarray\n",
    "    \"\"\"\n",
    "    k = x.shape[0] * x.shape[1] * x.shape[2]\n",
    "    xMeanVector = getMeanVector(x)\n",
    "    convMat = np.zeros(x.shape[3])\n",
    "    for n in range(x.shape[0]):\n",
    "        for i in range(x.shape[1]):\n",
    "            for j in range(x.shape[2]):\n",
    "                deviaVector = x[n][i][j] - xMeanVector\n",
    "                # 对x中取出的descripter向量进行升维\n",
    "                # 因为直接取出的descripter向量是一维的，直接相乘会出现问题，需转化为列向量\n",
    "                deviaVector = np.expand_dims(deviaVector, axis=1)\n",
    "                tempMat = np.matmul(deviaVector, deviaVector.T)\n",
    "                convMat = convMat + tempMat\n",
    "    return convMat / k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "求最大特征值所对应的特征向量以及指示矩阵。  \n",
    "原特征图的形状为(h,w,d)，可认为原特征图的每一个像素都构成了一个观测样本，d维的descripter向量就是该观测样本的值，再利用PCA的方法，求出该协方差矩阵的特征值和特征向量，并投影到最大特征值对应的特征向量方向上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getPrinCompVector(activation: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    获得主成分向量\n",
    "    \n",
    "    activation的形状为N*h*w*d，是卷积层输出的特征图\n",
    "    \"\"\"\n",
    "    covMatrix = cov(activation)\n",
    "    eigValue, eigVector = np.linalg.eig(covMatrix)\n",
    "    prinCompInd = np.argmax(eigValue)\n",
    "    prinCompVector = eigVector[:, prinCompInd]\n",
    "    # prinComp形状为(50,)，对其增加一维变为列向量\n",
    "    prinCompVector = np.expand_dims(prinCompVector, axis=1)\n",
    "    return prinCompVector\n",
    "\n",
    "\n",
    "def getIndicatorMatrix(activation: np.ndarray, ind: int,\n",
    "                       prinCompVector: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"获得索引为ind的图片的activation所对应的Indicator Matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : np.ndarray\n",
    "        维度为N*h*w*d,是N张图片的激活层构成的数组\n",
    "    ind : int\n",
    "        表示需求出指示矩阵的图片对应的索引\n",
    "    prinCompVector : np.ndarray\n",
    "        最大特征值对应的主成分向量\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray \n",
    "        指示矩阵\n",
    "    \"\"\"\n",
    "    img = activation[ind]\n",
    "    xMeanVector = getMeanVector(activation)\n",
    "    indicatorMatrix = np.zeros((activation.shape[1], activation.shape[2]))\n",
    "    for i in range(activation.shape[1]):\n",
    "        for j in range(activation.shape[2]):\n",
    "            indicatorMatrix[i, j] = np.matmul(prinCompVector.T,\n",
    "                                              img[i, j] - xMeanVector)\n",
    "    return indicatorMatrix\n",
    "\n",
    "\n",
    "# prinCompVector = getPrinCompVector(activat)\n",
    "# indicatorMat = getIndicatorMatrix(activat,0, prinCompVector)\n",
    "# print(np.where(indicatorMat > 0, 1, 0))\n",
    "# print(indicatorMat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "指示矩阵的正值反映了d维descriptor的相关性大小。其值越大表明相关性越大。主成分向量是由N张图片获得的，因此正的相关性就反映了N张图片的共同特征。因此可以用0值作为阈值，大于0表示共同的物体，小于0表示背景或者不经常出现的物体。\n",
    "将指示矩阵利用最近邻插值，变为原图片的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reSize(x: np.ndarray, targetSize=(512, 512)) -> np.ndarray:\n",
    "    \"\"\"利用最近邻插值，更改矩阵大小\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        需要变换的矩阵\n",
    "    targetSize : tuple, optional\n",
    "        目标大小, by default (512, 512)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        修改大小后的矩阵\n",
    "    \"\"\"\n",
    "    pointXCoord = np.floor(np.linspace(0, targetSize[0] - 1, x.shape[0]))\n",
    "    pointYCoord = np.floor(np.linspace(0, targetSize[1] - 1, x.shape[1]))\n",
    "    pointCoord = np.array([(i, j) for i in pointXCoord for j in pointYCoord])\n",
    "    X = np.arange(0, targetSize[0])\n",
    "    Y = np.arange(0, targetSize[1])\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    reSizedX = griddata(pointCoord, x.flatten(), (X, Y), method='nearest')\n",
    "    # 此处返回的数组应为插值后的数组的转置，原因在于meshgrid生成的X,Y数组的顺序不同\n",
    "    reSizedX = reSizedX.T\n",
    "    return reSizedX\n",
    "\n",
    "\n",
    "# reSizedIndicator = reSize(indicatorMat)\n",
    "# reSizedIndicator.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "将大小改变后的指示矩阵二值化，大于0的值修改为1，小于0的值修改为0.然后利用flood-fill算法找到正值区域的最大连通分量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getNextStartPoint(mask):\n",
    "    '''查找下一个未被标记的点的坐标，若有这样的点则返回一个这样的点的坐标，若无则返回None'''\n",
    "    ind = np.argwhere(mask == 0)\n",
    "    if ind.size > 0:\n",
    "        return tuple(ind[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def isInside(point: tuple, xBound: tuple, yBound: tuple) -> bool:\n",
    "    '''\n",
    "    判断点point是否在界限内\n",
    "    \n",
    "    xBound和yBound均为二元组，分别为x和y坐标的上下界。\n",
    "    '''\n",
    "    if xBound[0] <= point[0] <= xBound[1] and yBound[0] <= point[1] <= yBound[\n",
    "            1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def getConnectedComponet(reSizedIndicator: np.ndarray) -> tuple:\n",
    "    \"\"\"找到指示矩阵中的最大连通分量\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reSizedIndicator : np.ndarray\n",
    "        指示矩阵 形状为h*w\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : np.ndarray\n",
    "        连通分量的遮罩,为h*w二维数组,mask中大于0的值表示连通分量,不同的大于0的值表示不同的连通分量。\n",
    "    componentIndex : int\n",
    "        表示连通分量的个数\n",
    "    \"\"\"\n",
    "    if reSizedIndicator.ndim != 2:\n",
    "        raise ValueError('Dimensions of input array must be 2.Got {}'.format(\n",
    "            reSizedIndicator.ndim))\n",
    "    binaryIndicatorMat = np.where(reSizedIndicator > 0, 1, 0)\n",
    "    # mask用于指示reSizedIndicator中同位置的点是否被标记\n",
    "    # 若某点为0，表示还未被搜索到，若为-1，表示此点不在搜索区域内，若为正数，则用以区分不同的连通分量\n",
    "    mask = np.zeros_like(reSizedIndicator)\n",
    "    # binaryIndicatorMat中为0的点不属于搜索范围，需要在fill中将相应的点标为-1\n",
    "    mask[binaryIndicatorMat == 0] = -1\n",
    "    # 指定flood-fill算法的起始点坐标\n",
    "    # mask中起始点所对应的位置的值必须为0\n",
    "    filled = set()\n",
    "    #s = (0, 3)\n",
    "    s = getNextStartPoint(mask)\n",
    "    if s is None:\n",
    "        print('没有可供选择的起始点，mask中所有点都被标记了')\n",
    "    assert mask[s[0]][s[1]] == 0, '起始点不满足要求，请重新选择flood-fill算法起始点'\n",
    "    fill = set()\n",
    "    fill.add(s)\n",
    "    height, width = reSizedIndicator.shape[0] - 1, reSizedIndicator.shape[1] - 1\n",
    "    # componentIndex用于指示不同的连接分量，由1开始依次累加1\n",
    "    componentIndex = 1\n",
    "    while fill:\n",
    "        r, c = fill.pop()\n",
    "        # 去掉以下判断并在向fill中添加上下左右点时增加对界限的判断是因为\n",
    "        # 当(r,c)位于边界处，且此时fill为空时\n",
    "        # 由于continue的存在，会跳过寻找下一个起始点，直接结束循环，导致有连通分量被漏掉\n",
    "        # if c > width or r > height or r < 0 or c < 0:\n",
    "        #     continue\n",
    "        if mask[r][c] == 0:\n",
    "            #print(r,c,':',componentIndex)\n",
    "            mask[r][c] = componentIndex\n",
    "            filled.add((r, c))\n",
    "            leftUp = (r - 1, c - 1)\n",
    "            left = (r, c - 1)\n",
    "            leftDown = (r + 1, c - 1)\n",
    "            up = (r - 1, c)\n",
    "            down = (r + 1, c)\n",
    "            rightUp = (r - 1, c + 1)\n",
    "            right = (r, c + 1)\n",
    "            rightDown = (r + 1, c + 1)\n",
    "            if leftUp not in filled and isInside(leftUp, (0, height),\n",
    "                                                 (0, width)):\n",
    "                fill.add(leftUp)\n",
    "            if left not in filled and isInside(left, (0, height), (0, width)):\n",
    "                fill.add(left)\n",
    "            if leftDown not in filled and isInside(leftDown, (0, height),\n",
    "                                                   (0, width)):\n",
    "                fill.add(leftDown)\n",
    "            if up not in filled and isInside(up, (0, height), (0, width)):\n",
    "                fill.add(up)\n",
    "            if down not in filled and isInside(down, (0, height), (0, width)):\n",
    "                fill.add(down)\n",
    "            if rightUp not in filled and isInside(rightUp, (0, height),\n",
    "                                                  (0, width)):\n",
    "                fill.add(rightUp)\n",
    "            if right not in filled and isInside(right, (0, height),\n",
    "                                                (0, width)):\n",
    "                fill.add(right)\n",
    "            if rightDown not in filled and isInside(rightDown, (0, height),\n",
    "                                                    (0, width)):\n",
    "                fill.add(rightDown)\n",
    "        # print(fill)\n",
    "        # 若fill中此时没有别的点了，标明上下左右邻近范围内的点都已被搜索完，则已经完成一个连通分量的搜索\n",
    "        # 需要进行下一个连通分量的搜索\n",
    "        if not fill:\n",
    "            nextPoint = getNextStartPoint(mask)\n",
    "            #print('next:',nextPoint)\n",
    "            if nextPoint:\n",
    "                fill.add(nextPoint)\n",
    "                componentIndex = componentIndex + 1\n",
    "    return mask, componentIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# mask, componetIndex = getConnectedComponet(reSizedIndicator)\n",
    "\n",
    "\n",
    "def getLargestConnectedComponent(mask, componetIndex) -> np.ndarray:\n",
    "    \"\"\"获取最大连通分量\"\"\"\n",
    "    largestComponent = np.zeros_like(mask)\n",
    "    # *此处生成的componentNumlist序列\n",
    "    # *由于range(1,componetIndex)不包含componetIndex\n",
    "    # *会缺少mask中元素值等于compoentIndex的点的数量\n",
    "    # *将range(1,componetIndex)改为range(1,componetIndex+1)\n",
    "    componetNumlist = [\n",
    "        np.argwhere(mask == i).shape[0] for i in range(1, componetIndex + 1)\n",
    "    ]\n",
    "    largestComponetIndex = np.argmax(componetNumlist) + 1\n",
    "    largestComponent[mask == largestComponetIndex] = 1\n",
    "    return largestComponent\n",
    "\n",
    "\n",
    "# largestComp = getLargestConnectedComponent(mask, componetIndex)\n",
    "# plt.imshow(largestComp, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### DDT算法\n",
    "总结以上步骤，DDT算法流程为\n",
    "- 利用loadImages获取N张图片，形状为(N,C,H,W)\n",
    "- 利用getActivation获取这N张图片的激活图，形状为(N,H,W,D)。其中第n张图片的激活图即是一个高度为h，宽度为w的D维向量，称为deep desciptor\n",
    "- 利用getMeanVector获取所有N张图片的所有descriptors的平均向量\n",
    "- 利用cov获取这些descriptors的均方差矩阵\n",
    "- 利用getPrinCompVector计算均方差矩阵的最大成分向量，并作为主投影方向\n",
    "- 对于每一张图片\n",
    "    - 利用getIndicatorMatrix获取某一张图对应的指示矩阵。指示矩阵中的点是将对应图片的对应像素点的D维deep descriptor向量减去平均向量，再投影到最大成分向量的方向上。\n",
    "    - 利用reSize将指示矩阵的大小调整到原图片的大小(512,512)，得到新的指示矩阵\n",
    "    - 利用getConnectedComponet和getLargestConnectedComponent获取新指示矩阵的最大连接分量\n",
    "    - 返回每一张图片的最大连接分量形成的遮罩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def DDT(imgs: np.ndarray, net: torch.nn.Module):\n",
    "    \"\"\"利用DDT算法，获取图片的最大连接分量\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : np.ndarray\n",
    "        N张图片构成的数组，形状为NCHW\n",
    "    net : torch.nn.Module\n",
    "        使用的CNN网络\n",
    "    modelParaPath : str\n",
    "        模型参数的路径\n",
    "    \"\"\"\n",
    "    activation = getActivation(net, imgs)\n",
    "    prinCompVector = getPrinCompVector(activation)\n",
    "    # largestComps = []\n",
    "    largestComps = np.zeros((imgs.shape[0], imgs.shape[2], imgs.shape[3]),\n",
    "                            dtype=np.int8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        print('Processing pic {}/{}'.format(i + 1, imgs.shape[0]))\n",
    "        indicatorMat = getIndicatorMatrix(activation, i, prinCompVector)\n",
    "        reSizedIndicator = reSize(indicatorMat)\n",
    "        mask, componetIndex = getConnectedComponet(reSizedIndicator)\n",
    "        largestComp = getLargestConnectedComponent(mask, componetIndex)\n",
    "        # largestComps.append(largestComp)\n",
    "        largestComps[i] = largestComp\n",
    "    # largestComps = np.array(largestComps)\n",
    "    return largestComps\n",
    "\n",
    "\n",
    "def drawLargestComp(imgs: np.ndarray, largestComps: np.ndarray):\n",
    "    \"\"\"将图片与最大连接分量进行绘制\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : np.ndarray\n",
    "        N张图片构成的数组，形状为NCHW\n",
    "    largestComps : np.ndarray\n",
    "        N张图片的最大连接分量\n",
    "    \"\"\"\n",
    "    # !图片显示时，为0的像素点显示为黑色，为1的像素点显示为白色，\n",
    "    # !所以如果直接绘图的话，提取到的最大连通向量会显示为白色，无关的背景会显示为黑色\n",
    "    plt.figure(figsize=(3.6 * 2, 4 * imgs.shape[0]))\n",
    "    for i in range(imgs.shape[0]):\n",
    "        plt.subplot(imgs.shape[0], 2, 2 * i + 1)\n",
    "        plt.imshow(imgs[i, 0], cmap='gray')\n",
    "        # plt.suptitle(str(i))\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.subplot(imgs.shape[0], 2, 2 * i + 2)\n",
    "        # plt.suptitle(str(i))\n",
    "        # ?// 1-largestComps[i]使得黑白反转，这样最大连通分量就显示为黑色\n",
    "        plt.imshow(largestComps[i], cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 算法测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第一组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Programming\\\\codetest\\\\CMEclassfication\\\\log\\\\2022_04_13_20_00_25\\\\parameters.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m net \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mmodel_defination\u001B[38;5;241m.\u001B[39mLeNet5()\n\u001B[0;32m      2\u001B[0m parameter_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mProgramming\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcodetest\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mCMEclassfication\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mlog\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m2022_04_13_20_00_25\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mparameters.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_param\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameter_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mI:\\code\\CMEclassfication\\model\\model_defination.py:103\u001B[0m, in \u001B[0;36mLeNet5.load_param\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_param\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_path):\n\u001B[0;32m    101\u001B[0m     map_location \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\n\u001B[0;32m    102\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'D:\\\\Programming\\\\codetest\\\\CMEclassfication\\\\log\\\\2022_04_13_20_00_25\\\\parameters.pkl'"
     ]
    }
   ],
   "source": [
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = r'D:\\Programming\\codetest\\CMEclassfication\\log\\2022_04_13_20_00_25\\parameters.pkl'\n",
    "net.load_param(parameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  第一组验证\n",
    "imgarray = loadImages(r'D:\\Programming\\CME_data\\CME\\Halo')\n",
    "positiveArray = imgarray[net.predict(imgarray) == 1]\n",
    "largestComps = DDT(positiveArray, net)\n",
    "drawLargestComp(positiveArray, largestComps)\n",
    "# net.predict(imgarray)\n",
    "# plt.imshow(largestComps[3])\n",
    "# testmask,testindex=getConnectedComponet(largestComps[3])\n",
    "# largestComps[3].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第二组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  第二组测试\n",
    "imgarray2 = loadImages(r'D:\\Programming\\CME_data\\coLocValidation')\n",
    "positiveArray2=imgarray2[net.predict(imgarray2)==1]\n",
    "net.predict(imgarray2)\n",
    "# label = ['No CME', 'CME']\n",
    "# pred = [label[i] for i in net(torch.from_numpy((valiarray))).argmax(dim=1)]\n",
    "# plt.figure(figsize=(8 * 2, 4 * valiarray.shape[0]))\n",
    "# for i in range(valiarray.shape[0]):\n",
    "#     plt.subplot(valiarray.shape[0], 1, i + 1)\n",
    "#     plt.imshow(valiarray[i, 0], cmap='gray')\n",
    "#     plt.title(pred[i], fontsize=10, backgroundcolor='white')\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largeComps2=DDT(positiveArray2, net)\n",
    "# !迷惑的结果\n",
    "drawLargestComp(positiveArray2, largeComps2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第三组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 第三组测试\n",
    "imgarray3 = loadImages(r'D:\\Programming\\CME_data\\coLocaValidation2')\n",
    "positiveArray3 = imgarray3[net.predict(imgarray3) == 1]\n",
    "net.predict(imgarray3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largeComps3 = DDT(positiveArray3, net)\n",
    "drawLargestComp(positiveArray3, largeComps3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(positiveArray3, 1-largeComps3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stackArray3=np.concatenate((positiveArray3.squeeze(),255*largeComps3),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(stackArray3.shape[0]):\n",
    "    plt.figure(figsize=(7.07,14.14))\n",
    "    plt.imshow(stackArray3[i], cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.savefig(r'D:\\Programming\\CME_data\\coLocalResult\\pic{}.png'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(stackArray3.shape[0]):\n",
    "    arrayToPic(stackArray3[i]).save(r'D:\\Programming\\CME_data\\coLocalResult\\pic{}.png'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 去除中间日冕仪图像的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model.load_data import CenterCrop\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = r'I:\\code\\CMEclassfication\\log\\2022_04_13_20_00_25\\parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = r'I:\\code\\CMEclassfication\\log\\2022_06_22_17_59_11\\parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "crop = CenterCrop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第一组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.TiffTags' has no attribute 'LONG8'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m imgs \u001B[38;5;241m=\u001B[39m \u001B[43mloadImages\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mI:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcode\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mCMEclassfication\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mCME_data\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcoLocaValidation2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m cropImageArray \u001B[38;5;241m=\u001B[39m crop(imgs)\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mloadImages\u001B[1;34m(path_to_folder)\u001B[0m\n\u001B[0;32m     26\u001B[0m pics \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(path_to_folder)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 首先载入第一张图片\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m imgs \u001B[38;5;241m=\u001B[39m \u001B[43mloadSingleImg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_to_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpics\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(pics)):\n\u001B[0;32m     30\u001B[0m     img \u001B[38;5;241m=\u001B[39m loadSingleImg(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(path_to_folder, pics[i]))\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mloadSingleImg\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloadSingleImg\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m    载入单张图片，形状为NCHW\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     13\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     14\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(img, \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\PIL\\Image.py:2964\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   2962\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2963\u001B[0m     ndmax \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[1;32m-> 2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m>\u001B[39m ndmax:\n\u001B[0;32m   2965\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToo many dimensions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndmax\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2967\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m shape[\u001B[38;5;241m1\u001B[39m], shape[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\PIL\\Image.py:334\u001B[0m, in \u001B[0;36mpreinit\u001B[1;34m()\u001B[0m\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JpegImagePlugin\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m JpegImagePlugin\n\u001B[1;32m--> 334\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\PIL\\JpegImagePlugin.py:44\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtempfile\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image, ImageFile, TiffImagePlugin\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_binary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m i16be \u001B[38;5;28;01mas\u001B[39;00m i16\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_binary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m i32be \u001B[38;5;28;01mas\u001B[39;00m i32\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\PIL\\TiffImagePlugin.py:427\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    423\u001B[0m     __floor__ \u001B[38;5;241m=\u001B[39m _delegate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__floor__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    424\u001B[0m     \u001B[38;5;21m__round__\u001B[39m \u001B[38;5;241m=\u001B[39m _delegate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__round__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 427\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mImageFileDirectory_v2\u001B[39;00m(MutableMapping):\n\u001B[0;32m    428\u001B[0m     \u001B[38;5;124;03m\"\"\"This class represents a TIFF tag directory.  To speed things up, we\u001B[39;00m\n\u001B[0;32m    429\u001B[0m \u001B[38;5;124;03m    don't decode tags unless they're asked for.\u001B[39;00m\n\u001B[0;32m    430\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m \n\u001B[0;32m    486\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    488\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, ifh\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mII\u001B[39m\u001B[38;5;130;01m\\052\u001B[39;00m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, group\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
      "File \u001B[1;32mI:\\Anaconda\\lib\\site-packages\\PIL\\TiffImagePlugin.py:708\u001B[0m, in \u001B[0;36mImageFileDirectory_v2\u001B[1;34m()\u001B[0m\n\u001B[0;32m    686\u001B[0m     _load_dispatch[idx] \u001B[38;5;241m=\u001B[39m (  \u001B[38;5;66;03m# noqa: F821\u001B[39;00m\n\u001B[0;32m    687\u001B[0m         size,\n\u001B[0;32m    688\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;28mself\u001B[39m, data, legacy_api\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m: (\n\u001B[0;32m    689\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_unpack(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfmt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, data)\n\u001B[0;32m    690\u001B[0m         ),\n\u001B[0;32m    691\u001B[0m     )\n\u001B[0;32m    692\u001B[0m     _write_dispatch[idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mvalues: (  \u001B[38;5;66;03m# noqa: F821\u001B[39;00m\n\u001B[0;32m    693\u001B[0m         \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pack(fmt, value) \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m values)\n\u001B[0;32m    694\u001B[0m     )\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m    697\u001B[0m     \u001B[38;5;28mmap\u001B[39m(\n\u001B[0;32m    698\u001B[0m         _register_basic,\n\u001B[0;32m    699\u001B[0m         [\n\u001B[0;32m    700\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mSHORT, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshort\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    701\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mLONG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    702\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mSIGNED_BYTE, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msigned byte\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    703\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mSIGNED_SHORT, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msigned short\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    704\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mSIGNED_LONG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msigned long\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    705\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mFLOAT, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    706\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mDOUBLE, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    707\u001B[0m             (TiffTags\u001B[38;5;241m.\u001B[39mIFD, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m--> 708\u001B[0m             (\u001B[43mTiffTags\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLONG8\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQ\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlong8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    709\u001B[0m         ],\n\u001B[0;32m    710\u001B[0m     )\n\u001B[0;32m    711\u001B[0m )\n\u001B[0;32m    713\u001B[0m \u001B[38;5;129m@_register_loader\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Basic type, except for the legacy API.\u001B[39;00m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_byte\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, legacy_api\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    715\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'PIL.TiffTags' has no attribute 'LONG8'"
     ]
    }
   ],
   "source": [
    "imgs = loadImages(r'I:\\code\\CMEclassfication\\CME_data\\coLocaValidation2')\n",
    "cropImageArray = crop(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 没有去除日冕仪图像中间的结果\n",
    "largecomp = DDT(imgs,net)\n",
    "croplargecomp = DDT(cropImageArray,cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#去除日冕仪中间的结果\n",
    "drawImageArrays(imgs.squeeze(), largecomp, cropImageArray.squeeze(),\n",
    "                croplargecomp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "savefig((np.concatenate((imgs.squeeze(), 255 * largecomp), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult','previous')\n",
    "savefig((np.concatenate((cropImageArray.squeeze(), 255 * croplargecomp), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult', 'crop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第二组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs4 = loadImages(r'D:\\Programming\\CME_data\\coLocValidation3')\n",
    "cropimgs4 = crop(imgs4)\n",
    "posimgs4 = imgs4[net.predict(imgs4)==1]\n",
    "poscropimgs4 = cropimgs4[cropnet.predict(cropimgs4)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largecomp4=DDT(posimgs4,net)\n",
    "croplargecomp4=DDT(poscropimgs4,cropnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(posimgs4, largecomp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(poscropimgs4,croplargecomp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "savefig((np.concatenate((posimgs4.squeeze(), 255 * largecomp4), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult3','previous')\n",
    "savefig((np.concatenate((poscropimgs4.squeeze(), 255 * croplargecomp4), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult3', 'crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第三组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs5 = loadImages(r'D:\\Programming\\CME_data\\coLocValidation4')\n",
    "cropimgs5 = crop(imgs5)\n",
    "posimgs5 = imgs5[net.predict(imgs5)==1]\n",
    "poscropimgs5 = cropimgs5[cropnet.predict(cropimgs5)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largecomp5=DDT(posimgs5,net)\n",
    "croplargecomp5=DDT(poscropimgs5,cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawImageArrays(poscropimgs5.squeeze(), croplargecomp5.squeeze(), posimgs5.squeeze(), largecomp5.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "savefig((np.concatenate((posimgs5.squeeze(), 255 * largecomp5), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult2','previous')\n",
    "savefig((np.concatenate((poscropimgs5.squeeze(), 255 * croplargecomp5), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult2', 'crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第四组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs6 = loadImages(r'D:\\Programming\\CME_data\\coLocValidation5')\n",
    "cropimgs6 = crop(imgs6)\n",
    "posimgs6 = imgs6[net.predict(imgs6)==1]\n",
    "poscropimgs6 = cropimgs6[cropnet.predict(cropimgs6)==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cropnet.predict(cropimgs6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largecomp6=DDT(posimgs6,net)\n",
    "croplargecomp6=DDT(poscropimgs6,cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(posimgs6,largecomp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(poscropimgs6, croplargecomp6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "savefig((np.concatenate((posimgs6.squeeze(), 255 * largecomp6), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult4','previous')\n",
    "savefig((np.concatenate((poscropimgs6.squeeze(), 255 * croplargecomp6), axis=2)),\n",
    "        r'D:\\Programming\\CME_data\\cropresult4', 'crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "针对迷惑结果的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cropimgs2=crop(imgarray2)\n",
    "poscropimgs2=cropimgs2[cropnet.predict(cropimgs2)==1]\n",
    "croplargecomp2=DDT(poscropimgs2,cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawLargestComp(poscropimgs2,croplargecomp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "翻转连接分量\n",
    "由于最大连接向量有时为背景，有时为CME。对于最大连接向量（白色部分）为背景的图片，尝试进行黑白反转，并重新提取最大连接向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getReverseLargeComp(largecomp: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"对最大连接向量进行黑白反转，并重新提取最大连接向量\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    largecomp : np.ndarray\n",
    "        原本的最大连接向量，形状为NHW\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        翻转并重新提取的最大连接向量，形状为NHW\n",
    "    \"\"\"\n",
    "\n",
    "    reverseLargeComp = 1 - largecomp\n",
    "    reverseLargeCompArray = np.zeros_like(reverseLargeComp)\n",
    "    for i in range(reverseLargeComp.shape[0]):\n",
    "        mask, componentIndex = getConnectedComponet(reverseLargeComp[i])\n",
    "        reverlargecomp = getLargestConnectedComponent(mask, componentIndex)\n",
    "        reverseLargeCompArray[i] = reverlargecomp\n",
    "    return reverseLargeCompArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reverseLargeComp=getReverseLargeComp(croplargecomp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawImageArrays(poscropimgs6.squeeze(), 255 * croplargecomp6,\n",
    "                255 * reverseLargeComp)\n",
    "savefig(np.concatenate(\n",
    "    (poscropimgs6.squeeze(), 255 * croplargecomp6, 255 * reverseLargeComp),\n",
    "    axis=2),\n",
    "        path=r'D:\\Programming\\CME_data\\cropresult4',\n",
    "        preffix='reverse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04e8fab6f2841560ebde8d0311a5471ee06f181872fffccc0e8c49576f944811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}