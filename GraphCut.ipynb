{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import model.train_schedule\n",
    "import model.model_defination\n",
    "import model\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normoalizeArray(array: np.ndarray, newmin, newmax) -> np.ndarray:\n",
    "    '''\n",
    "    将array中的最大值变为max，最小值变为min，其他的数值按照平均原则计算。\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:需要改变的的数组\n",
    "    newmin:最小值\n",
    "    newmax:最大值\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    值改变后的数组\n",
    "    '''\n",
    "    originMax = array.max()\n",
    "    originMin = array.min()\n",
    "    difference = originMax - originMin\n",
    "    newArray = (array - originMin) / difference * (newmax - newmin) + newmin\n",
    "    return newArray\n",
    "\n",
    "\n",
    "def arrayToImg(arr: np.ndarray) -> np.ndarray:\n",
    "    '''将数组的值改为[0,255]范围内\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        被改变的数组\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        修改后的数组\n",
    "    '''\n",
    "    return normoalizeArray(arr.astype('np.uint8'), 0, 255)\n",
    "\n",
    "\n",
    "def saveImg(arr: np.ndarray, filename: str):\n",
    "    '''保存图片\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        被保存的数组\n",
    "    filename : str\n",
    "        图片的文件名\n",
    "    '''\n",
    "    Image.fromarray(arrayToImg(arr)).save(filename)\n",
    "\n",
    "\n",
    "def showImg(arr: np.ndarray):\n",
    "    '''将数组作为图片展示\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        需要展示的数组\n",
    "    '''\n",
    "    Image.fromarray(arrayToImg(arr)).show()\n",
    "\n",
    "\n",
    "def grayImageToRGB(arr: np.ndarray) -> np.ndarray:\n",
    "    '''将灰度图像的复制三份，成为RGB图像\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        灰度图像，形状为HW或者NHW\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        RGB三通道图像，形状为HWC或者NHWC\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        图像的维度必须为2，否则会引发ValueError\n",
    "    '''\n",
    "    if arr.ndim == 3:\n",
    "        return np.concatenate((np.expand_dims(arr, 3), np.expand_dims(\n",
    "            arr, 3), np.expand_dims(arr, 3)),\n",
    "                              axis=3)\n",
    "    elif arr.ndim == 2:\n",
    "        return np.concatenate((np.expand_dims(arr, 2), np.expand_dims(\n",
    "            arr, 2), np.expand_dims(arr, 2)),\n",
    "                              axis=2)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Dimensions of input array must be 3(NHW) or 2(HW),got {}'.format(\n",
    "                arr.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互式GraphCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现交互式的Graph Cut算法\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124723416\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "drawing = False\n",
    "mode = False\n",
    "\n",
    "\n",
    "class GraphCutXupt:\n",
    "    def __init__(self, t_img):\n",
    "        self.img = t_img\n",
    "        self.img_raw = img.copy()\n",
    "        self.img_width = img.shape[0]\n",
    "        self.img_height = img.shape[1]\n",
    "        self.scale_size = 640 * self.img_width // self.img_height\n",
    "        if self.img_width > 640:\n",
    "            self.img = cv2.resize(self.img, (640, self.scale_size),\n",
    "                                  interpolation=cv2.INTER_AREA)\n",
    "        self.img_show = self.img.copy()\n",
    "        self.img_gc = self.img.copy()\n",
    "        self.img_gc = cv2.GaussianBlur(self.img_gc, (3, 3), 0)\n",
    "        self.lb_up = False\n",
    "        self.rb_up = False\n",
    "        self.lb_down = False\n",
    "        self.rb_down = False\n",
    "        self.mask = np.full(self.img.shape[:2], 2, dtype=np.uint8)\n",
    "        self.firt_choose = True\n",
    "\n",
    "\n",
    "# 鼠标的回调函数\n",
    "def mouse_event2(event, x, y, flags, param):\n",
    "    global drawing, last_point, start_point\n",
    "    # 左键按下：开始画图\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        last_point = (x, y)\n",
    "        start_point = last_point\n",
    "        param.lb_down = True\n",
    "        print('mouse lb down')\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        drawing = True\n",
    "        last_point = (x, y)\n",
    "        start_point = last_point\n",
    "        param.rb_down = True\n",
    "        print('mouse rb down')\n",
    "    # 鼠标移动，画图\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            if param.lb_down:\n",
    "                cv2.line(param.img_show, last_point, (x, y), (0, 0, 255), 2,\n",
    "                         -1)\n",
    "                cv2.rectangle(param.mask, last_point, (x, y), 1, -1, 4)\n",
    "            else:\n",
    "                cv2.line(param.img_show, last_point, (x, y), (255, 0, 0), 2,\n",
    "                         -1)\n",
    "                cv2.rectangle(param.mask, last_point, (x, y), 0, -1, 4)\n",
    "            last_point = (x, y)\n",
    "    # 左键释放：结束画图\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        param.lb_up = True\n",
    "        param.lb_down = False\n",
    "        cv2.line(param.img_show, last_point, (x, y), (0, 0, 255), 2, -1)\n",
    "        if param.firt_choose:\n",
    "            param.firt_choose = False\n",
    "        cv2.rectangle(param.mask, last_point, (x, y), 1, -1, 4)\n",
    "        print('mouse lb up')\n",
    "    elif event == cv2.EVENT_RBUTTONUP:\n",
    "        drawing = False\n",
    "        param.rb_up = True\n",
    "        param.rb_down = False\n",
    "        cv2.line(param.img_show, last_point, (x, y), (255, 0, 0), 2, -1)\n",
    "        if param.firt_choose:\n",
    "            param.firt_choose = False\n",
    "            param.mask = np.full(param.img.shape[:2], 3, dtype=np.uint8)\n",
    "        cv2.rectangle(param.mask, last_point, (x, y), 0, -1, 4)\n",
    "        print('mouse rb up')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = cv2.imread(\n",
    "        \"MiddleData/original/20130820_092405_lasc2rdf_aia193rdf.png\",\n",
    "        flags=1)  # 读取彩色图像(Youcans)\n",
    "    g_img = GraphCutXupt(img)\n",
    "\n",
    "    cv2.namedWindow('image')\n",
    "    # 定义鼠标的回调函数\n",
    "    cv2.setMouseCallback('image', mouse_event2, g_img)\n",
    "    while (True):\n",
    "        cv2.imshow('image', g_img.img_show)\n",
    "        if g_img.lb_up or g_img.rb_up:\n",
    "            g_img.lb_up = False\n",
    "            g_img.rb_up = False\n",
    "            bgdModel = np.zeros((1, 65), np.float64)\n",
    "            fgdModel = np.zeros((1, 65), np.float64)\n",
    "            rect = (1, 1, g_img.img.shape[1], g_img.img.shape[0])\n",
    "            print(g_img.mask)\n",
    "            mask = g_img.mask\n",
    "            g_img.img_gc = g_img.img.copy()\n",
    "            cv2.grabCut(g_img.img_gc, mask, rect, bgdModel, fgdModel, 5,\n",
    "                        cv2.GC_INIT_WITH_MASK)\n",
    "            mask2 = np.where((mask == 2) | (mask == 0), 0,\n",
    "                             1).astype('uint8')  # 0和2做背景\n",
    "            g_img.img_gc = g_img.img_gc * mask2[:, :,\n",
    "                                                np.newaxis]  # 使用蒙板来获取前景区域\n",
    "            cv2.imshow('youcans', g_img.img_gc)\n",
    "        # 按下ESC键退出\n",
    "        if cv2.waitKey(20) == 27:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.subplot(221), plt.axis('off'), plt.title(\"xupt\")\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "    plt.subplot(222), plt.axis('off'), plt.title(\"mask\")\n",
    "    plt.imshow(mask, 'gray')\n",
    "    plt.subplot(223), plt.axis('off'), plt.title(\"mask2\")\n",
    "    plt.imshow(mask2, 'gray')\n",
    "    plt.subplot(224), plt.axis('off'), plt.title(\"Grab Cut\")\n",
    "    plt.imshow(cv2.cvtColor(g_img.img_gc, cv2.COLOR_BGR2RGB))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用前景选框的GrabCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现GrabCut算法(前景选框)\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124744467\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"/home/lin/Pictures/R-C.jpg\", flags=1)  # 读取彩色图像(BGR)\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "# 定义矩形框，框选目标前景\n",
    "# rect = (118, 125, 220, 245)  # 直接设置矩形的位置参数，也可以鼠标框选 ROI\n",
    "print(\"Select a ROI and then press SPACE or ENTER button!\\n\")\n",
    "roi = cv2.selectROI(image, showCrosshair=True, fromCenter=False)\n",
    "xmin, ymin, w, h = roi  # 矩形裁剪区域 (ymin:ymin+h, xmin:xmin+w) 的位置参数\n",
    "rect = (xmin, ymin, w, h)  # 边界框矩形的坐标和尺寸\n",
    "imgROI = np.zeros_like(image)  # 创建与 image 相同形状的黑色图像\n",
    "imgROI[ymin:ymin + h, xmin:xmin + w] = image[ymin:ymin + h,\n",
    "                                             xmin:xmin + w].copy()\n",
    "print(xmin, ymin, w, h)\n",
    "\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(image,\n",
    "                                       mask,\n",
    "                                       rect,\n",
    "                                       bgModel,\n",
    "                                       fgModel,\n",
    "                                       iter,\n",
    "                                       mode=cv2.GC_INIT_WITH_RECT)  # 框选前景分割模式\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(231), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(232), plt.axis('off'), plt.title(\"Bounding box\")\n",
    "plt.imshow(cv2.cvtColor(imgROI, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(233), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite background(White area)\")\n",
    "maskBGD = (mask == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "plt.subplot(234), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable background(white area)\")\n",
    "maskPBGD = (mask == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "plt.subplot(235), plt.axis('off'), plt.title(\n",
    "    \"GrabCut Mask(White for Foreground,Black for Background)\")\n",
    "# maskGrabCut = np.where((mask==cv2.GC_BGD) | (mask==cv2.GC_PR_BGD), 0, 1)\n",
    "plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(236), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(maskImg, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用掩膜图像的GrabCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现GrabCut算法（掩模图像）\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124744517\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# cv::GrabCutClasses {\n",
    "#   cv::GC_BGD = 0,\n",
    "#   cv::GC_FGD = 1,\n",
    "#   cv::GC_PR_BGD = 2,\n",
    "#   cv::GC_PR_FGD = 3\n",
    "# }\n",
    "image = cv2.imread(\n",
    "    r\"C:\\Programing\\MiddleData/original/20130820_102405_lasc2rdf_aia193rdf.png\",\n",
    "    flags=1)  # 读取彩色图像(BGR)\n",
    "# *对于CME灰度图像，三个通道的值完全相同\n",
    "maskImg = cv2.imread(r\"C:\\Programing\\MiddleData/cropresult/crop9.png\",\n",
    "                     flags=0)[:, 512:]  # 读取掩模图像(xupt) TODO need modification\n",
    "\n",
    "# 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "mask[maskImg == 0] = cv2.GC_BGD\n",
    "# print(mask.shape, maskInv.shape)\n",
    "# apply GrabCut using the the mask segmentation method\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(image,\n",
    "                                       mask,\n",
    "                                       None,\n",
    "                                       bgModel,\n",
    "                                       fgModel,\n",
    "                                       iter,\n",
    "                                       mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "# //原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "# //否则显示的区域就成了背景，所以进行修改\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(231), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(232), plt.axis('off'), plt.title(\"Mask image\")\n",
    "plt.imshow(maskImg, 'gray')  # definite background\n",
    "plt.subplot(233), plt.axis('off'), plt.title(\"GrabCut mask\")\n",
    "plt.imshow(mask, 'gray')\n",
    "plt.subplot(234), plt.axis('off'), plt.title(\"Mask for definite background\")\n",
    "maskBGD = (mask == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "plt.subplot(235), plt.axis('off'), plt.title(\"Mask for probable background\")\n",
    "maskPBGD = (mask == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "# plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "# plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(236), plt.axis('off'), plt.title(\"Youcans Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对掩膜图像的GrabCut进行了自行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# cv::GrabCutClasses {\n",
    "#   cv::GC_BGD = 0,\n",
    "#   cv::GC_FGD = 1,\n",
    "#   cv::GC_PR_BGD = 2,\n",
    "#   cv::GC_PR_FGD = 3\n",
    "# }\n",
    "image = cv2.imread(\n",
    "    r\"C:\\Programing\\MiddleData/original/20130820_102405_lasc2rdf_aia193rdf.png\",\n",
    "    flags=1)  # 读取彩色图像(BGR)\n",
    "# *对于CME灰度图像，三个通道的值完全相同\n",
    "maskImg = cv2.imread(r\"C:\\Programing\\MiddleData/cropresult/crop9.png\",\n",
    "                     flags=0)[:, 512:]  # 读取掩模图像(xupt) TODO need modification\n",
    "\n",
    "# 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "mask[maskImg == 0] = cv2.GC_BGD\n",
    "# print(mask.shape, maskInv.shape)\n",
    "# apply GrabCut using the the mask segmentation method\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(maskAfter, bgModel, fgModel) = cv2.grabCut(\n",
    "    image,\n",
    "    mask,\n",
    "    None,\n",
    "    bgModel,\n",
    "    fgModel,\n",
    "    # None,\n",
    "    # None,\n",
    "    iter,\n",
    "    mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD),\n",
    "                      0, 1)\n",
    "# *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "# *否则显示的区域就成了背景，所以进行修改\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "plt.subplot(332), plt.axis('off'), plt.title(\"Mask image\")\n",
    "plt.imshow(maskImg, 'gray')  # definite background\n",
    "\n",
    "plt.subplot(333), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "plt.subplot(334), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite background(white)\")\n",
    "maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "plt.subplot(335), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable background(white)\")\n",
    "maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "# plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "# plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(336), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "plt.subplot(337), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite foreground(white)\")\n",
    "maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "plt.subplot(338), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable foreground(white)\")\n",
    "maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#原图 掩膜图 grabcut生成的mask 前景 背景 绝对前景 绝对背景 grabcut后的图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩膜图像的GrabCut方法封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import DDT\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "def cutWithMask(image: np.ndarray,\n",
    "                maskImg: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对image进行GrabCut，使用maskImg作为掩膜\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        需要进行GrabCut的图像，为BGR图像，形状为NHWC，类型必须为uint8\n",
    "    maskImg : np.ndarray\n",
    "        作为掩膜的图像，为灰度图，形状为NHW\n",
    "    Returns\n",
    "    ----------\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理过后的掩膜，该数组的值为[0,1,2,3]其中之一，其定义为\n",
    "        cv::GC_BGD = 0,\n",
    "        cv::GC_FGD = 1,\n",
    "        cv::GC_PR_BGD = 2,\n",
    "        cv::GC_PR_FGD = 3\n",
    "    imgGrabCut : np.ndarray\n",
    "        经过GrabCut算法处理过后的图像，形状为NHWC\n",
    "    '''\n",
    "    #image = cv2.imread(imagePath, flags=1)  # 读取彩色图像(BGR)\n",
    "    # *CV2中，图片默认为BGR格式\n",
    "    for i in range(image.shape[0]):\n",
    "        image[i] = cv2.cvtColor(image[i], cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 对于CME灰度图像，三个通道的值完全相同\n",
    "    #maskImg = cv2.imread(maskImgPath, flags=0)[:, 512:]  # 读取掩模图像(xupt)\n",
    "\n",
    "    # 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "    mask = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "    mask[maskImg == 0] = cv2.GC_BGD\n",
    "    # print(mask.shape, maskInv.shape)\n",
    "    # apply GrabCut using the the mask segmentation method\n",
    "    fgModel = np.zeros((image.shape[0], 1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "    bgModel = np.zeros((image.shape[0], 1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "    iter = 5\n",
    "    maskAfter = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    imgGrabCut = np.zeros_like(image)\n",
    "    for i in range(image.shape[0]):\n",
    "        (maskAfter[i], bgModel[i],\n",
    "         fgModel[i]) = cv2.grabCut(image[i],\n",
    "                                   mask[i],\n",
    "                                   None,\n",
    "                                   bgModel[i],\n",
    "                                   fgModel[i],\n",
    "                                   iter,\n",
    "                                   mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "        maskOutput = np.where(\n",
    "            (maskAfter[i] == cv2.GC_BGD) | (maskAfter[i] == cv2.GC_PR_BGD), 0,\n",
    "            1)\n",
    "        # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "        # *否则显示的区域就成了背景，所以进行修改\n",
    "        maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "        imgGrabCut[i] = cv2.bitwise_and(image[i], image[i],\n",
    "                                        mask=maskGrabCut).astype('uint8')\n",
    "    return maskAfter, imgGrabCut\n",
    "\n",
    "\n",
    "def showGrabCutWithMask(image: np.ndarray, maskImg: np.ndarray,\n",
    "                        maskAfter: np.ndarray):\n",
    "    '''绘制GrabCut的结果\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        经GrabCut处理之前的原图像，形状为HWC\n",
    "    maskImg : np.ndarray\n",
    "        掩膜图像，形状为HW\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理之后的mask，形状为HW\n",
    "    '''\n",
    "    # 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "    maskOutput = np.where(\n",
    "        (maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD), 0, 1)\n",
    "    # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "    # *否则显示的区域就成了背景，所以进行修改\n",
    "    maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "    imgGrabCut = cv2.bitwise_and(image, image,\n",
    "                                 mask=maskGrabCut).astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "    plt.imshow(cv2.cvtColor(image.astype('uint8'),\n",
    "                            cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "    plt.subplot(332), plt.axis('off'), plt.title(\"Mask image\")\n",
    "    plt.imshow(maskImg, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(333), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "    plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "    plt.subplot(334), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "    plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "    plt.subplot(335), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite background(white)\")\n",
    "    maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(336), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable background(white)\")\n",
    "    maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "    # plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "    # plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "\n",
    "    plt.subplot(337), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite foreground(white)\")\n",
    "    maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "    plt.subplot(338), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable foreground(white)\")\n",
    "    maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.subplots_adjust(hspace=0.2,wspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from DDT import getLargestComponent\n",
    "\n",
    "\n",
    "def maskImage(imgs: np.ndarray, mask: np.ndarray, mode=cv2.COLORMAP_AUTUMN):\n",
    "    '''以mask为遮罩，得到img被遮罩后的数组\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : np.ndarray, uint8\n",
    "        原图像，形状为NHWC,值范围为[0,255]\n",
    "    mask : np.ndarray, uint8\n",
    "        遮罩，形状为NHW，值为0或1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        原图被遮罩遮盖后的数组\n",
    "    '''\n",
    "    if not np.issubsctype(mask.dtype, np.uint8) or not np.issubdtype(\n",
    "            imgs.dtype, np.uint8):\n",
    "        raise ValueError(\n",
    "            'Input img and mask dtype both should be uint8, got {} and {}'.\n",
    "            format(imgs.dtype, mask.dtype))\n",
    "    imgAfterMask = np.zeros((imgs.shape[0], imgs.shape[1], imgs.shape[2], 3),\n",
    "                            dtype=np.uint8)\n",
    "    mask = mask * 255\n",
    "    for i in range(imgs.shape[0]):\n",
    "        maskColored = np.expand_dims(mask[i], axis=0).transpose(1, 2, 0)\n",
    "        maskColored = cv2.cvtColor(cv2.applyColorMap(maskColored, mode),\n",
    "                                   cv2.COLOR_BGR2RGB)\n",
    "        img = np.tile(imgs[i], (1, 1, 3))\n",
    "        imgAfterMask[i] = cv2.addWeighted(img, 0.5, maskColored, 0.5, 0.0)\n",
    "    return imgAfterMask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前景选框的GrabCut方法封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWithROI(image: np.ndarray) -> tuple:\n",
    "    '''对image进行GrabCut，采用框选的方式选择前景\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        待分割的图像数组，形状为NHWC\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理过后的掩膜，该数组的值为[0,1,2,3]其中之一，其定义为\n",
    "        cv::GC_BGD = 0,\n",
    "        cv::GC_FGD = 1,\n",
    "        cv::GC_PR_BGD = 2,\n",
    "        cv::GC_PR_FGD = 3\n",
    "    '''\n",
    "    # *CV2中，图片默认为BGR格式\n",
    "    picNums = image.shape[0]\n",
    "    for i in range(image.shape[0]):\n",
    "        image[i] = cv2.cvtColor(image[i], cv2.COLOR_RGB2BGR)\n",
    "    mask = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "\n",
    "    # 定义矩形框，框选目标前景\n",
    "    # rect = (118, 125, 220, 245)  # 直接设置矩形的位置参数，也可以鼠标框选 ROI\n",
    "    rois = []\n",
    "    # imgsROI = np.zeros_like(image)  # 创建与 image 相同形状的黑色图像，方便展示选框框住的物体\n",
    "    for i in range(picNums):\n",
    "        windowName = 'Selecting ROI {}/{}'.format(i, picNums)\n",
    "        roi = cv2.selectROI(windowName,\n",
    "                            image[i],\n",
    "                            showCrosshair=True,\n",
    "                            fromCenter=False)\n",
    "        rois.append(roi)\n",
    "        # xmin, ymin, w, h = roi\n",
    "        # imgsROI[i, ymin:ymin + h, xmin:xmin + w] = image[i, ymin:ymin + h,\n",
    "        #                                                  xmin:xmin + w].copy()\n",
    "\n",
    "    fgModel = np.zeros((picNums, 1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "    bgModel = np.zeros((picNums, 1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "    iter = 5\n",
    "    maskAfter = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    for i in range(picNums):\n",
    "        (maskAfter[i], bgModel[i],\n",
    "         fgModel[i]) = cv2.grabCut(image[i],\n",
    "                                   mask[i],\n",
    "                                   rois[i],\n",
    "                                   bgModel[i],\n",
    "                                   fgModel[i],\n",
    "                                   iter,\n",
    "                                   mode=cv2.GC_INIT_WITH_RECT)  # 框选前景分割模式\n",
    "    return maskAfter, rois\n",
    "\n",
    "\n",
    "def showGrabCutWithROI(image: np.ndarray, roi: tuple, maskAfter: np.ndarray):\n",
    "    '''绘制利用ROI框选前景的GrabCut结果\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        待分割的原图片数组,形状为HW\n",
    "    rois : list\n",
    "        图片选框的四元组，包含xmin,ymin,w,h\n",
    "    maskAfter : np.ndarray\n",
    "        GrabCut输出的mask\n",
    "    '''\n",
    "    # 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "    maskOutput = np.where(\n",
    "        (maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD), 0, 1)\n",
    "    # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "    # *否则显示的区域就成了背景，所以进行修改\n",
    "    maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "    imgGrabCut = cv2.bitwise_and(image, image,\n",
    "                                 mask=maskGrabCut).astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "    imgROI = np.zeros_like(image)\n",
    "    xmin, ymin, w, h = roi\n",
    "    imgROI[ymin:ymin + h, xmin:xmin + w] = image[ymin:ymin + h,\n",
    "                                                 xmin:xmin + w].copy()\n",
    "    plt.subplot(332), plt.axis('off'), plt.title(\"Bounding Box\")\n",
    "    plt.imshow(imgROI, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(333), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "    plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "    plt.subplot(334), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "    plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "    plt.subplot(335), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite background(white)\")\n",
    "    maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(336), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable background(white)\")\n",
    "    maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "    # plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "    # plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "\n",
    "    plt.subplot(337), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite foreground(white)\")\n",
    "    maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "    plt.subplot(338), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable foreground(white)\")\n",
    "    maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrabCut算法测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一 20140106_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "crop = utils.CenterCrop('NCHW')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_1 = r'CME_data\\20140106_1'\n",
    "imageArray20140106_1 = utils.loadImageFolder(PathCME20140106_1)\n",
    "cropimageArray20140106_1 = crop(imageArray20140106_1)\n",
    "cropnet.predict(cropimageArray20140106_1)\n",
    "cropimageArray20140106_1 = cropimageArray20140106_1[cropnet.predict(\n",
    "    cropimageArray20140106_1) == 1]\n",
    "largestComp20140106_1 = DDT.DDT(cropimageArray20140106_1, cropnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropimageArray20140106_1 = cropimageArray20140106_1.squeeze()\n",
    "stackcropimageArray20140106_1 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1).astype('uint8')\n",
    "#!最大连接分量存在翻转情况\n",
    "reverlargestComp20140106_1 = 1 - largestComp20140106_1\n",
    "# *CutWithMask所接受的Image图片必须为uint8类型\n",
    "maskafter20140106_1 = cutWithMask(stackcropimageArray20140106_1,\n",
    "                                  reverlargestComp20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawLargestComp(cropimageArray20140106_1[:, None, :, :],\n",
    "                    reverlargestComp20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskafterROI, rois = cutWithROI(stackcropimageArray20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 1\n",
    "showGrabCutWithMask(cropimageArray20140106_1[picIndex],\n",
    "                    reverlargestComp20140106_1[picIndex],\n",
    "                    maskafter20140106_1[picIndex])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 8\n",
    "showGrabCutWithROI(cropimageArray20140106_1[picIndex], rois[picIndex],\n",
    "                   maskafterROI[picIndex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试二 20140106_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "crop = utils.CenterCrop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_2 = 'CME_data/20140106_2'\n",
    "imageArray20140106_2 = utils.loadImageFolder(PathCME20140106_2)\n",
    "cropimageArray20140106_2 = crop(imageArray20140106_2)\n",
    "print('cropnet predict:', cropnet.predict(cropimageArray20140106_2))\n",
    "cropimageArray20140106_2 = cropimageArray20140106_2[cropnet.predict(\n",
    "    cropimageArray20140106_2) == 1]\n",
    "largestComp20140106_2 = DDT.DDT(cropimageArray20140106_2, cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawLargestComp(cropimageArray20140106_2, largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropimageArray20140106_2 = cropimageArray20140106_2.squeeze()\n",
    "stackcropimageArray20140106_2 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_2).astype('uint8')\n",
    "# *CutWithMask所接受的Image图片必须为uint8类型\n",
    "maskafter20140106_2 = cutWithMask(stackcropimageArray20140106_2,\n",
    "                                  largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawImageArrays(cropimageArray20140106_2, largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 2\n",
    "showGrabCutWithMask(cropimageArray20140106_2[picIndex],\n",
    "                    largestComp20140106_2[picIndex],\n",
    "                    maskafter20140106_2[picIndex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更换第三方DDT算法的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一 20140106_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "# 使用ToTensor变换的网络\n",
    "cropnet_new = model.model_defination.LeNet5()\n",
    "parameter_path_new = 'trainIssueLog/2022_10_13_22_40_16/parameters.pkl'\n",
    "cropnet_new.load_param(parameter_path_new)\n",
    "crop = utils.CenterCrop('NCHW', value=127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_1 = r'CME_data\\20120204'\n",
    "imageArray20140106_1 = utils.loadImageFolder(PathCME20140106_1)\n",
    "cropimageArray20140106_1 = crop(imageArray20140106_1)\n",
    "projectedMap = DDT.DDTThirdParty(cropimageArray20140106_1, cropnet)\n",
    "colormap = DDT.toColorMap(projectedMap)\n",
    "outputImage = DDT.colorMapImage(cropimageArray20140106_1.astype('uint8'),\n",
    "                                projectedMap)\n",
    "\n",
    "\n",
    "def divide255(image):\n",
    "    return image / 255\n",
    "\n",
    "\n",
    "# 筛选了新的数据集，读取的图片默认归为[0,1]的网络结果\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_new = utils.loadImageFolder(PathCME20140106_1, trans)\n",
    "projectedMap_new = DDT.DDTThirdParty(cropimageArray20140106_1_new, cropnet_new)\n",
    "colormap_new = DDT.toColorMap(projectedMap_new)\n",
    "# 255*cropimageArray20140106_1_new.astype('uint8')=255*(cropimageArray20140106_1_new.astype('uint8'))\n",
    "outputImage_new = DDT.colorMapImage(\n",
    "    (cropimageArray20140106_1_new * 255).astype('uint8'), projectedMap_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp = getLargestComponent(projectedMap)\n",
    "imgConnectCut = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1).astype('uint8'),\n",
    "    largestcomp)\n",
    "largestcomp_new = getLargestComponent(projectedMap_new)\n",
    "imgConnectCut_new = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    largestcomp_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于进行着色的测试\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = cropimageArray20140106_1_new\n",
    "project_map = projectedMap_new\n",
    "mode = cv2.COLORMAP_JET\n",
    "output_imgs = np.zeros((imgs.shape[0], imgs.shape[2], imgs.shape[3], 3),\n",
    "                       dtype=np.uint8)\n",
    "masks = np.zeros((imgs.shape[0], imgs.shape[2], imgs.shape[3], 3),\n",
    "                 dtype=np.uint8)\n",
    "for i in range(imgs.shape[0]):\n",
    "    # img = cv2.resize(cv2.imread(os.path.join('./data', name)), (224, 224)) #读取为BGR格式\n",
    "    # 将project_map repeat为(3,H,W)再转置为(H,W,3)\n",
    "    # 这里的mask类似于自己的IndicatorMat\n",
    "    mask = np.tile(project_map[i], reps=(3, 1, 1)).transpose(1, 2, 0)\n",
    "    mask = cv2.cvtColor(cv2.applyColorMap(mask.astype(np.uint8), mode),\n",
    "                        cv2.COLOR_BGR2RGB)\n",
    "    masks[i] = mask\n",
    "    # addWeighted接受的两个数组应当为同样形状：HWC，因此将imgs[i]转为HWC形状\n",
    "    img = np.tile(imgs[i] * 255, (3, 1, 1)).transpose(1, 2, 0).astype('uint8')\n",
    "    output_img = cv2.addWeighted(img, 0.5, mask, 0.5, 0.0)\n",
    "    output_imgs[i] = output_img\n",
    "plt.imshow(output_imgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组会绘图\n",
    "ttle = [\n",
    "    'original image', 'original project map', 'mask image', 'Lenet(224) image',\n",
    "    'Lenet(224) project map', 'Lenet(224) mask image'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(\n",
    "    cropimageArray20140106_1.astype('uint8'))[6:9],\n",
    "                      utils.NCHWtoNHWC(projectedMap)[6:9],\n",
    "                      outputImage[6:9],\n",
    "                      utils.NCHWtoNHWC(cropimageArray20140106_1_new)[6:9],\n",
    "                      utils.NCHWtoNHWC(projectedMap_new)[6:9],\n",
    "                      outputImage_new[6:9],\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧网络的project_map产生亮圆环的原因是旧网络的图片中间填充的灰色而不是黑色\n",
    "colormap = DDT.toColorMap(projectedMap)\n",
    "colormap_new = DDT.toColorMap(projectedMap_new)\n",
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1.astype('uint8')),\n",
    "    utils.NCHWtoNHWC(projectedMap), outputImage, colormap,\n",
    "    largestcomp[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "    utils.NCHWtoNHWC(projectedMap_new), outputImage_new, colormap_new,\n",
    "    largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原网络的效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1.squeeze()).astype('uint8')\n",
    "maskafter20140106_1, imggrabcut20140106_1 = cutWithMask(\n",
    "    stackcropimageArray20140106_1, projectedMap.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 2\n",
    "showGrabCutWithMask(cropimageArray20140106_1[picIndex].squeeze(),\n",
    "                    projectedMap.squeeze()[picIndex],\n",
    "                    maskafter20140106_1[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(224)效果测试(ToTensor reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !新网络读取的图片数据为[0,1]内，因此需要转换至[0,255]\n",
    "stackcropimageArray20140106_1_new = (utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_new.squeeze() * 255)).astype('uint8')\n",
    "maskafter20140106_1_new, imggrabcut20140106_1_new = cutWithMask(\n",
    "    stackcropimageArray20140106_1_new, projectedMap_new.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 9\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_new[picIndex].squeeze(),\n",
    "                    projectedMap_new.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_new[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(112)效果测试(ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropnet_112 = model.model_defination.LeNet5(size=112)\n",
    "cropnet_112.load_param(r'trainIssueLog\\2022_10_16_20_12_25_112\\parameters.pkl')\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_112 = utils.loadImageFolder(PathCME20140106_1, trans)\n",
    "projectedMap_112 = DDT.DDTThirdParty(cropimageArray20140106_1_112, cropnet_112)\n",
    "colormap_112 = DDT.toColorMap(projectedMap_112)\n",
    "outputImage_112 = DDT.colorMapImage(\n",
    "    (255 * cropimageArray20140106_1_112).astype('uint8'), projectedMap_112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp_112 = getLargestComponent(projectedMap_112)\n",
    "imgConnectCut_112 = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_112).astype('uint8'),\n",
    "    largestcomp_112)\n",
    "colormap_112 = DDT.toColorMap(projectedMap_112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "    utils.NCHWtoNHWC(projectedMap_new), outputImage_new, colormap_new,\n",
    "    largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_new,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112),\n",
    "    utils.NCHWtoNHWC(projectedMap_112), outputImage_112, colormap_112,\n",
    "    largestcomp_112[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1_112 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_112.squeeze() * 255).astype('uint8')\n",
    "maskafter20140106_1_112, imggrabcut20140106_1_112 = cutWithMask(\n",
    "    stackcropimageArray20140106_1_112, projectedMap_112.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 10\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_112[picIndex].squeeze(),\n",
    "                    projectedMap_112.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_112[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(112)效果测试(ToTensor BoxBlur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from kornia.filters import box_blur\n",
    "import torch\n",
    "\n",
    "cropnet_112B = model.model_defination.LeNet5(size=112)\n",
    "cropnet_112B.load_param(\n",
    "    r'trainIssueLog\\2022_10_17_01_08_17_112_blur\\parameters.pkl')\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_112B = box_blur(\n",
    "    torch.from_numpy(utils.loadImageFolder(PathCME20140106_1, trans)),\n",
    "    (3, 3)).numpy()\n",
    "projectedMap_112B = DDT.DDTThirdParty(cropimageArray20140106_1_112B,\n",
    "                                      cropnet_112B)\n",
    "colormap_112B = DDT.toColorMap(projectedMap_112B)\n",
    "outputImage_112B = DDT.colorMapImage(\n",
    "    (255 * cropimageArray20140106_1_112B).astype('uint8'), projectedMap_112B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp_112B = getLargestComponent(projectedMap_112B)\n",
    "imgConnectCut_112B = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_112B).astype('uint8'),\n",
    "    largestcomp_112B)\n",
    "colormap_112B = DDT.toColorMap(projectedMap_112B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112),\n",
    "    utils.NCHWtoNHWC(projectedMap_112), outputImage_112, colormap_112,\n",
    "    largestcomp_112[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112B),\n",
    "    utils.NCHWtoNHWC(projectedMap_112B), outputImage_112B, colormap_112B,\n",
    "    largestcomp_112B[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1_112B = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_112B.squeeze() * 255).astype('uint8')\n",
    "maskafter20140106_1_112B, imggrabcut20140106_1_112B = cutWithMask(\n",
    "    stackcropimageArray20140106_1_112B, projectedMap_112B.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 8\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_112B[picIndex].squeeze(),\n",
    "                    projectedMap_112B.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_112B[picIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'Origin pic', 'origin mask', 'origin cut', 'reduced mask', 'reduced cut',\n",
    "    '112 mask', '112 cut', '112B mask', '112B cut'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "                      outputImage,\n",
    "                      imggrabcut20140106_1,\n",
    "                      outputImage_new,\n",
    "                      imggrabcut20140106_1_new,\n",
    "                      outputImage_112,\n",
    "                      imggrabcut20140106_1_112,\n",
    "                      outputImage_112B,\n",
    "                      imggrabcut20140106_1_112B,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 边缘检测与二值化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对得到的projectMap利用Canny算子计算得到边缘，将值范围为[0,255]的projectMap变为值为0或1的二值图，以便得到CME区域的位置。\n",
    "\n",
    "Canny算子需要给定低阈值和高阈值，一般把高阈值定为低阈值的2或3倍。因此，需要给定低阈值。梯度大小小于低阈值的边缘将被直接放弃，梯度大小大于高阈值的边缘被保留，梯度大小在低阈值和高阈值之间的边缘，若它连接着高于高阈值的边缘则会保留。选择一个合适的阈值可以使得Canny算法效果最优。\n",
    "\n",
    "**在进行canny算子边缘检测之前，以最大连接分量作为mask，遮罩共定位图，可以排除一些无关的图片部分**\n",
    "\n",
    "otsu算法可以自适应地选择阈值，将共定位图转化为0/1二值图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "\n",
    "def cannyThreshold(\n",
    "    src: np.ndarray,\n",
    "    lowThreshold: float,\n",
    "    kernelSize: int,\n",
    "    ratio=3,\n",
    "    color: Tuple[int, int, int] = (255, 0, 0)\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对src使用Canny边缘检测算法\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src : np.ndarray,uint8\n",
    "        需要进行处理的原图像，形状为NHWC\n",
    "    lowThreshold : float\n",
    "        低阈值\n",
    "    kernelSize : int\n",
    "        滤波器核大小\n",
    "    ratio : int, optional\n",
    "        高阈值与低阈值之比, by default 3\n",
    "    color : Tuple[int,int,int], optional\n",
    "        对检测出的边缘所赋的颜色\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    detecetedEdges : np.ndarray\n",
    "        算法输出的边缘，形状为NHW\n",
    "    dst : np.ndarray\n",
    "        将原图上的边缘标明后的图像,形状为NHWC\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        当src不为Uint8时抛出\n",
    "    '''\n",
    "    if not np.issubsctype(src.dtype, np.uint8):\n",
    "        raise ValueError(\n",
    "            'Expected input src dtype shouble uint8 ,got {}'.format(src.dtype))\n",
    "    detectedEdges = np.zeros((src.shape[0], src.shape[1], src.shape[2]),\n",
    "                             dtype=src.dtype)\n",
    "    dst = np.zeros_like(src, dtype=src.dtype)\n",
    "    for i in range(src.shape[0]):\n",
    "        srcGray = cv2.cvtColor(src[i], cv2.COLOR_RGB2GRAY)\n",
    "        imgBlur = cv2.blur(srcGray, (3, 3))\n",
    "        detectedEdges = cv2.Canny(imgBlur, lowThreshold, lowThreshold * ratio,\n",
    "                                  kernelSize)\n",
    "        mask = detectedEdges != 0\n",
    "        srciCopy = np.copy(src[i])\n",
    "        srciCopy[mask] = np.array(color)\n",
    "        dst[i] = srciCopy\n",
    "    return detectedEdges, dst\n",
    "\n",
    "\n",
    "from utils import applyMaskOnImage\n",
    "\n",
    "\n",
    "def otsuThreshold(img: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对图片数组应用otsu二值化\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        需要进行二值化的数组，形状为NHW，类型为uint8\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thre : float\n",
    "        每一张图片分割的阈值\n",
    "    binarypic : np.ndarray\n",
    "        图片应用otsu法产生的阈值后产生的二值图，值为0或255\n",
    "    '''\n",
    "    thre = np.zeros((img.shape[0], ))\n",
    "    binarypic = np.zeros_like(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        # 以下threshold参数第三项不能为img[i].max()，而应该直接给定255，\n",
    "        # 因为对于某些数组，最大值若不为255，\n",
    "        # 二值化后大于阈值的元素将变为原本的最大值，而不是255\n",
    "        thre[i], binarypic[i] = cv2.threshold(\n",
    "            img[i], 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thre, binarypic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "from typing import Tuple\n",
    "\n",
    "max_lowThreshold = 300\n",
    "window_name = 'Edge Map'\n",
    "title_trackbar = 'Min Threshold:'\n",
    "ratio = 3\n",
    "kernel_size = 3\n",
    "\n",
    "\n",
    "def CannyThreshold(val):\n",
    "    low_threshold = val\n",
    "    img_blur = cv.blur(src_gray, (3, 3))\n",
    "    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold * ratio,\n",
    "                              kernel_size)\n",
    "    mask = detected_edges != 0\n",
    "    # dst = src * (mask[:,:,None].astype(src.dtype))\n",
    "    dst = np.copy(image)\n",
    "    dst[mask] = np.array([0, 0, 255])\n",
    "    plt.imshow(dst)\n",
    "    # 为避免cv弹窗而注释以下行\n",
    "    # cv.imshow(window_name, dst)\n",
    "    return detected_edges, dst[:, :, ::-1]\n",
    "\n",
    "\n",
    "args = edict()\n",
    "args.input = r'C:\\Programing\\CMEclassfication\\CME_data\\20111122\\20111122_212813_lasc2rdf_aia193rdf.png'\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Code for Canny Edge Detector tutorial.')\n",
    "# parser.add_argument('--input',\n",
    "#                     help='Path to input image.',\n",
    "#                     default='fruits.jpg')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# src = cv.imread(cv.samples.findFile(args.input))\n",
    "# if src is None:\n",
    "#     print('Could not open or find the image: ', args.input)\n",
    "#     exit(0)\n",
    "# src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "picindex = 5\n",
    "maskedprojectmap_new = np.concatenate(tuple(\n",
    "    cv2.bitwise_and(projectedMap_new[i, 0].astype('uint8'),\n",
    "                    projectedMap_new[i, 0].astype('uint8'),\n",
    "                    mask=largestcomp_new[i])[None, :, :]\n",
    "    for i in range(projectedMap_new.shape[0])),\n",
    "                                      axis=0)\n",
    "image = (cropimageArray20140106_1_new[picindex] * 255).astype('uint8')\n",
    "image = np.tile(image.transpose(1, 2, 0), reps=3)\n",
    "src = np.tile(maskedprojectmap_new[picindex].astype('uint8'),\n",
    "              reps=(3, 1, 1)).transpose(1, 2, 0)\n",
    "src_gray = maskedprojectmap_new[picindex].astype('uint8')\n",
    "# 为了避免cv弹窗而注释以下两行\n",
    "# cv2.namedWindow(window_name)\n",
    "# cv2.createTrackbar(title_trackbar, window_name, 0, max_lowThreshold,\n",
    "#                    CannyThreshold)\n",
    "edge, dst = CannyThreshold(65)\n",
    "# cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 7))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.imshow(dst)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.imshow(edge, cmap='gray')\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(src)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(projectedMap_new[picindex, 0].astype('uint8'), cmap='gray')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(colormap_new[picindex].astype('uint8'))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(largestcomp_new[picindex].astype('uint8'), cmap='gray')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(maskedprojectmap_new[picindex].astype('uint8'), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阈值的选择\n",
    "\n",
    "观察得到的CME共定位图，相关性最高的地方应该是CME的核心区域，在此处，图片的亮度最高。在明亮区域的四周分布着低亮度的区域。首先，可以尝试使用最大连接分量作为mask，去掉那些零散的部分。其次，从共定位图的低相关性部分到CME高相关性部分，在接近CME时，梯度应当有着巨大的改变。选取这样的一个梯度值，可以使得边缘检测分割效果最佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制图像灰度直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图像直方图\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Code for Histogram Calculation tutorial.')\n",
    "# parser.add_argument('--input', help='Path to input image.', default='lena.jpg')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args = edict()\n",
    "# args.input = r'C:\\Users\\lenovo\\Pictures\\th5.jfif'\n",
    "# src = cv.imread(cv.samples.findFile(args.input))\n",
    "# if src is None:\n",
    "#     print('Could not open or find the image:', args.input)\n",
    "#     exit(0)\n",
    "\n",
    "src = maskedprojectmap_new[picindex].astype('uint8')\n",
    "histSize = 10\n",
    "histRange = (0, 256)  # the upper boundary is exclusive\n",
    "accumulate = False\n",
    "hist = cv.calcHist([src], [0],\n",
    "                   None, [histSize],\n",
    "                   histRange,\n",
    "                   accumulate=accumulate)\n",
    "hist_w = 512\n",
    "hist_h = 400\n",
    "bin_w = int(round(hist_w / histSize))\n",
    "histImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n",
    "cv.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n",
    "for i in range(1, histSize):\n",
    "    cv.line(histImage, (bin_w * (i - 1), hist_h - int(hist[i - 1])),\n",
    "            (bin_w * (i), hist_h - int(hist[i])), (255, 0, 0),\n",
    "            thickness=2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(histImage)\n",
    "# 为避免cv弹窗而注释以下三行\n",
    "# cv.imshow('Source image', src)\n",
    "# cv.imshow('calcHist Demo', histImage)\n",
    "# cv.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图通过梯度直方图来找到一个合适的阈值，以便分割图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制梯度梯度直方图\n",
    "src = maskedprojectmap_new[picindex].astype('uint8')\n",
    "# src = projectedMap[picindex].astype('uint8').squeeze()\n",
    "gx = cv.Sobel(src, cv.CV_32F, 1, 0, ksize=1)\n",
    "gy = cv.Sobel(src, cv.CV_32F, 0, 1, ksize=1)\n",
    "abs_gx = cv.convertScaleAbs(gx)\n",
    "abs_gy = cv.convertScaleAbs(gy)\n",
    "grad = cv.addWeighted(abs_gx, 0.5, abs_gy, 0.5, 0)\n",
    "\n",
    "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "histSize = 10\n",
    "histRange = (0, 256)  # the upper boundary is exclusive\n",
    "accumulate = False\n",
    "hist = cv.calcHist([mag], [0],\n",
    "                   None, [histSize],\n",
    "                   histRange,\n",
    "                   accumulate=accumulate)\n",
    "hist_w = 512\n",
    "hist_h = 400\n",
    "bin_w = int(round(hist_w / histSize))\n",
    "histImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n",
    "cv.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n",
    "for i in range(1, histSize):\n",
    "    cv.line(histImage, (bin_w * (i - 1), hist_h - int(hist[i - 1])),\n",
    "            (bin_w * (i), hist_h - int(hist[i])), (255, 0, 0),\n",
    "            thickness=2)\n",
    "# cv.imshow('Source image', src)\n",
    "# cv.imshow('Gradient hist', histImage)\n",
    "# cv.waitKey()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(histImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试使用otsu算法直接分割共定位图和梯度强度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otsu算法直接作用于梯度强度图和共定位图的效果\n",
    "gradthreshold, gradbinarypic = cv2.threshold(\n",
    "    grad, 0, grad.max(), cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "srcthreshold, srcbinarypic = cv2.threshold(src, 0, 1,\n",
    "                                           cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "srcbinarypic = srcbinarypic.astype('uint8')\n",
    "imgthreshold = maskImage(\n",
    "    utils.NCHWtoNHWC(\n",
    "        255 *\n",
    "        cropimageArray20140106_1_new[picindex][None, :, :, :]).astype('uint8'),\n",
    "    srcbinarypic[None, :, :])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(src, cmap='gray')\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(srcbinarypic, cmap='gray')  # otsu作用于projectMap\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(gradbinarypic, cmap='gray')  # otsu作用于梯度强度图\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(imgthreshold[0])  # 将projectmap的otsu结果遮罩到原图上\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(dst)  # 使用对projectMap使用canny算子，再遮罩到原图上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(src, cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mag > 0.25 * mag.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histnp, binnp = np.histogram(mag, bins=20, range=(0, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试otsuThreshold函数\n",
    "maskedprojectedMap_new = applyMaskOnImage(\n",
    "    projectedMap_new.squeeze(),  #共定位图使用最大连接分量遮罩后\n",
    "    largestcomp_new)\n",
    "thre_new, otsuprojectMap_new = otsuThreshold(  # 直接对共定位图使用otsu算法\n",
    "    projectedMap_new.squeeze().astype('uint8'))\n",
    "maskedthre_new, otsumaskedprojectedMap_new = otsuThreshold(  # 对遮罩的共定位图使用otsu算法\n",
    "    maskedprojectedMap_new.squeeze().astype('uint8'))\n",
    "otsuCut = maskImage(  # 共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    (otsuprojectMap_new / 255).astype('uint8'))\n",
    "maskedotsuCut = maskImage(  # 遮罩后的共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    (otsumaskedprojectedMap_new / 255).astype('uint8'))\n",
    "ttle = [\n",
    "    'projectedMap_new', 'largestcomp_new', 'otsuprojectMap_new',\n",
    "    'outputImage_new', 'otsuCut', 'maskedprojectedMap_new',\n",
    "    'otsumaskedprojectedMap_new', 'maskedotsuCut'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(projectedMap_new),\n",
    "                      largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "                      otsuprojectMap_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "                      outputImage_new,\n",
    "                      otsuCut,\n",
    "                      maskedprojectedMap_new[:,\n",
    "                                             None, :, :].transpose(0, 2, 3, 1),\n",
    "                      otsumaskedprojectedMap_new[:, None, :, :].transpose(\n",
    "                          0, 2, 3, 1),\n",
    "                      maskedotsuCut,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 物理参数提取测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论文中的方法\n",
    "1. 将得到的CME区域遮罩转变为极坐标形式 warpToPolar\n",
    "2. 得到区域遮罩极坐标形势下每一列的尖端位置 getTipLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换极坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = outputImage_new[5]\n",
    "\n",
    "h, w = img.shape[:2]  # 图片的高度和宽度\n",
    "cx, cy = 258, 243  # 以图像中心点作为变换中心\n",
    "maxR = max(cx, cy)  # 最大变换半径\n",
    "\n",
    "# img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "# imgPolar = cv2.linearPolar(img, (258,243), maxR, cv2.INTER_LINEAR)\n",
    "imgPolar = cv2.warpPolar(img, (360, 360), (258, 243), 254,\n",
    "                         cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPR = cv2.rotate(imgPolar, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(131), plt.imshow(img), plt.title(\"Original\")\n",
    "plt.subplot(132), plt.imshow(imgPR), plt.title(\"PolarTrans\")\n",
    "plt.show()\n",
    "\n",
    "img = largestcomp_new[5]\n",
    "# imgPolar = cv2.warpPolar(img, (512, 512), (258, 243), 254,\n",
    "#                          cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPolar = cv2.warpPolar(img, (360, 360), (258, 243), 240,\n",
    "                         cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPR = cv2.rotate(imgPolar, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(131), plt.imshow(img), plt.title(\"Original\")\n",
    "plt.subplot(132), plt.imshow(imgPR), plt.title(\"PolarTrans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "极坐标转换，获取尖端位置，过滤尖端函数定义\n",
    "\n",
    "warpCartToPolar,getTipLocation,filterTip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_polar_mapping(output_coords, k_angle, k_radius, center):\n",
    "    \"\"\"从欧几里得坐标系转换为极坐标系的反向坐标变换函数  \n",
    "    \n",
    "    修改skimage.transform._linear_polar_mapping为以图像北侧为极轴，逆时针测量的变换。配合skimage.transform.warp使用\n",
    "    skimage.transform中原本的函数是以图像东侧为极轴，顺时针测量。\n",
    "    \"\"\"\n",
    "    angle = output_coords[:, 1] / k_angle\n",
    "    rr = ((output_coords[:, 0] / k_radius) * -np.cos(angle)) + center[0]\n",
    "    cc = ((output_coords[:, 0] / k_radius) * -np.sin(angle)) + center[1]\n",
    "    coords = np.column_stack((cc, rr))\n",
    "    return coords\n",
    "\n",
    "\n",
    "def safeAsInt(val, atol=1e-3):\n",
    "    \"\"\"\n",
    "    安全地将值转化为整形\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    val : scalar or iterable of scalars\n",
    "        Number or container of numbers which are intended to be interpreted as\n",
    "        integers, e.g., for indexing purposes, but which may not carry integer\n",
    "        type.\n",
    "    atol : float\n",
    "        Absolute tolerance away from nearest integer to consider values in\n",
    "        ``val`` functionally integers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    val_int : NumPy scalar or ndarray of dtype `np.int64`\n",
    "        Returns the input value(s) coerced to dtype `np.int64` assuming all\n",
    "        were within ``atol`` of the nearest integer.\n",
    "    \"\"\"\n",
    "    mod = np.asarray(val) % 1  # Extract mantissa\n",
    "\n",
    "    # Check for and subtract any mod values > 0.5 from 1\n",
    "    if mod.ndim == 0:  # Scalar input, cannot be indexed\n",
    "        if mod > 0.5:\n",
    "            mod = 1 - mod\n",
    "    else:  # Iterable input, now ndarray\n",
    "        mod[mod > 0.5] = 1 - mod[mod > 0.5]  # Test on each side of nearest int\n",
    "\n",
    "    try:\n",
    "        np.testing.assert_allclose(mod, 0, atol=atol)\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'Integer argument required but received '\n",
    "                         f'{val}, check inputs.')\n",
    "\n",
    "    return np.round(val).astype(np.int64)\n",
    "\n",
    "\n",
    "def warpPolar(image: np.ndarray,\n",
    "              center: Optional[Tuple[float, float]] = None,\n",
    "              *,\n",
    "              radius: Optional[int] = None,\n",
    "              output_shape: Optional[Tuple[int, int]] = None,\n",
    "              channel_axis=None,\n",
    "              **kwargs):\n",
    "    \"\"\"将图像重新映射至极坐标\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ndarray\n",
    "        Input image. Only 2-D arrays are accepted by default. 3-D arrays are\n",
    "        accepted if a `channel_axis` is specified.\n",
    "    center : tuple (row, col), optional\n",
    "        Point in image that represents the center of the transformation (i.e.,\n",
    "        the origin in cartesian space). Values can be of type `float`.\n",
    "        If no value is given, the center is assumed to be the center point\n",
    "        of the image.\n",
    "    radius : float, optional\n",
    "        Radius of the circle that bounds the area to be transformed.\n",
    "    output_shape : tuple (row, col), optional\n",
    "    multichannel : bool, optional\n",
    "        Whether the image is a 3-D array in which the third axis is to be\n",
    "        interpreted as multiple channels. If set to `False` (default), only 2-D\n",
    "        arrays are accepted. This argument is deprecated: specify\n",
    "        `channel_axis` instead.\n",
    "    channel_axis : int or None, optional\n",
    "        If None, the image is assumed to be a grayscale (single channel) image.\n",
    "        Otherwise, this parameter indicates which axis of the array corresponds\n",
    "        to channels.\n",
    "    **kwargs : keyword arguments\n",
    "        Passed to `transform.warp`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    warped : ndarray\n",
    "        The polar or log-polar warped image.\n",
    "    \"\"\"\n",
    "    multichannel = channel_axis is not None\n",
    "    if image.ndim != 2 and not multichannel:\n",
    "        raise ValueError(f'Input array must be 2-dimensional when '\n",
    "                         f'`channel_axis=None`, got {image.ndim}')\n",
    "\n",
    "    if image.ndim != 3 and multichannel:\n",
    "        raise ValueError(f'Input array must be 3-dimensional when '\n",
    "                         f'`channel_axis` is specified, got {image.ndim}')\n",
    "\n",
    "    if center is None:\n",
    "        center = (np.array(image.shape)[:2] / 2) - 0.5\n",
    "\n",
    "    if radius is None:\n",
    "        w, h = np.array(image.shape)[:2] / 2\n",
    "        radius = np.sqrt(w**2 + h**2)\n",
    "\n",
    "    if output_shape is None:\n",
    "        height = 360\n",
    "        width = int(np.ceil(radius))\n",
    "        output_shape = (height, width)\n",
    "    else:\n",
    "        output_shape = safeAsInt(output_shape)\n",
    "        height = output_shape[0]\n",
    "        width = output_shape[1]\n",
    "\n",
    "    k_radius = width / radius\n",
    "    map_func = _linear_polar_mapping\n",
    "\n",
    "    k_angle = height / (2 * np.pi)\n",
    "    warp_args = {'k_angle': k_angle, 'k_radius': k_radius, 'center': center}\n",
    "\n",
    "    from skimage.transform import warp\n",
    "    warped = warp(image,\n",
    "                  map_func,\n",
    "                  map_args=warp_args,\n",
    "                  output_shape=output_shape,\n",
    "                  preserve_range=True,\n",
    "                  order=0,\n",
    "                  **kwargs)\n",
    "    return warped\n",
    "\n",
    "\n",
    "def warpCartToPolar(img: np.ndarray, dsize: tuple[int, int],\n",
    "                    center: tuple[int, int], maxRadius: float) -> np.ndarray:\n",
    "    '''将数组转换为极坐标形式\n",
    "\n",
    "    坐标变换会改变图像像素值分布\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        需要转换的数组，形状为NHW或NHWC\n",
    "    dsize : tuple\n",
    "        输出的图像大小，格式为(列,行)\n",
    "    center : tuple\n",
    "        转换中心，格式为(列,行)\n",
    "    maxRadius : float\n",
    "        最大半径，决定转换的最大范围\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        转换后的图片数组，类型同输入图像数组img相同\n",
    "    '''\n",
    "    # skimage接受的中心坐标和图像尺寸都是(行,列)，需要进行转换\n",
    "    center = (center[1], center[0])\n",
    "    dsize = (dsize[1], dsize[0])\n",
    "    if img.ndim != 3 and img.ndim != 4:\n",
    "        raise ValueError('Input image shape must be 3 or 4 ,got {}'.format(\n",
    "            img.ndim))\n",
    "    if img.ndim == 3:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1]),\n",
    "                              dtype=img.dtype)\n",
    "    else:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w,C)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1], img.shape[3]),\n",
    "                              dtype=img.dtype)\n",
    "    channel_axis = None if img.ndim == 3 else 3\n",
    "    from skimage.transform import rotate\n",
    "    for i in range(img.shape[0]):\n",
    "        imgPol = warpPolar(img[i],\n",
    "                           center,\n",
    "                           radius=maxRadius,\n",
    "                           output_shape=dsize,\n",
    "                           channel_axis=channel_axis)\n",
    "        imgPol = rotate(imgPol, 90, order=0, preserve_range=True)\n",
    "        # imgPol = cv2.warpPolar(img[i], dsize, center, maxRadius,\n",
    "        #                        cv2.INTER_NEAREST + cv2.WARP_POLAR_LINEAR)\n",
    "        # imgPol = cv2.rotate(imgPol, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        imgInPolar[i] = imgPol\n",
    "    return imgInPolar\n",
    "\n",
    "\n",
    "def getTipLocation(imgInPolar: np.ndarray, fromBottom=True) -> np.ndarray:\n",
    "    '''获取极坐标图像中的尖端位置\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgInPolar : np.ndarray\n",
    "        极坐标图像，形状为NHW，值为0或1，类型为uint8\n",
    "    fromBottom : bool\n",
    "        若为True，尖端位置从图像底部开始计数，若为False，则从图像顶部开始计数\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        尖端位置，形状为NW，当fromBottom为True时，表示第N张图片中横坐标为W的那一列中的尖端距离图像顶端的距离，\n",
    "        若为False，表示尖端距离图像底部的距离。\n",
    "        \n",
    "        若值为np.nan则表示该列不存在CME遮罩。\n",
    "        \n",
    "        存在距离顶端和距离底部两种距离的原因在于，\n",
    "        将图像数据用ndarray表示时，ndarray的索引原点在左上角，\n",
    "        但对于图像来说，习惯的原点位置在左下角。\\n\n",
    "        *****数组中存在nan，可能会导致意料之外的问题*****\n",
    "    '''\n",
    "    if imgInPolar.ndim != 3 and imgInPolar.ndim != 4:\n",
    "        raise ValueError('Input image shape must be 3 or 4 ,got {}'.format(\n",
    "            imgInPolar.ndim))\n",
    "    if not np.issubsctype(imgInPolar.dtype, np.uint8):\n",
    "        raise ValueError('Input array dtype must be np.uint8 , got {}'.format(\n",
    "            imgInPolar.dtype))\n",
    "    if not imgInPolar.max() == 1:\n",
    "        raise ValueError(\n",
    "            'Input array must be a 0/1 binary array , got input array max value {}'\n",
    "            .format(imgInPolar.max()))\n",
    "    # tipLocation的类型之前选择为uint8，导致超过256的数字会溢出\n",
    "    tipLocation = np.zeros((imgInPolar.shape[0], imgInPolar.shape[2]))\n",
    "    for i in range(imgInPolar.shape[0]):\n",
    "        for j in range(imgInPolar.shape[2]):\n",
    "            # 需要获得极坐标图像中CME每一个位置角上CME遮罩的最大高度\n",
    "            # 由于ndarray数组是以左上角为原点，两个维度为HW\n",
    "            # 因此需要求在W维度上(每一列)值为1的那些元素的纵坐标的最小值\n",
    "            if np.argwhere(imgInPolar[i][:, j] == 1).size == 0:\n",
    "                tipLocation[i, j] = np.nan\n",
    "            else:\n",
    "                if fromBottom:\n",
    "                    tipLocation[i, j] = imgInPolar.shape[1] - np.argwhere(\n",
    "                        imgInPolar[i][:, j] == 1).min() - 1\n",
    "                else:\n",
    "                    tipLocation[i,\n",
    "                                j] = np.argwhere(imgInPolar[i][:,\n",
    "                                                               j] == 1).min()\n",
    "    return tipLocation\n",
    "\n",
    "\n",
    "def filterTip(tip: np.ndarray, height: float) -> np.ndarray:\n",
    "    '''对包含尖端位置的数组进行过滤\n",
    "\n",
    "    过滤条件：\n",
    "    - 最大高度低于height的方位角将被抛弃\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tip : np.ndarray\n",
    "        包含尖端位置的数组\n",
    "    height : float\n",
    "        尖端位置低于height的位置角将被移除\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        经过过滤的数组\n",
    "    '''\n",
    "    tip = np.copy(tip)\n",
    "    tip[:, np.nanmax(tip, axis=0) < 180] = np.nan\n",
    "    return tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "binaryMap = otsumaskedprojectedMap_new\n",
    "binaryMap_polar = warpCartToPolar(binaryMap, (360, 360), (258, 243), 245)\n",
    "tipb = getTipLocation((binaryMap_polar / 255).astype('uint8'))\n",
    "tipu = getTipLocation((binaryMap_polar / 255).astype('uint8'), False)\n",
    "filtedtipb = filterTip(\n",
    "    tipb, binaryMap_polar.shape[1])  # binaryMap_polar.shape[1]即图像高度\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(binaryMap[ind])\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(binaryMap_polar[ind])\n",
    "tipImage = np.zeros_like(binaryMap_polar)\n",
    "for i in range(tipu.shape[0]):\n",
    "    for j in range(tipu.shape[1]):\n",
    "        if not np.isnan(tipu[i, j]):\n",
    "            tipImage[i, tipu[i, j].astype('uint'), j] = 1\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "# plt.plot(np.arange(512), 512-np.nan_to_num(tipu[ind]))\n",
    "plt.plot(np.arange(tipb.shape[1]), np.nan_to_num(tipb[ind]))\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "plt.plot(np.arange(tipu.shape[1]),\n",
    "         binaryMap_polar.shape[1] - np.nan_to_num(tipu[ind]))\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "plt.plot(np.arange(tipb.shape[1]), np.nan_to_num(filtedtipb[ind]))\n",
    "# plt.imshow(tipImage[ind])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(binaryMap_polar[ind])\n",
    "img[:, np.nanmax(tipb, axis=0) < 180] = 0\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算中央位置角"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内部定义函数的修改\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "\n",
    "def findContinusNumbers(numList: List[int]) -> List[tuple]:\n",
    "    '''寻找列表中连续的数字段\n",
    "    \n",
    "    返回一个由元组构成的列表，每一个元组都是一段连续数字的起始数字和结束数字\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    numList : List[int]\n",
    "        连续的数字序列\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[tuple]\n",
    "        元组构成的列表，每一个元组都是一组连续数字的起始数字和结束数字\n",
    "    '''\n",
    "    numList.sort()\n",
    "    ind = 1\n",
    "    findList = []\n",
    "    while ind <= len(numList) - 1:\n",
    "        if numList[ind] - numList[ind - 1] == 1:\n",
    "            flag = ind - 1\n",
    "            while numList[ind] - numList[ind - 1] == 1:\n",
    "                ind += 1\n",
    "                if ind > len(numList) - 1:\n",
    "                    break\n",
    "            findList.append((numList[flag], numList[ind - 1]))\n",
    "        else:\n",
    "            ind += 1\n",
    "    return findList\n",
    "\n",
    "\n",
    "def calcCPA(tip: np.ndarray):\n",
    "    startEndPosition = []\n",
    "    for i in range(tip.shape[0]):\n",
    "        effectivePosition = np.argwhere(\n",
    "            ~np.isnan(filtedtipb[i])).squeeze().tolist()\n",
    "        currentStartEnd = findContinusNumbers(effectivePosition)\n",
    "        startEndPosition.append(currentStartEnd)\n",
    "    return startEndPosition\n",
    "\n",
    "\n",
    "a = calcCPA(filtedtipb)\n",
    "print(list(map(lambda x: len(x), a)))\n",
    "a[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算方位角\n",
    "effectivePA = np.argwhere(\n",
    "    ~np.isnan(filtedtipb[ind])).squeeze().tolist()  # tip不为nan的位置角\n",
    "\n",
    "\n",
    "def pixelToDegree(PAinPixel, width):\n",
    "    kangle = 360 / width\n",
    "    return kangle * PAinPixel % 360\n",
    "\n",
    "\n",
    "startEndPA = []\n",
    "start = effectivePA[0]\n",
    "end = effectivePA[1]\n",
    "lastValue = effectivePA[0]\n",
    "index = 1\n",
    "while index <= len(effectivePA) - 1:\n",
    "    if effectivePA[index] - effectivePA[index - 1] == 1:\n",
    "        flag = index - 1\n",
    "        while effectivePA[index] - effectivePA[index - 1] == 1:\n",
    "            index += 1\n",
    "            if index > len(effectivePA) - 1:\n",
    "                break\n",
    "        startEndPA.append([effectivePA[flag], effectivePA[index - 1]])\n",
    "    else:\n",
    "        index += 1\n",
    "\n",
    "if startEndPA[0][0] == 0 and startEndPA[-1][1] == 359:\n",
    "    startEndPA[-1][1] += startEndPA[0][1]\n",
    "    startEndPA\n",
    "startEndPA.pop(0)\n",
    "CPAs = []\n",
    "for PAs in startEndPA:\n",
    "    CPAs.append(\n",
    "        pixelToDegree(sum(PAs) / len(PAs), width=binaryMap_polar.shape[1]))\n",
    "CPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(binaryMap_polar[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试cv2 contours系列函数\n",
    "考虑到只得到CME尖端区域的坐标来计算CME物理参数会缺失很多信息，造成困难，因此尝试使用cv2 contours轮廓线系列函数，使用完整的CME轮廓信息来完成跟踪。\n",
    "\n",
    "预期步骤：\n",
    "1. 获取每一帧的CME区域\n",
    "2. 计算与前一帧所有的CME区域两两之间的形状相似度\n",
    "3. 根据计算得到的形状相似度进行分配，获知两帧之间哪些CME区域是相同的，哪些区域是新的，哪些区域消失了，并给每一个CME区域分配ID号用以标识\n",
    "4. 计算速度、角速度、角宽度等物理量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试使用cv2 contours系列函数\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind]).astype('uint8'), cv2.RETR_EXTERNAL, cv2.\n",
    "    CHAIN_APPROX_NONE)  # contours是个元组，每一个元素都是一个array，包含了轮廓点的x,y坐标，坐标系原点在左上角\n",
    "plt.imshow(cv2.drawContours(\n",
    "    cv2.merge((np.copy(binaryMap_polar[ind]), np.copy(binaryMap_polar[ind]),\n",
    "               np.copy(binaryMap_polar[ind]))),\n",
    "    contours,\n",
    "    contourIdx=0,\n",
    "    color=(255, 0, 0),\n",
    "    thickness=1,\n",
    "),\n",
    "           cmap='gray')\n",
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看contours的寻找是否正确\n",
    "img = np.zeros((360, 360))\n",
    "for i in contours[0].squeeze():\n",
    "    img[i[1], i[0]] = 1\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用模板匹配方法\n",
    "试图获得CME区域的边缘框，再利用cv2的模板匹配来获取两个帧之间CME区域的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得边缘框来选择CME区域\n",
    "Gcentera, Gcenterb, w, h = cv2.boundingRect(contours[0].squeeze())\n",
    "plt.imshow(\n",
    "    cv2.rectangle(\n",
    "        cv2.merge((binaryMap_polar[ind], binaryMap_polar[ind],\n",
    "                   binaryMap_polar[ind])), (Gcentera, Gcenterb),\n",
    "        (Gcentera + w, Gcenterb + h), (255, 255, 255), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行模板匹配\n",
    "imgtempl = binaryMap_polar[ind][Gcenterb:Gcenterb + h, Gcentera:Gcentera + w]\n",
    "plt.imshow(imgtempl, cmap='gray')\n",
    "# imgMatched = cv2.matchTemplate(binaryMap[ind],\n",
    "#                                imgtempl[y:y + h, x:x + w],\n",
    "#                                cv2.TM_CCOEFF_NORMED)\n",
    "imgMatched = cv2.matchTemplate(\n",
    "    binaryMap_polar[ind], binaryMap_polar[ind][Gcenterb:Gcenterb + h,\n",
    "                                               Gcentera:Gcentera + w],\n",
    "    cv2.TM_CCOEFF_NORMED)\n",
    "minval, maxval, minloc, maxloc = cv2.minMaxLoc(imgMatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgMatched, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    cv2.circle(\n",
    "        cv2.merge((binaryMap_polar[ind], binaryMap_polar[ind],\n",
    "                   binaryMap_polar[ind])),\n",
    "        (int(maxloc[0] + imgtempl.shape[1] / 2),\n",
    "         int(maxloc[1] + imgtempl.shape[0] / 2)), 20, (0, 0, 255), 2, 8, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用形状相似度方法\n",
    "1. 使用cv2.matchShape函数获取形状相似度\n",
    "2. 利用linear_sum_assignment将两帧中的CME区域进行匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取CME区域轮廓线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\"\n",
    "\n",
    "ind = 0\n",
    "contoursNext, hierarchyNext = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind + 1]).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_NONE)\n",
    "contoursNext = sorted(list(contoursNext),\n",
    "                      key=lambda x: x.shape[0],\n",
    "                      reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "print('ContoursNext:', list(map(lambda x: len(x), contoursNext)))\n",
    "contoursNumThre = contoursNext[0].shape[0] / 10\n",
    "contoursNext = list(\n",
    "    filter(lambda x: x.shape[0] > contoursNumThre,\n",
    "           contoursNext))  # 过滤掉点数小于最大轮廓线点数1/10的轮廓线\n",
    "\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind]).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_NONE)\n",
    "contours = sorted(list(contours), key=lambda x: x.shape[0],\n",
    "                  reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "print('Contours:', list(map(lambda x: len(x), contours)))\n",
    "contoursNumThre = contours[0].shape[0] / 10\n",
    "contours = list(filter(lambda x: x.shape[0] > contoursNumThre,\n",
    "                       contours))  # 过滤掉点数小于最大轮廓线点数1/10的轮廓线\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'Next Frame {ind+1}:total {len(contoursNext)} contours')\n",
    "imgNext = cv2.merge(\n",
    "    (np.copy(binaryMap_polar[ind + 1]), np.copy(binaryMap_polar[ind + 1]),\n",
    "     np.copy(binaryMap_polar[ind + 1])))\n",
    "for i in range(len(contoursNext)):\n",
    "    # orgx, orgy = contoursNext[i].squeeze(\n",
    "    # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "    orgx, orgy = int(contoursNext[i].squeeze().mean(axis=0)[0]), int(\n",
    "        contoursNext[i].squeeze().mean(axis=0)[1])\n",
    "    cv2.putText(imgNext,\n",
    "                str(i),\n",
    "                org=(orgx, orgy),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(14, 173, 238),\n",
    "                thickness=2)\n",
    "    cv2.drawContours(\n",
    "        imgNext,\n",
    "        contoursNext,\n",
    "        contourIdx=i,\n",
    "        color=(127, 127, 127),\n",
    "        thickness=1,\n",
    "    )\n",
    "plt.imshow(imgNext)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'Curr Frame {ind} :total {len(contours)} contours')\n",
    "img = cv2.merge((np.copy(binaryMap_polar[ind]), np.copy(binaryMap_polar[ind]),\n",
    "                 np.copy(binaryMap_polar[ind])))\n",
    "for i in range(len(contours)):\n",
    "    # orgx, orgy = contoursNext[i].squeeze(\n",
    "    # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "    orgx, orgy = int(contours[i].squeeze().mean(axis=0)[0]), int(\n",
    "        contours[i].squeeze().mean(axis=0)[1])\n",
    "    cv2.putText(img,\n",
    "                str(i),\n",
    "                org=(orgx, orgy),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(14, 173, 238),\n",
    "                thickness=2)\n",
    "    cv2.drawContours(\n",
    "        img,\n",
    "        contours,\n",
    "        contourIdx=i,\n",
    "        color=(127, 127, 127),\n",
    "        thickness=1,\n",
    "    )\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同一片CME区域在前后帧上的形态应当接近，同时所在的位置应当接近，面积也应当相近。因此CostMat不光需要有形状相似项，还应当有位置相似项和面积相似项。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算轮廓线距离函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchPosition(cntA: np.ndarray, cntB: np.ndarray) -> float:\n",
    "    '''计算两个轮廓线之间的距离\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cnaA : np.ndarray\n",
    "        轮廓线A\n",
    "    cntB : np.array\n",
    "        轮廓线B\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        两个轮廓线之间的距离\n",
    "    '''\n",
    "    ma = cv2.moments(cntA)\n",
    "    mb = cv2.moments(cntB)\n",
    "    Gcentera = np.array(\n",
    "        [int(ma['m10'] / ma['m00']),\n",
    "         int(ma['m01'] / ma['m00'])])\n",
    "    Gcenterb = np.array(\n",
    "        [int(mb['m10'] / mb['m00']),\n",
    "         int(mb['m01'] / mb['m00'])])\n",
    "    # 余弦距离\n",
    "    # return np.dot(Gcentera, Gcenterb) / (np.linalg.norm(Gcentera) * np.linalg.norm(Gcenterb))\n",
    "    # 加权欧几里得距离\n",
    "    # X = np.vstack([Gcentera, Gcenterb])\n",
    "    # sigma = np.var(X, axis=0, ddof=1)\n",
    "    # return np.sqrt(((Gcentera - Gcenterb)**2 / sigma).sum())\n",
    "    # 兰氏距离\n",
    "    d = 0\n",
    "    for i in range(len(Gcentera)):\n",
    "        if Gcentera[i] == 0 and Gcenterb[i] == 0:\n",
    "            d += 0\n",
    "        else:\n",
    "            d += abs(Gcentera[i] - Gcenterb[i]) / (abs(Gcentera[i]) +\n",
    "                                                   abs(Gcenterb[i]))\n",
    "    return d\n",
    "\n",
    "\n",
    "def matchArea(cntA: np.ndarray, cntB: np.ndarray) -> float:\n",
    "    '''计算轮廓线之间面积的差异\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    cntB : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        两个轮廓线面积的差值绝对数除以面积之和\n",
    "    '''\n",
    "    areaA = cv2.contourArea(cntA)\n",
    "    areaB = cv2.contourArea(cntB)\n",
    "    return abs(areaA - areaB) / (areaA + areaB)\n",
    "\n",
    "\n",
    "def matchContour(cntA: np.ndarray,\n",
    "                 cntB: np.ndarray,\n",
    "                 alpha: float = 1,\n",
    "                 beta: float = 1,\n",
    "                 gamma: float = 1) -> float:\n",
    "    '''加权计算两个轮廓线之间的相似度，包括形状相似度，位置相似度，面积相似度。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    cntB : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    alpha : float, optional\n",
    "        形状相似度系数, by default 1\n",
    "    beta : float, optional\n",
    "        位置相似度系数, by default 1\n",
    "    gamma : float, optional\n",
    "        面积相似度系数, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        综合考虑形态、位置、面积相似度的结果\n",
    "    '''\n",
    "    a = alpha * cv2.matchShapes(cntA, cntB, cv2.CONTOURS_MATCH_I3,\n",
    "                                0)  # 计算两帧之间不同CME区域的形态相似度\n",
    "    b = beta * matchPosition(cntA, cntB)  # 计算位置相似度\n",
    "    c = gamma * matchArea(cntA, cntB)  # 计算面积相似度\n",
    "    return a + b + c\n",
    "\n",
    "\n",
    "def measureSimilarity(contoursA: List[np.ndarray],\n",
    "                      contoursB: List[np.ndarray],\n",
    "                      alpha: float = 1,\n",
    "                      beta: float = 1,\n",
    "                      gamma: float = 1) -> np.ndarray:\n",
    "    '''两两成对计算列表中的轮廓线的相似度，返回一个相似度矩阵\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : List[np.ndarray]\n",
    "        包含轮廓线的列表\n",
    "    cntB : List[np.ndarray]\n",
    "        包含轮廓线的列表\n",
    "    alpha : float, optional\n",
    "        形状相似度系数, by default 1\n",
    "    beta : float, optional\n",
    "        位置相似度系数, by default 1\n",
    "    gamma : float, optional\n",
    "        面积相似度系数, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        两个轮廓线列表中的轮廓线成对计算的相似度矩阵，形状为len(contoursA), len(contoursB)\n",
    "    '''\n",
    "    similar = np.zeros((len(contoursA), len(contoursB)))\n",
    "    for i in range(len(contoursA)):\n",
    "        for j in range(len(contoursB)):\n",
    "            similar[i, j] = matchContour(contoursA[i], contoursB[j], alpha,\n",
    "                                         beta, gamma)\n",
    "    return similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour和Frame类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class Contour:\n",
    "    '''每一个轮廓线所属的类\n",
    "    '''\n",
    "    def __init__(self, contourArray: np.ndarray, binaryProjectMap: np.ndarray,\n",
    "                 contourIndex: int, binaryProjectMapIndex: int):\n",
    "        '''表示轮廓线的类\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        contourArray : np.ndarray\n",
    "            构成轮廓线的点所组成的数组，形状为(N,2)\n",
    "        binaryProjectMap : np.ndarray\n",
    "            该轮廓线来源的二值共定位图\n",
    "        contourIndex: int\n",
    "            该轮廓线在二值共定位图所有的轮廓线中的索引\n",
    "        binaryProjectMapIndex : int\n",
    "            二值共定位图在原共定位图数组中的索引\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        array : np.ndarray\n",
    "            轮廓线的点构成的数组，包含了轮廓点的x,y坐标，坐标系原点在左上角\n",
    "        binaryProjectMap : np.ndarray\n",
    "            该轮廓线所属的二值共定位图\n",
    "        size  : int\n",
    "            轮廓线点的个数\n",
    "        contourIndex : int\n",
    "            该轮廓线在二值共定位图所有的轮廓线中的索引\n",
    "        binaryProjectMapIndex : int\n",
    "            二值共定位图在原共定位图数组中的索引\n",
    "        '''\n",
    "        self.array: np.ndarray = contourArray\n",
    "        self.binaryProjectMap = binaryProjectMap\n",
    "        self.size = self.array.shape[0]\n",
    "        self.contourIndex = contourIndex\n",
    "        self.binaryProjectMapIndex = binaryProjectMapIndex\n",
    "\n",
    "    def center(self):\n",
    "        return self.array.mean(axis=0).astype('uint')\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmtstr = f'{self.__class__.__name__}(binaryMapIndex:{self.binaryProjectMapIndex},contourIndex:{self.contourIndex},Points:{self.size})'\n",
    "        return fmtstr\n",
    "\n",
    "    def pixelToDegree(self, xCord: int) -> float:\n",
    "        '''假定图像最左侧为0度，最右侧为360，将位置横坐标转化为角度\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xCord : int\n",
    "            位置横坐标\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            横坐标对应的角度值\n",
    "        '''\n",
    "        kangle = 360 / self.binaryProjectMap.shape[1]\n",
    "        return int(kangle * xCord)\n",
    "\n",
    "    def getPositionalAngel(self) -> List:\n",
    "        '''返回轮廓线左右两侧的位置角,以图像北侧为极轴，逆时针方向测量角度\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            左侧和右侧位置角构成的数组\n",
    "        '''\n",
    "        xLeft = self.array.min(axis=0)[0]  # 轮廓线最左侧横坐标\n",
    "        xRight = self.array.max(axis=0)[0]  # 轮廓线最右侧横坐标\n",
    "        xLeft = self.pixelToDegree(xLeft)\n",
    "        xRight = self.pixelToDegree(xRight)\n",
    "        return [xLeft, xRight]\n",
    "\n",
    "    def getCentralPositionAngel(self) -> float:\n",
    "        '''获取CME轮廓线中央位置角度，以图像北侧为极轴，逆时针方向测量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            该CME轮廓线的中央位置角\n",
    "        '''\n",
    "        CPA = sum(self.getPositionalAngel()) / 2\n",
    "        return CPA\n",
    "\n",
    "    def getAngularWidth(self):\n",
    "        startEndPositionalAngel = self.getPositionalAngel()\n",
    "        return abs(startEndPositionalAngel[0] - startEndPositionalAngel[1])\n",
    "\n",
    "    def getContourHeight(self, FOV: float) -> float:\n",
    "        '''得到轮廓线最顶端距离图片最底部端(太阳表面)的实际距离\n",
    "\n",
    "        按照公式y=-(FOV/H)*y+FOV计算，其中H是图片高度，y是顶点坐标\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        FOV : float\n",
    "            望远镜的视野大小，表示图片的最底端到最顶端实际的距离\n",
    "            (LASCO C2望远镜视野大小为6倍太阳半径)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            轮廓线最顶端距离图片最底部端(太阳表面)的实际距离\n",
    "        '''\n",
    "        upCoordinate = self.boundingRect()[2]  # 轮廓线最上方点的纵坐标（左上角为原点）\n",
    "        return -FOV / self.binaryProjectMap.shape[0] * upCoordinate + FOV\n",
    "\n",
    "    def boundingRect(self) -> List:\n",
    "        '''返回轮廓线内接边框上下左右的坐标\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            内接边框的左、右、上、下的坐标值\n",
    "        '''\n",
    "        x, y, w, h = cv2.boundingRect(self.array)  # 边框左下角的x,y坐标，框的宽和高\n",
    "        xLeft, xRight, yUp, yDown = x, x + w - 1, y, y + h - 1\n",
    "        return [xLeft, xRight, yUp, yDown]\n",
    "\n",
    "    def toArray(self):\n",
    "        return self.array\n",
    "\n",
    "    def getDistanceTo(self, cnt: Contour, alpha=1, beta=1, gamma=1) -> float:\n",
    "        '''加权计算两个轮廓线之间的相似度，包括形状相似度，位置相似度，面积相似度。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cnt : Contour\n",
    "            需要计算的轮廓线\n",
    "        alpha : float, optional\n",
    "            形状相似度系数, by default 1\n",
    "        beta : float, optional\n",
    "            位置相似度系数, by default 1\n",
    "        gamma : float, optional\n",
    "            面积相似度系数, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            综合考虑形态、位置、面积相似度的结果\n",
    "        '''\n",
    "        return matchContour(self.array, cnt.array, alpha, beta, gamma)\n",
    "\n",
    "    def drawContour(self, color=(255, 127, 127)) -> np.ndarray:\n",
    "        '''在图上画出轮廓线，返回图像数组\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        color : tuple, optional\n",
    "            画出轮廓线的颜色，为RGB元组, by default (255, 127, 127)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            表示图像的数组\n",
    "        '''\n",
    "        img = cv2.merge((self.binaryProjectMap, self.binaryProjectMap,\n",
    "                         self.binaryProjectMap))\n",
    "        cv2.drawContours(\n",
    "            img,\n",
    "            (self.array, ),\n",
    "            contourIdx=0,\n",
    "            color=color,\n",
    "            thickness=cv2.FILLED,\n",
    "        )\n",
    "        return img\n",
    "\n",
    "\n",
    "class ConnectedContour(Contour):\n",
    "    '''用以描述被视为一个整体的两个原本分开的轮廓线\n",
    "    '''\n",
    "    def __init__(self, cntA: Contour, cntB: Contour):\n",
    "        connectedArray = np.concatenate((cntA.array, cntB.array), axis=0)\n",
    "        if cntA.binaryProjectMapIndex != cntB.binaryProjectMapIndex:\n",
    "            raise ValueError(\n",
    "                'Contours connected should be in the SAME binary map')\n",
    "        xa, ya, wa, ha = cv2.boundingRect(cntA.array)\n",
    "        xb, yb, wb, hb = cv2.boundingRect(cntB.array)\n",
    "        # 要求组合为整体的两个轮廓线必须是被极坐标转换而分开的\n",
    "        # 这要求左侧的轮廓线最左侧坐标为0，右侧轮廓线最右侧坐标为360\n",
    "        if (xa != 0 or xb + wb != 360) and (xb != 0 and xa + wa != 360):\n",
    "            raise ValueError(\n",
    "                'Contours are not seprated by polar transformation,xa,wa,xb,wb={},{},{},{}'\n",
    "                .format(xa, wa, xb, wb))\n",
    "        super().__init__(connectedArray, cntA.binaryProjectMap,\n",
    "                         cntA.contourIndex, cntA.binaryProjectMapIndex)\n",
    "        self._cntA = cntA\n",
    "        self._cntB = cntB\n",
    "\n",
    "    def getPositionalAngel(self) -> List:\n",
    "        '''返回组合轮廓线的位置角,返回值为[0,a,b,359]的形式\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            由两个子轮廓线左侧和右侧位置角构成的数组，分别为\n",
    "            [0,cntARight,cntBLeft,359]\n",
    "        '''\n",
    "        cntAPA = self._cntA.getPositionalAngel()\n",
    "        cntBPA = self._cntB.getPositionalAngel()\n",
    "        if 0 in cntAPA:\n",
    "            PA = cntAPA + cntBPA\n",
    "        else:\n",
    "            PA = cntBPA + cntAPA\n",
    "\n",
    "        return PA\n",
    "\n",
    "    def getCentralPositionAngel(self) -> float:\n",
    "        '''获取CME轮廓线中央位置角度\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        isWarpToNorth : bool\n",
    "            若为True，则将角度转化为以图像北侧为0度，逆时针方式测量的值\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            该CME轮廓线的中央位置角\n",
    "        '''\n",
    "        cntARight = self.getPositionalAngel()[1]\n",
    "        cntBLeft = self.getPositionalAngel()[2]\n",
    "        CPA = int(((cntARight + cntBLeft) / 2 + 180) % 360)\n",
    "        return CPA\n",
    "\n",
    "    def getAngularWidth(self) -> int:\n",
    "        '''返回角宽度\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            角宽度\n",
    "        '''\n",
    "        cntARight = self.getPositionalAngel()[1]\n",
    "        cntBLeft = self.getPositionalAngel()[2]\n",
    "        return 360 - cntBLeft + cntARight\n",
    "\n",
    "\n",
    "class Frame:\n",
    "    '''每一个二值图像所属的类，可以生成、管理多个轮廓线Contour对象\n",
    "    '''\n",
    "    def __init__(self, binaryProjectMap: np.ndarray, frameInd: int):\n",
    "        '''初始化一个Frame类\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMap : np.ndarray\n",
    "            二值化的共定位图，形状为HW\n",
    "        frameInd : int\n",
    "            帧序号，表明该帧在原数组中的序号\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        contourList : List[Contour]\n",
    "            轮廓线对象构成的列表\n",
    "        binaryProjectMap : np.ndarry\n",
    "            用以生成轮廓线的二值化的共定位图，形状为HW\n",
    "        size : int\n",
    "            Frame对象所包含的轮廓线的个数\n",
    "        frameInd : int\n",
    "            帧序号，表明该帧在原数组中的序号\n",
    "        '''\n",
    "        self.contourList = self.makeContours(binaryProjectMap, frameInd)\n",
    "        self.binaryProjectMap = binaryProjectMap\n",
    "        self.size = len(self.contourList)\n",
    "        self.frameInd = frameInd\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        fmtstr = f'{self.__class__.__name__}(Ind {self.frameInd},Contour size:'\n",
    "        for cnt in self.contourList:\n",
    "            fmtstr += str(cnt.size) + ','\n",
    "        fmtstr += ')'\n",
    "        return fmtstr\n",
    "\n",
    "    def __getitem__(self, key) -> Contour:\n",
    "        return self.contourList[key]\n",
    "\n",
    "    def toArray(self) -> List[np.ndarray]:\n",
    "        '''返回所有的轮廓线点数组构成的列表\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[np.ndarray]\n",
    "            所有的轮廓线点数组构成的列表\n",
    "        '''\n",
    "        arrs = []\n",
    "        for cnt in self.contourList:\n",
    "            arrs.append(cnt.array)\n",
    "        return arrs\n",
    "\n",
    "    @staticmethod\n",
    "    def makeContours(binaryProjectMap: np.ndarray,\n",
    "                     frameInd: int) -> List[Contour]:\n",
    "        '''由二值化的共定位图中得到轮廓线，再进行排序，过滤等步骤生成Contour的列表\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMap : np.ndarray\n",
    "            二值化的共定位图，形状为HW\n",
    "        frameind : int\n",
    "            二值共定位图在原共定位图数组中的索引\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Contour]\n",
    "            包含有Contour对象的列表\n",
    "        '''\n",
    "        contours, hierarchy = cv2.findContours(\n",
    "            np.copy(binaryProjectMap).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_NONE)\n",
    "        contours = sorted(list(contours),\n",
    "                          key=lambda x: x.shape[0],\n",
    "                          reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "        contoursPointThre = int(contours[0].shape[0] /\n",
    "                                5)  # 过滤阈值为最大contour点数的1/5\n",
    "        contours = list(\n",
    "            filter(lambda x: x.shape[0] > contoursPointThre,\n",
    "                   contours))  # 过滤掉点数小于contoursPointThre的轮廓线\n",
    "        contourList: List[Contour] = []\n",
    "        for i in range(len(contours)):\n",
    "            contourList.append(\n",
    "                Contour(contours[i].squeeze(), binaryProjectMap, i, frameInd))\n",
    "        # *试验性质的添加ConnnectedContours\n",
    "        # 逐个两两成对判断轮廓线是否是由一个轮廓线被极坐标转换分隔开来的\n",
    "        # connectedContours: List[Tuple] = []\n",
    "        # for i in range(len(contourList)):\n",
    "        #     for j in range(len(contourList)):\n",
    "        #         # 需要判断connectedContours中的轮廓线是否出现重复\n",
    "        #         # 如果出现重复，则应当跳至下一对轮廓线\n",
    "        #         isDumplicated = False\n",
    "        #         for ind in range(len(connectedContours)):\n",
    "        #             if contourList[i] in connectedContours[ind]:\n",
    "        #                 isDumplicated = True\n",
    "        #                 break\n",
    "        #             if contourList[j] in connectedContours[ind]:\n",
    "        #                 isDumplicated = True\n",
    "        #                 break\n",
    "        #         if isDumplicated:\n",
    "        #             continue\n",
    "        #         if contourList[i].boundingRect(\n",
    "        #         )[0] == 0 and contourList[j].boundingRect()[1] == 359:\n",
    "        #             connectedContours.append((contourList[i], contourList[j]))\n",
    "        # # 将轮廓线组合起来形成新的轮廓线，并从contourList删除原来的\n",
    "        # for cnta, cntb in connectedContours:\n",
    "        #     connectedContour = ConnectedContour(cnta, cntb)\n",
    "        #     contourList.append(connectedContour)\n",
    "        #     contourList.remove(cnta)\n",
    "        #     contourList.remove(cntb)\n",
    "\n",
    "        return contourList\n",
    "\n",
    "    def linearAssign(self,\n",
    "                     frm,\n",
    "                     alpha: float = 1,\n",
    "                     beta: float = 1,\n",
    "                     gamma: float = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        '''与另一个Frame对象的所有轮廓线进行匹配，解决线性和分配问题\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frm : Frame\n",
    "            另一个包含轮廓线的帧对象\n",
    "        alpha : float, optional\n",
    "            形状相似度系数, by default 1\n",
    "        beta : float, optional\n",
    "            位置相似度系数, by default 1\n",
    "        gamma : float, optional\n",
    "            面积相似度系数, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            给出最佳匹配结果的元组，第一个是行序号构成的数组，第二个是列序号构成的数组\n",
    "        '''\n",
    "        cost = self.getDistanceTo(frm, alpha, beta, gamma)\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        rowInd, colInd = linear_sum_assignment(cost)\n",
    "        return rowInd, colInd\n",
    "\n",
    "    def getDistanceTo(self,\n",
    "                      frm,\n",
    "                      alpha: float = 1,\n",
    "                      beta: float = 1,\n",
    "                      gamma: float = 1) -> np.ndarray:\n",
    "        '''与另一个Frame对象的所有轮廓线两两计算相似度\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frm : Frame\n",
    "            另一个包含轮廓线的帧对象\n",
    "        alpha : float, optional\n",
    "            形状相似度系数, by default 1\n",
    "        beta : float, optional\n",
    "            位置相似度系数, by default 1\n",
    "        gamma : float, optional\n",
    "            面积相似度系数, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            两个Frame对象所包含的轮廓线两两之间的相似度，形状为(self.size, frm.size)\n",
    "        '''\n",
    "        cost = np.zeros((self.size, frm.size))\n",
    "        for i in range(self.size):\n",
    "            for j in range(frm.size):\n",
    "                cost[i, j] = matchContour(self.contourList[i].array,\n",
    "                                          frm.contourList[j].array, alpha,\n",
    "                                          beta, gamma)\n",
    "        return cost\n",
    "\n",
    "    def drawFrame(self, color=(255, 127, 127)) -> np.ndarray:\n",
    "        '''返回一个数组，可用于绘制出该Frame对象中所有的轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        color : tuple, optional\n",
    "            绘制轮廓线的颜色, by default (255, 127, 127)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            可用于直接成图的数组\n",
    "        '''\n",
    "        img = cv2.merge(\n",
    "            (np.copy(self.binaryProjectMap), np.copy(self.binaryProjectMap),\n",
    "             np.copy(self.binaryProjectMap)))\n",
    "        for i in range(len(self.contourList)):\n",
    "            # orgx, orgy = contoursNext[i].squeeze(\n",
    "            # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "            orgx, orgy = int(self.contourList[i].center()[0]), int(\n",
    "                self.contourList[i].center()[1])\n",
    "            cv2.putText(img,\n",
    "                        str(i),\n",
    "                        org=(orgx, orgy),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=1,\n",
    "                        color=(14, 173, 238),\n",
    "                        thickness=2)\n",
    "            cv2.drawContours(\n",
    "                img,\n",
    "                self.toArray(),\n",
    "                contourIdx=i,\n",
    "                color=color,\n",
    "                thickness=1,\n",
    "            )\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用改进的距离度量方法\n",
    "import pandas as pd\n",
    "\n",
    "CostMat = measureSimilarity(contoursNext, contours)\n",
    "\n",
    "indicies = [f'Next {i}' for i in range(len(contoursNext))]\n",
    "cols = [f'Curr {i}' for i in range(len(contours))]\n",
    "df = pd.DataFrame(CostMat, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(CostMat, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(CostMat)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))\n",
    "\n",
    "# 找出未配对成功的track和detection\n",
    "trackIndicies = np.arange(len(contoursNext))\n",
    "detectionIndicies = np.arange(len(contours))\n",
    "rowInd, colInd = linear_sum_assignment(CostMat)\n",
    "matches, unmatchedTracks, unmatchedDetections = [], [], []\n",
    "for col, detectionIdx in enumerate(detectionIndicies):\n",
    "    if col not in colInd:\n",
    "        unmatchedDetections.append(detectionIdx)\n",
    "for row, trackIdx in enumerate(trackIndicies):\n",
    "    if row not in rowInd:\n",
    "        unmatchedTracks.append(trackIdx)\n",
    "for row, col in zip(rowInd, colInd):\n",
    "    trackIdx = trackIndicies[row]\n",
    "    detectionIdx = detectionIndicies[col]\n",
    "    if CostMat[row, col] > 2:\n",
    "        unmatchedTracks.append(trackIdx)\n",
    "        unmatchedDetections.append(detectionIdx)\n",
    "    else:\n",
    "        matches.append((trackIdx, detectionIdx))\n",
    "print('matches', matches)\n",
    "print('unmatchedTracks', unmatchedTracks)\n",
    "print('unmatchedDetections', unmatchedDetections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用原本的形态相似度量方法\n",
    "import pandas as pd\n",
    "\n",
    "CostMat = np.zeros((len(contoursNext), len(contours)))\n",
    "for i in range(len(contoursNext)):\n",
    "    for j in range(len(contours)):\n",
    "        CostMat[i, j] = cv2.matchShapes(contoursNext[i], contours[j],\n",
    "                                        cv2.CONTOURS_MATCH_I3,\n",
    "                                        0)  # 计算两帧之间不同CME区域的形态相似度\n",
    "# CostMat = measureSimilarity(contoursNext, contours)\n",
    "\n",
    "indicies = [f'Next {i}' for i in range(len(contoursNext))]\n",
    "cols = [f'Curr {i}' for i in range(len(contours))]\n",
    "df = pd.DataFrame(CostMat, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(CostMat, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(CostMat)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame类和Contour类测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "fnext = Frame(np.copy(binaryMap_polar[ind + 1]).astype('uint8'), ind + 1)\n",
    "f = Frame(np.copy(binaryMap_polar[ind]).astype('uint8'), ind)\n",
    "print(list(map(lambda x: x.size, fnext.contourList)))\n",
    "print(list(map(lambda x: x.size, f.contourList)))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'Next Frame {ind+1} :total {fnext.size} contours')\n",
    "plt.imshow(fnext.drawFrame())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'Curr Frame {ind} :total {f.size} contours')\n",
    "plt.imshow(f.drawFrame())\n",
    "row, col = fnext.linearAssign(f)\n",
    "for i, j in zip(row, col):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))\n",
    "\n",
    "dis = fnext.getDistanceTo(f)\n",
    "indicies = [f'Next {i}' for i in range(fnext.size)]\n",
    "cols = [f'Curr {i}' for i in range(f.size)]\n",
    "df = pd.DataFrame(dis, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(dis, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轮廓线追踪类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "# from __future__ import annotations\n",
    "\n",
    "\n",
    "class TrackState(Enum):\n",
    "    confirmed = 1\n",
    "    deleted = 2\n",
    "\n",
    "\n",
    "class Track:\n",
    "    '''表示一个已经被追踪到的连续变化的CME Contour轮廓线\n",
    "    '''\n",
    "    def __init__(self, ID: int, frameIndex: int, contour: Contour,\n",
    "                 maxAge: int) -> None:\n",
    "        '''表示一个已经被追踪到的连续变化的CME Contour轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ID : int\n",
    "            每一个Track独一无二的身份号\n",
    "        frameIndex : int\n",
    "            初始化Track时所使用的轮廓线出现的帧号\n",
    "        contour : Contour\n",
    "            轮廓线\n",
    "        maxAge : int\n",
    "            表示该Track从上次更新轮廓线到现在所允许的最大的帧数，\n",
    "            超过该数字应当标记其state为deleted\n",
    "\n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        ID : int\n",
    "            追踪的身份号，每一个Track的身份号都应该是独一无二的\n",
    "        age : int\n",
    "            表示该Track从第一次出现到现在所经历的帧数\n",
    "        _maxAge : int:\n",
    "            表示该Track从上次更新轮廓线到现在所允许的最大的帧数，\n",
    "            超过该数字应当标记其state为deleted\n",
    "        contourApperenceInd : List[int]\n",
    "            contour中的轮廓线所出现的frame索引\n",
    "        contour : List[Contour]\n",
    "            追踪到的轮廓线\n",
    "        timeSinceLastUpdate : int\n",
    "            自上次更新轮廓线到现在所经历的帧数\n",
    "        state : TrackState\n",
    "            Track的状态，delete或者confirm\n",
    "        '''\n",
    "        self.ID: int = ID\n",
    "        self.age: int = 1\n",
    "        self._maxAge = maxAge\n",
    "        self.contourApperenceInd: List[int] = [frameIndex]\n",
    "        self.contourList: List[Contour] = [contour]\n",
    "        self.timeSinceLastUpdate = 0\n",
    "        self.state: TrackState = TrackState.confirmed\n",
    "\n",
    "    def update(self, newContourInd: int, newContour: Contour) -> None:\n",
    "        '''增加新的被追踪到的轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        newContourInd : int\n",
    "            新的轮廓线的出现的所在帧号\n",
    "        newContour : Contour\n",
    "            新的轮廓线\n",
    "        '''\n",
    "        self.contourList.append(newContour)\n",
    "        self.contourApperenceInd.append(newContourInd)\n",
    "        self.timeSinceLastUpdate = 0\n",
    "        self.age += 1\n",
    "\n",
    "    def markMissed(self) -> None:\n",
    "        self.timeSinceLastUpdate += 1\n",
    "        if self.timeSinceLastUpdate > self._maxAge:\n",
    "            self.state = TrackState.deleted\n",
    "\n",
    "    def isDeleted(self) -> bool:\n",
    "        return self.state == TrackState.deleted\n",
    "\n",
    "    def isConfirm(self) -> bool:\n",
    "        return self.state == TrackState.confirmed\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        fmtstr = f'{self.__class__.__name__}(ID:{self.ID},Age:{self.age},State:{self.state.name},{len(self.contourList)} contours,Appear in{self.contourApperenceInd})'\n",
    "        return fmtstr\n",
    "\n",
    "    def getRecordsNum(self) -> int:\n",
    "        '''返回Track所跟踪到的轮廓线的数量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Track所包含的轮廓线的数量\n",
    "        '''\n",
    "        return len(self.contourList)\n",
    "\n",
    "    def drawTrack(self, cols: int = 5, title: Optional[str] = None) -> None:\n",
    "        from math import ceil\n",
    "        contourNums = len(self.contourList)\n",
    "        rows = ceil(contourNums / cols)\n",
    "        plt.figure(figsize=(cols * 5, rows * 5))\n",
    "        if not title:\n",
    "            plt.suptitle(title)\n",
    "        else:\n",
    "            plt.suptitle(f'Track ID {self.ID}')\n",
    "        ind = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ax = plt.subplot(rows, cols, ind + 1)\n",
    "                ax.set_title(\n",
    "                    f'Frame {self.contourApperenceInd[ind]} Contour Ind {self.contourList[ind].contourIndex}'\n",
    "                )\n",
    "                plt.imshow(self.contourList[ind].drawContour())\n",
    "                ind += 1\n",
    "                if ind >= contourNums:\n",
    "                    break\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    '''完成CME区域跟踪类\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 binaryProjectMaps: np.ndarray,\n",
    "                 maxDistance: float,\n",
    "                 maxAge: int = 0):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMaps : np.ndarray\n",
    "            用于进行CME区域跟踪的二值化共定位图，形状为NHW\n",
    "        maxDistance : float\n",
    "            所能允许的轮廓线之间的最大距离，超过该距离将被认为不匹配\n",
    "        maxAge : int, optional\n",
    "            track状态设置为deleted前所能容许最大track不更新的帧数, by default 0\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        binaryProjectMaps : np.ndarray\n",
    "            一次CME事件完整的二值化共定位图 形状为NHW\n",
    "        maxDistance : float\n",
    "            两个轮廓线之间相似度的最大允许值，若相似度大于此值则认为不匹配\n",
    "        tracks: List[Track]\n",
    "            跟踪到的Track构成的列表\n",
    "        maxAge : int\n",
    "            若track在超过maxAge个帧后仍未被更新，则会标记为丢失\n",
    "        '''\n",
    "        self.binaryProjectMaps: np.ndarray = binaryProjectMaps\n",
    "        self.maxDistance: float = maxDistance\n",
    "        self.tracks: List[Track] = []\n",
    "        self._nextID = 0\n",
    "        self.maxAge: int = maxAge\n",
    "\n",
    "    def _initiateTrack(self, frameIndex: int, contour: Contour) -> None:\n",
    "        '''利用Contour初始化一个Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frameIndex : int\n",
    "            Contour所在帧的编号\n",
    "        contour : Contour\n",
    "            新的轮廓线\n",
    "        '''\n",
    "        self.tracks.append(\n",
    "            Track(self._nextID, frameIndex, contour, self.maxAge))\n",
    "        self._nextID += 1\n",
    "\n",
    "    def _match(\n",
    "        self, nextFrame: Frame\n",
    "    ) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:\n",
    "        '''与新的一帧进行匹配，并返回匹配的tracks、未匹配的tracks、未匹配的detections的编号\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nextFrame : Frame\n",
    "            表示下一张图片的帧对象\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[Tuple[int, int]], List[int], List[int]]\n",
    "            返回一个包含以下三项的元组\n",
    "            - 包含匹配成功的track和detection的索引的列表\n",
    "            - 包含未匹配成功的tracks的索引列表\n",
    "            - 包含未匹配成功的detection的索引列表\n",
    "        '''\n",
    "        # 只有state为Track.Confirmed的track才能用于匹配\n",
    "        # 将被用于匹配的tracks的索引，将cost矩阵中的索引映射到在self.track中的索引\n",
    "        confirmedTrackIndicies = [\n",
    "            i for i, t in enumerate(self.tracks) if t.isConfirm()\n",
    "        ]\n",
    "        confirmedDetectionIndicies = np.arange(\n",
    "            nextFrame.size)  # 将被用于匹配的detections的索引（所有的detection都将被用于匹配）\n",
    "        cost = np.zeros((len(confirmedTrackIndicies),\n",
    "                         nextFrame.size))  #每一行代表track，每一列代表detection\n",
    "        for i in range(len(confirmedTrackIndicies)):\n",
    "            for j in range(nextFrame.size):\n",
    "                # 使用tracks中最新的那一个contour来计算相似度\n",
    "                cost[i, j] = self.tracks[\n",
    "                    confirmedTrackIndicies[i]].contourList[-1].getDistanceTo(\n",
    "                        nextFrame[j])\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        rowInd, colInd = linear_sum_assignment(cost)\n",
    "        # 从rowInd和ColInd中找出匹配成功和未被匹配的track和detection\n",
    "        matches, unmatchedTracks, unmatchedDetections = [], [], []\n",
    "        # * 由于实际用于匹配的tracks并不是所有的tracks\n",
    "        # * 因此cost中的某一个track在cost中的行索引(rowInd)并不等于在self.tracks中的索引(trackIdx)\n",
    "        # * 因此若col不在colInd中，则把它对应的trackIdx加入到未匹配的track中\n",
    "        for col, detectionIdx in enumerate(confirmedDetectionIndicies):\n",
    "            if col not in colInd:\n",
    "                unmatchedDetections.append(detectionIdx)\n",
    "        for row, trackIdx in enumerate(confirmedTrackIndicies):\n",
    "            if row not in rowInd:\n",
    "                unmatchedTracks.append(trackIdx)\n",
    "        for row, col in zip(rowInd, colInd):\n",
    "            trackIdx = confirmedTrackIndicies[row]\n",
    "            detectionIdx = confirmedDetectionIndicies[col]\n",
    "            if cost[row, col] > self.maxDistance:\n",
    "                unmatchedTracks.append(trackIdx)\n",
    "                unmatchedDetections.append(detectionIdx)\n",
    "            else:\n",
    "                matches.append((trackIdx, detectionIdx))\n",
    "        return matches, unmatchedTracks, unmatchedDetections\n",
    "\n",
    "    def update(self, nextFrame: Frame) -> None:\n",
    "        '''将现有的track列表推进更新至下一帧\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nextFrame : Frame\n",
    "            代表下一帧图像的Frame类\n",
    "        '''\n",
    "        if not self.tracks:  # 如果目前没有任何活动跟踪，则将从nextFrame新建\n",
    "            for cnt in nextFrame.contourList:\n",
    "                self._initiateTrack(nextFrame.frameInd, cnt)\n",
    "        else:\n",
    "            matches, unmatchedTracks, unmatchedDetections = self._match(\n",
    "                nextFrame)  # 用当前的track匹配下一帧中的轮廓线\n",
    "            for trackIdx, detectionIdx in matches:\n",
    "                # 对于匹配成功的track，更新状态\n",
    "                self.tracks[trackIdx].update(nextFrame.frameInd,\n",
    "                                             nextFrame[detectionIdx])\n",
    "            for trackIdx in unmatchedTracks:\n",
    "                # 对于匹配不成功的track，标记为丢失\n",
    "                self.tracks[trackIdx].markMissed()\n",
    "            for detectionIdx in unmatchedDetections:\n",
    "                # 对于未被匹配的轮廓线，则从直接新建Track\n",
    "                self._initiateTrack(nextFrame.frameInd,\n",
    "                                    nextFrame[detectionIdx])\n",
    "\n",
    "    def getTrackByID(self, ID: int) -> Track:\n",
    "        '''返回给定ID号的Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ID : int\n",
    "            ID号\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[Track]\n",
    "            符合给定ID号的Track\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            若没有track符合给定ID号则抛出异常\n",
    "        '''\n",
    "        for t in self.tracks:\n",
    "            if t.ID == ID:\n",
    "                return t\n",
    "        else:\n",
    "            raise ValueError('No tracks match given ID {ID}')\n",
    "\n",
    "    def getTrackByIndex(self, index: int) -> Track:\n",
    "        '''返回对应于tracks列表中索引的Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            索引，对应于tracks列表中的索引\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Track\n",
    "            在tracks列表中索引为index的Track\n",
    "        '''\n",
    "        return self.tracks[index]\n",
    "\n",
    "    def drawImg(self, cols=5) -> None:\n",
    "        '''绘制出tracker追踪的轮廓线所属的二值图像\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : int, optional\n",
    "            绘制的图像中每一列子图个数, by default 5\n",
    "        '''\n",
    "        from math import ceil\n",
    "        nums = self.binaryProjectMaps.shape[0]\n",
    "        rows = ceil(nums / cols)\n",
    "        plt.figure(figsize=(cols * 5, rows * 5))\n",
    "        ind = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ax = plt.subplot(rows, cols, ind + 1)\n",
    "                ax.set_title(f'Frame {ind}')\n",
    "                # plt.imshow(self.binaryPeojectMaps[ind], cmap='gray')\n",
    "                plt.imshow(Frame(self.binaryProjectMaps[ind], ind).drawFrame())\n",
    "                ind += 1\n",
    "                if ind >= nums:\n",
    "                    break\n",
    "\n",
    "    def run(self, isSort: bool = True) -> None:\n",
    "        for i, binartProjectMap in enumerate(self.binaryProjectMaps):\n",
    "            nextFrm = Frame(binartProjectMap, i)\n",
    "            self.update(nextFrm)\n",
    "        if isSort:\n",
    "            self.tracks.sort(key=lambda x: x.getRecordsNum(), reverse=True)\n",
    "\n",
    "\n",
    "tracker = Tracker(binaryMap_polar, 1.6, 0)\n",
    "tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.drawImg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.getTrackByIndex(0).drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "frame = [Frame(binaryMap_polar[i], i) for i in range(binaryMap_polar.shape[0])]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(frame[0].drawFrame())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(frame[0].drawFrame())\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(frame[0][0].drawContour())\n",
    "# pd.DataFrame(frame[1].getDistanceTo(frame[2]))\n",
    "# plt.imshow(frame[0][0].show())\n",
    "kw = {'alpha': 1, 'beta': 0, 'gamma': 0}\n",
    "linear_sum_assignment(frame[0].getDistanceTo(frame[0], **kw))\n",
    "pd.DataFrame(frame[0].getDistanceTo(frame[0], **kw))\n",
    "frame[0][0].getDistanceTo(frame[0][0], **kw)\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# InteractiveShell.ast_node_interactivity = \"last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {'alpha': 1, 'beta': 1, 'gamma': 1}\n",
    "linear_sum_assignment(frame[9].getDistanceTo(frame[10], **kw))\n",
    "pd.DataFrame(frame[0].getDistanceTo(frame[0], **kw))\n",
    "frame[0][0].getDistanceTo(frame[0][0], **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = cv2.boundingRect(frame[5][0].array)  # 边框左下角的x,y坐标，框的宽和高\n",
    "print(x, x + w - 1, y, y + h - 1)\n",
    "frame[5][0].boundingRect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤Track列表，获得中央位置角、角宽度、高度\n",
    "\n",
    "Track需满足以下条件才会被纳入考虑：\n",
    "- 在连续两帧向外侧运动\n",
    "- 到达日冕仪视野外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTrackList(trkList: List[Track]) -> List[Track]:\n",
    "    '''从轨迹列表中筛选出满足条件的轨迹\n",
    "\n",
    "    依据的条件包括：\n",
    "    在连续两帧向外侧运动\n",
    "    到达日冕仪视野外\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trkList : List[Track]\n",
    "        包含有轨迹的列表\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Track]\n",
    "        满足条件的轨迹组成的列表\n",
    "    '''\n",
    "    satisfiedIndex = [True] * len(trkList)\n",
    "    for trkInd, trk in enumerate(trkList):\n",
    "        if trk.getRecordsNum() <= 3:\n",
    "            # track所跟踪的轮廓线必须超过3个\n",
    "            satisfiedIndex[trkInd] = False\n",
    "            continue\n",
    "        else:\n",
    "            # 判断条件1：至少有两帧是在向外运动是否被满足\n",
    "            ind = 1\n",
    "            count = 0  # 记录有多少帧是在向外运动的\n",
    "            # *打印高度以方便调试\n",
    "            print(\n",
    "                f'Track ID {trk.ID} Height:{list([cnt.boundingRect()[2] for cnt in trk.contourList])}'\n",
    "            )\n",
    "            while ind <= trk.getRecordsNum() - 1:\n",
    "                # *比较Track前一个轮廓线和后一个轮廓线的高度坐标\n",
    "                # *由于图像数组的原点在左上角，因此位置越高，坐标越小\n",
    "                # 考虑到前一帧和后一帧高度坐标相等的情况，只要高度坐标不为0也认为是满足条件的\n",
    "                if trk.contourList[ind].boundingRect()[2] <= trk.contourList[\n",
    "                        ind - 1].boundingRect(\n",
    "                        )[2] and trk.contourList[ind].boundingRect()[2] != 0:\n",
    "                    count += 1\n",
    "                    ind += 1\n",
    "                    if ind > trk.getRecordsNum() - 1:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            if count <= 2:\n",
    "                satisfiedIndex[trkInd] = False\n",
    "                print(f'Track ID {trk.ID}不满足条件1,count={count}')\n",
    "                continue\n",
    "            # 判断条件2：达到日冕仪视野外是否被满足\n",
    "            flag = False\n",
    "            for contour in trk.contourList:\n",
    "                # 当轮廓线达到最顶端，其坐标应当为0\n",
    "                if contour.boundingRect()[2] == 0:\n",
    "                    flag = True\n",
    "            else:\n",
    "                if not flag:\n",
    "                    satisfiedIndex[trkInd] = False\n",
    "                    print(f'Track ID {trk.ID}不满足条件2')\n",
    "    satisfiedTracks = [\n",
    "        trkList[i] for i, isValid in enumerate(satisfiedIndex) if isValid\n",
    "    ]\n",
    "\n",
    "    return satisfiedTracks\n",
    "\n",
    "\n",
    "filteredTrks = filterTrackList(tracker.tracks)\n",
    "heightsCord = [cnt.boundingRect()[2]\n",
    "               for cnt in filteredTrks[0].contourList]  # 轮廓线顶点坐标\n",
    "heights = np.array([\n",
    "    cnt.getContourHeight(6.2) for cnt in filteredTrks[0].contourList\n",
    "])  # 轮廓线顶点距离太阳表面的距离（单位为太阳半径）\n",
    "CPAs = np.array(\n",
    "    list(\n",
    "        map(lambda x: x.getCentralPositionAngel(),\n",
    "            filteredTrks[0].contourList)))\n",
    "AWs = np.array(\n",
    "    list(map(lambda x: x.getAngularWidth(), filteredTrks[0].contourList)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTrks[0].drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "CPAs\n",
    "AWs\n",
    "heights\n",
    "heightsCord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv和scikit-image极坐标转换的差别\n",
    "\n",
    "from skimage.transform import warp_polar, rotate\n",
    "\n",
    "# imgPolar = cv2.warpPolar(img, (360, 360), (258, 243), 240,\n",
    "#                          cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "\n",
    "arr = utils.loadImageFolder(PathCME20140106_1).astype('uint8')\n",
    "arr_polar = warpCartToPolar(arr.squeeze(), (360, 360), (258, 243), 245)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('origin')\n",
    "plt.imshow(arr[5][0], cmap='gray')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('opencv')\n",
    "plt.imshow(arr_polar[5], cmap='gray')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('scikit-image')\n",
    "plt.imshow(rotate(\n",
    "    warp_polar(arr[5, 0], (258, 243),\n",
    "               radius=240,\n",
    "               output_shape=(360, 360),\n",
    "               scaling='linear'), 90),\n",
    "           cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.concatenate((arr_polar[5], binaryMap_polar[5]), axis=1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(pic, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取每张CME轮廓线的观测时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from natsort import natsorted\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "def getTimeFromPicName(picName: str) -> datetime.datetime:\n",
    "    '''从形如20111122_205731_lasc2rdf_aia193rdf.png的文件名中提取图片时间\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    picName : str\n",
    "        图片文件名，格式类似20111122_205731_lasc2rdf_aia193rdf.png\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datetime.datetime\n",
    "        datetime.datetime对象\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        当解析时间的正则表达式无法解析picName时抛出\n",
    "    '''\n",
    "    pattern = r'^([\\d|_]{15})(?=_lasc\\drdf)'\n",
    "    timeExtracted = re.search(pattern, picName)\n",
    "    if not timeExtracted:\n",
    "        raise ValueError(f'pattern {pattern} can not match {picName}')\n",
    "    timeExtracted = timeExtracted.group()\n",
    "    timeExtracted = time.strptime(timeExtracted, \"%Y%m%d_%H%M%S\")\n",
    "    timeExtracted = datetime.datetime(*timeExtracted[:6])\n",
    "    return timeExtracted\n",
    "\n",
    "\n",
    "def getTimeFromPicNameList(picNameList: List[str], trk: Track) -> np.ndarray:\n",
    "    '''以图片文件名列表中的第一个文件名为时间起点，计算后续每一个时间的所经过的时间秒数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    picNameList : List[str]\n",
    "        图片文件名列表，形如20111122_205731_lasc2rdf_aia193rdf.png\n",
    "    trk : Track\n",
    "        追踪轮廓线的Track对象，以便判断轮廓线所对应的时间，以便计算高度-时间图\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        包含每一个时间点经过秒数的数组\n",
    "    '''\n",
    "    timeSinceLast = [0]\n",
    "    picNameList = natsorted(picNameList)  # 对文件名按照时间进行排序\n",
    "    picNameList = [\n",
    "        picNameList[cnt.binaryProjectMapIndex] for cnt in trk.contourList\n",
    "    ]  # 获取Track中所有Contour所对应的共定位图的文件名\n",
    "    beginning = getTimeFromPicName(picNameList[0])\n",
    "    i = 1\n",
    "    while i < len(picNameList):\n",
    "        currentTime = getTimeFromPicName(picNameList[i])\n",
    "        timeSinceLast.append((currentTime - beginning).seconds)\n",
    "        i += 1\n",
    "    return np.array(timeSinceLast)\n",
    "\n",
    "\n",
    "class CoronagraphType(Enum):\n",
    "    C2 = 'c2'\n",
    "    C3 = 'c3'\n",
    "\n",
    "\n",
    "class CoronagraphResult:\n",
    "    '''包装单个日冕仪图像识别结果的类，包含图像名称、时间、类型、高度、CPA、AW等量\n",
    "    '''\n",
    "    def __init__(self, imageName: str, time: datetime.datetime,\n",
    "                 tpe: CoronagraphType, height: float, CPA: float, AW: float,\n",
    "                 contour: Contour):\n",
    "        self.imageName = imageName\n",
    "        self.time_ = time\n",
    "        self.type_ = tpe\n",
    "        self.height = height\n",
    "        self.CPA = CPA\n",
    "        self.AW = AW\n",
    "        self.contour: Contour = contour\n",
    "\n",
    "    def __repr__(self):\n",
    "        string='Coronagraph('\n",
    "        string += str(self.time_)\n",
    "        string += (',type=C' + str(self.type_).strip(r'CoronagraphType.'))\n",
    "        string += ',height={:.2f},CPA={:.2f},AW={:.2f})'.format(self.height, self.CPA,\n",
    "                                                   self.AW)\n",
    "        return string\n",
    "\n",
    "\n",
    "def summrizeFromContourlist(\n",
    "        imageNames: List[str],\n",
    "        contourList: List[Contour]) -> List[CoronagraphResult]:\n",
    "    '''从包含有轮廓线的列表和日冕仪图像文件名中获取时间、日冕仪类型、轮廓线高度、中央位置角度等物理量，\n",
    "    并生成包含CoronagraphResult的列表\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    picNames : List[str]\n",
    "        日冕仪文件名(函数内会进行自然排序)\n",
    "    contourList : List[Contour]\n",
    "        包含有Contour的列表\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[CoronagraphResult]\n",
    "        包含日冕仪图像识别结果CoronagraphResult的列表\n",
    "    '''\n",
    "    coronagraphResults: List[CoronagraphResult] = []\n",
    "    imageNames = natsorted(imageNames)  # 对文件名按照时间进行排序\n",
    "    imageNames = [\n",
    "        imageNames[cnt.binaryProjectMapIndex] for cnt in contourList\n",
    "    ]  # 获取Track中所有Contour所对应的共定位图的文件名\n",
    "    for picName, cnt in zip(imageNames, contourList):\n",
    "        _time = getTimeFromPicName(picName)\n",
    "        if picName.find('c2') == -1:\n",
    "            _type = CoronagraphType.C3\n",
    "            _height = cnt.getContourHeight(30)\n",
    "        else:\n",
    "            _type = CoronagraphType.C2\n",
    "            _height = cnt.getContourHeight(6.2)\n",
    "        _CPA = cnt.getCentralPositionAngel()\n",
    "        _AW = cnt.getAngularWidth()\n",
    "        result = CoronagraphResult(picName, _time, _type, _height, _CPA, _AW,\n",
    "                                   cnt)\n",
    "        coronagraphResults.append(result)\n",
    "    return coronagraphResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据高度-时间图进行拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "picName = natsorted(os.listdir(PathCME20140106_1))\n",
    "xTime = getTimeFromPicNameList(picName, filteredTrks[0])\n",
    "Rsun = 696300\n",
    "# # 拟合SOHO 20111122目录中的数据\n",
    "# soho = np.array([2.49, 2.74, 2.90, 3.00, 3.24, 3.41, 3.79, 4.13, 4.66, 6.44])\n",
    "# print('SOHO目录的数据:')\n",
    "# LinearRegression().fit(xTime.reshape(-1, 1), soho * Rsun).coef_\n",
    "\n",
    "# 使用sklearn线性拟合高度-时间图\n",
    "reg = LinearRegression()\n",
    "reg.fit(xTime.reshape(-1, 1), Rsun * heights)\n",
    "print(f'线性拟合:y={reg.coef_[0]:.6f}x{reg.intercept_:+.6f}')\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(Rsun*heights,reg.predict(xTime.reshape(-1,1))))}'\n",
    ")\n",
    "print(f'R^2={reg.score(xTime.reshape(-1,1),Rsun*heights)}')\n",
    "\n",
    "# 进行二次拟合\n",
    "xTimeQuad = PolynomialFeatures(degree=2).fit_transform(xTime.reshape(-1, 1))\n",
    "regQuad = LinearRegression().fit(xTimeQuad, Rsun * heights)\n",
    "print(\n",
    "    f'二次拟合:y={regQuad.intercept_:.6f}{regQuad.coef_[0]:+.2f}{regQuad.coef_[1]:+.6f}x{regQuad.coef_[2]:+.6f}x**2'\n",
    ")\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(Rsun*heights,regQuad.predict(xTimeQuad)))}'\n",
    ")\n",
    "print(f'R^2={regQuad.score(xTimeQuad,Rsun*heights)}')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(xTime, heights * Rsun, label='Result')\n",
    "plt.plot(xTime, reg.predict(xTime.reshape(-1, 1)), label='Linear model')\n",
    "plt.plot(xTime, regQuad.predict(xTimeQuad), label='Quad model')\n",
    "textString = f'Width={AWs[-1]}\\n'\n",
    "textString += f'CPAs={CPAs[-1]}\\n'\n",
    "textString += f'Velocity={reg.coef_[0]:.3f}km/s\\n'\n",
    "textString += f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\n'\n",
    "textString += f'R$^2$={reg.score(xTime.reshape(-1,1),Rsun*heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(Rsun*heights,reg.predict(xTime.reshape(-1,1)))):.2f}\\n'\n",
    "textString += f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\n'\n",
    "textString += f'R$^2$={regQuad.score(xTimeQuad,Rsun*heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(Rsun*heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "plt.text(xTime[-4], heights[2] * Rsun, textString)\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2] * Rsun,\n",
    "#     f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\nR$^2$={reg.score(xTime.reshape(-1,1),Rsun*heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(Rsun*heights,reg.predict(xTime.reshape(-1,1)))):.2f}'\n",
    "# )\n",
    "# plt.text(\n",
    "#     xTime[-4], (heights[2] - 0.3) * Rsun,\n",
    "#     f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\nR$^2$={regQuad.score(xTimeQuad,Rsun*heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(Rsun*heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "# )\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Height(km)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "picName = natsorted(os.listdir(PathCME20140106_1))\n",
    "xTime = getTimeFromPicNameList(picName, filteredTrks[0])\n",
    "Rsun = 696300\n",
    "# # 拟合SOHO 20111122目录中的数据\n",
    "# soho = np.array([2.49, 2.74, 2.90, 3.00, 3.24, 3.41, 3.79, 4.13, 4.66, 6.44])\n",
    "# print('SOHO目录的数据:')\n",
    "# LinearRegression().fit(xTime.reshape(-1, 1), soho * Rsun).coef_\n",
    "\n",
    "# 使用sklearn线性拟合高度-时间图\n",
    "reg = LinearRegression()\n",
    "reg.fit(xTime.reshape(-1, 1), heights)\n",
    "print(f'线性拟合:y={reg.coef_[0]:.6f}x{reg.intercept_:+.6f}')\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1))))}'\n",
    ")\n",
    "print(f'R^2={reg.score(xTime.reshape(-1,1),heights)}')\n",
    "\n",
    "# 进行二次拟合\n",
    "xTimeQuad = PolynomialFeatures(degree=2).fit_transform(xTime.reshape(-1, 1))\n",
    "regQuad = LinearRegression().fit(xTimeQuad, heights)\n",
    "print(\n",
    "    f'二次拟合:y={regQuad.intercept_:.6f}{+-regQuad.coef_[0]:+.2f}{+-regQuad.coef_[1]:+.6f}x{+-regQuad.coef_[2]:+.6f}x**2'\n",
    ")\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad)))}')\n",
    "print(f'R^2={regQuad.score(xTimeQuad,heights)}')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(xTime, heights, label='Result')\n",
    "plt.plot(xTime, reg.predict(xTime.reshape(-1, 1)), label='Linear model')\n",
    "plt.plot(xTime, regQuad.predict(xTimeQuad), label='Quad model')\n",
    "textString = f'Width={AWs[-1]}\\n'\n",
    "textString += f'CPAs={CPAs[-1]}\\n'\n",
    "textString += f'velocity={reg.coef_[0]*Rsun:.3f}km/s\\n'\n",
    "textString += f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\n'\n",
    "textString += f'R$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}\\n'\n",
    "textString += f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\n'\n",
    "textString += f'R$^2$={regQuad.score(xTimeQuad,heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "plt.text(xTime[-4], heights[2], textString)\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2],\n",
    "#     f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\nR$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}'\n",
    "# )\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2] - 0.3,\n",
    "#     f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\nR$^2$={regQuad.score(xTimeQuad,heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "# )\n",
    "\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Height(Rsun)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整理CME识别、追踪的全过程\n",
    "\n",
    "包含\n",
    "1. 得到共定位图\n",
    "2. 获取最大连接分量\n",
    "3. 利用最大连接分量遮罩共定位图\n",
    "4. 利用otsu二值化将共定位图转换为二值图像\n",
    "5. 二值图像遮罩图像转换为极坐标图像\n",
    "6. 利用极坐标图像得到每一帧图像的轮廓线\n",
    "7. 度量CME轮廓线的相似度，使用匈牙利算法，匹配得到一系列的轮廓线，认为它们是同一个CME区域，称为一个Track\n",
    "8. 并依照达到日冕仪边缘、连续两帧向外运动的条件对结果进行过滤\n",
    "9. 得到中央位置角、角宽度、高度等物理参数\n",
    "10. 对高度-时间进行拟合获得CME速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import DDT\n",
    "import model.model_defination\n",
    "import utils\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "picPath = r'CME_data\\20140106_1'\n",
    "\n",
    "\n",
    "def divide255(x):\n",
    "    return x / 255\n",
    "\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "netParaPath = 'trainIssueLog/2022_10_13_22_40_16/parameters.pkl'\n",
    "net.load_param(netParaPath)\n",
    "\n",
    "trans = Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "imageArray = utils.loadImageFolder(picPath, trans)\n",
    "projectedMap = DDT.DDTThirdParty(imageArray, net)\n",
    "largestComp = DDT.getLargestComponent(projectedMap)\n",
    "maskedprojectedMap = utils.applyMaskOnImage(\n",
    "    projectedMap.squeeze(),  #共定位图使用最大连接分量遮罩后\n",
    "    largestComp)\n",
    "colorMapedImage = DDT.colorMapImage((imageArray * 255).astype('uint8'),\n",
    "                                    projectedMap)\n",
    "maskedthre, otsumaskedprojectedMap = otsuThreshold(  # 对遮罩的共定位图使用otsu算法\n",
    "    maskedprojectedMap.squeeze().astype('uint8'))\n",
    "binaryMap = otsumaskedprojectedMap\n",
    "binaryMapPolar = warpCartToPolar(binaryMap, (360, 360), (258, 243), 245)\n",
    "\n",
    "tracker = Tracker(binaryMapPolar, 1.6, 0)\n",
    "tracker.run()\n",
    "filteredTrks = filterTrackList(tracker.tracks)\n",
    "heightsCord = [cnt.boundingRect()[2]\n",
    "               for cnt in filteredTrks[0].contourList]  # 轮廓线顶点坐标\n",
    "heights = np.array([\n",
    "    cnt.getContourHeight(6.2) for cnt in filteredTrks[0].contourList\n",
    "])  # 轮廓线顶点距离太阳表面的距离（单位为太阳半径）\n",
    "CPAs = np.array(\n",
    "    list(\n",
    "        map(lambda x: x.getCentralPositionAngel(),\n",
    "            filteredTrks[0].contourList)))\n",
    "AWs = np.array(\n",
    "    list(map(lambda x: x.getAngularWidth(), filteredTrks[0].contourList)))\n",
    "print(f'有效的追踪轨迹共{len(filteredTrks)}个')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrayInFlat(largestComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.tracks[2].drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "picName = natsorted(os.listdir(picPath))\n",
    "xTime = getTimeFromPicNameList(picName, filteredTrks[0])\n",
    "Rsun = 696300\n",
    "# # 拟合SOHO 20111122目录中的数据\n",
    "# soho = np.array([2.49, 2.74, 2.90, 3.00, 3.24, 3.41, 3.79, 4.13, 4.66, 6.44])\n",
    "# print('SOHO目录的数据:')\n",
    "# LinearRegression().fit(xTime.reshape(-1, 1), soho * Rsun).coef_\n",
    "\n",
    "# 使用sklearn线性拟合高度-时间图\n",
    "reg = LinearRegression()\n",
    "reg.fit(xTime.reshape(-1, 1), heights)\n",
    "print(f'线性拟合:y={reg.coef_[0]:.6f}x{reg.intercept_:+.6f}')\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1))))}'\n",
    ")\n",
    "print(f'R^2={reg.score(xTime.reshape(-1,1),heights)}')\n",
    "\n",
    "# 进行二次拟合\n",
    "xTimeQuad = PolynomialFeatures(degree=2).fit_transform(xTime.reshape(-1, 1))\n",
    "regQuad = LinearRegression().fit(xTimeQuad, heights)\n",
    "print(\n",
    "    f'二次拟合:y={regQuad.intercept_:.6f}{+-regQuad.coef_[0]:+.2f}{+-regQuad.coef_[1]:+.6f}x{+-regQuad.coef_[2]:+.6f}x^2'\n",
    ")\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad)))}')\n",
    "print(f'R^2={regQuad.score(xTimeQuad,heights)}')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(xTime, heights, label='Result')\n",
    "plt.plot(xTime, reg.predict(xTime.reshape(-1, 1)), label='Linear model')\n",
    "plt.plot(xTime, regQuad.predict(xTimeQuad), label='Quad model')\n",
    "textString = f'Width={AWs[-1]}\\n'\n",
    "textString += f'CPAs={CPAs[-1]}\\n'\n",
    "textString += f'velocity={reg.coef_[0]*Rsun:.3f}km/s\\n'\n",
    "textString += f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\n'\n",
    "textString += f'R$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}\\n'\n",
    "textString += f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\n'\n",
    "textString += f'R$^2$={regQuad.score(xTimeQuad,heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "plt.text(xTime[-4], heights[2], textString)\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2],\n",
    "#     f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\nR$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}'\n",
    "# )\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2] - 0.3,\n",
    "#     f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\nR$^2$={regQuad.score(xTimeQuad,heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "# )\n",
    "\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Height(Rsun)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(heightsCord)\n",
    "print(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对Lasco C3的尝试"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import applyMaskOnImage\n",
    "from kornia.filters import box_blur\n",
    "\n",
    "PathCMEC3 = r'CME_data\\LascoC3\\20140602'\n",
    "\n",
    "trans = torchvision.transforms.Compose(\n",
    "    [divide255,\n",
    "     utils.CenterCrop('NCHW', circlePoint=(244, 256), radius=37)])\n",
    "# cropimageArray_C3 = box_blur(\n",
    "#     torch.from_numpy(utils.loadImageFolder(PathCMEC3, trans)), (3, 3)).numpy()\n",
    "cropimageArray_C3 = utils.loadImageFolder(PathCMEC3, trans)\n",
    "cropnetC3 = model.model_defination.LeNet5(size=224)\n",
    "cropnetC3.load_param(r'trainIssueLog\\2023_02_14_15_58_32\\parameters.pkl')\n",
    "projectedMapC3 = DDT.DDTThirdParty(cropimageArray_C3, cropnetC3)  # 共定位图\n",
    "colormapC3 = DDT.toColorMap(projectedMapC3)  # 共定位图着色\n",
    "# 255*cropimageArray20140106_1_new.astype('uint8')=255*(cropimageArray20140106_1_new.astype('uint8'))\n",
    "outputImageC3 = DDT.colorMapImage((cropimageArray_C3 * 255).astype('uint8'),\n",
    "                                  projectedMapC3)  # 共定位图叠加原图着色\n",
    "largestcompC3 = getLargestComponent(projectedMapC3)  # 共定位图最大连接分量\n",
    "maskedprojectedmapC3 = applyMaskOnImage(projectedMapC3.squeeze(),\n",
    "                                        largestcompC3)  # 共定位图最大连接部分\n",
    "maskedoutputImageC3 = DDT.colorMapImage(\n",
    "    (cropimageArray_C3 * 255).astype('uint8'),\n",
    "    maskedprojectedmapC3)  # 共定位图最大连接部分叠加原图着色\n",
    "threC3, otsuBinaryMapC3 = otsuThreshold(\n",
    "    projectedMapC3.squeeze().astype('uint8'))  # 共定位图otsu\n",
    "otsuCutC3 = maskImage(  # 共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray_C3).astype('uint8'),\n",
    "    (otsuBinaryMapC3 / 255).astype('uint8'))\n",
    "otsuthreC3, otsumaskedbinarymapC3 = otsuThreshold(\n",
    "    maskedprojectedmapC3.squeeze().astype('uint8'))  # 共定位图最大连接部分otsu\n",
    "otsumaskedCutC3 = maskImage(  # 最大连接遮罩共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray_C3).astype('uint8'),\n",
    "    (otsumaskedbinarymapC3 / 255).astype('uint8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'Original', 'projected map', 'projoected map > 0', 'otsu projected map',\n",
    "    'otsucut on original', 'largest comp', 'largest comp project map',\n",
    "    'otsu largestcomp project map', 'otsumaskedCut on original'\n",
    "]\n",
    "utils.drawImageArrays(cropimageArray_C3.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC3.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC3.transpose(0, 2, 3, 1) > 0,\n",
    "                      otsuBinaryMapC3,\n",
    "                      otsuCutC3,\n",
    "                      largestcompC3,\n",
    "                      maskedprojectedmapC3,\n",
    "                      otsumaskedbinarymapC3,\n",
    "                      otsumaskedCutC3,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'Original', 'ProjectMap', 'ProjectMap Colored',\n",
    "    'LargestComp', 'ProjectMap with LargestCompMask', 'otsu'\n",
    "]\n",
    "utils.drawImageArrays(cropimageArray_C3.transpose(0, 2, 3, 1),\n",
    "                      outputImageC3,\n",
    "                      colormapC3,\n",
    "                      largestcompC3[:, :, :, None],\n",
    "                      maskedoutputImageC3,\n",
    "                      otsumaskedCutC3,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跟踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryMapC3 = otsuBinaryMapC3\n",
    "binaryMapPolarC3 = warpCartToPolar(binaryMapC3, (360, 360), (256, 244), 245)\n",
    "trackerC3 = Tracker(binaryMapPolarC3, 1.6, 0)\n",
    "trackerC3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrayInFlat(binaryMapPolarC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerC3.getTrackByIndex(0).drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredTrks = filterTrackList(trackerC3.tracks)\n",
    "selectedTrack = trackerC3.getTrackByIndex(0)\n",
    "heightsCord = [cnt.boundingRect()[2]\n",
    "               for cnt in selectedTrack.contourList]  # 轮廓线顶点坐标\n",
    "heights = np.array([\n",
    "    cnt.getContourHeight(30) for cnt in selectedTrack.contourList\n",
    "])  # 轮廓线顶点距离太阳表面的距离（单位为太阳半径）\n",
    "CPAs = np.array(\n",
    "    list(\n",
    "        map(lambda x: x.getCentralPositionAngel(),\n",
    "            selectedTrack.contourList)))\n",
    "AWs = np.array(\n",
    "    list(map(lambda x: x.getAngularWidth(), selectedTrack.contourList)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picName = natsorted(os.listdir(PathCMEC3))\n",
    "xTime = getTimeFromPicNameList(picName, selectedTrack)\n",
    "Rsun = 696300\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(xTime.reshape(-1, 1), heights * Rsun)\n",
    "print(f'线性拟合:y={reg.coef_[0]:.6f}x{reg.intercept_:+.6f}')\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1))))}'\n",
    ")\n",
    "print(f'R^2={reg.score(xTime.reshape(-1,1),heights)}')\n",
    "\n",
    "# 进行二次拟合\n",
    "xTimeQuad = PolynomialFeatures(degree=2).fit_transform(xTime.reshape(-1, 1))\n",
    "regQuad = LinearRegression().fit(xTimeQuad, heights * Rsun)\n",
    "print(\n",
    "    f'二次拟合:y={regQuad.intercept_:.6f}{+-regQuad.coef_[0]:+.2f}{+-regQuad.coef_[1]:+.6f}x{+-regQuad.coef_[2]:+.6f}x^2'\n",
    ")\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad)))}')\n",
    "print(f'R^2={regQuad.score(xTimeQuad,heights)}')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(xTime, heights * Rsun, label='Result')\n",
    "plt.plot(xTime, reg.predict(xTime.reshape(-1, 1)), label='Linear model')\n",
    "plt.plot(xTime, regQuad.predict(xTimeQuad), label='Quad model')\n",
    "textString = f'Width={AWs[-1]}\\n'\n",
    "textString += f'CPAs={CPAs[-1]}\\n'\n",
    "textString += f'velocity={reg.coef_[0]*Rsun:.3f}km/s\\n'\n",
    "textString += f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\n'\n",
    "textString += f'R$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}\\n'\n",
    "textString += f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\n'\n",
    "textString += f'R$^2$={regQuad.score(xTimeQuad,heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "plt.text(xTime[-4], heights[2], textString)\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2],\n",
    "#     f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\nR$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}'\n",
    "# )\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2] - 0.3,\n",
    "#     f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\nR$^2$={regQuad.score(xTimeQuad,heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "# )\n",
    "\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Height(km)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并C2与C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagePath = 'CME_data/LascoC2C3/20140113'\n",
    "# start = datetime.datetime.strptime('2014/01/14 00:00:00', '%Y/%m/%d %H:%M:%S')\n",
    "# end = datetime.datetime.strptime('2014/01/14 07:18:07', '%Y/%m/%d %H:%M:%S')\n",
    "# utils.downloadLascoImageBetween(start, end, imagePath=imagePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.filters import box_blur\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "imagePath = r'CME_data\\LascoC2C3\\20150209'\n",
    "trans = Compose([divide255])\n",
    "images = utils.loadImageFolder(imagePath, trans)\n",
    "imageNames = natsorted(os.listdir(imagePath))\n",
    "imageArrayC2 = []\n",
    "imageArrayC3 = []\n",
    "imageNamesC2: List[str] = []\n",
    "imageNamesC3: List[str] = []\n",
    "for i, imageName in enumerate(imageNames):\n",
    "    if imageName.find('c2') != -1:\n",
    "        imageArrayC2.append(images[i])\n",
    "        imageNamesC2.append(imageName)\n",
    "    else:\n",
    "        imageArrayC3.append(images[i])\n",
    "        imageNamesC3.append(imageName)\n",
    "imageArrayC2 = utils.CenterCrop('NCHW')(\n",
    "    np.concatenate(imageArrayC2)[:, None, :, :])\n",
    "imageArrayC3 = box_blur(\n",
    "    torch.from_numpy(\n",
    "        utils.CenterCrop('NCHW', circlePoint=(244, 256),\n",
    "                         radius=37)(np.concatenate(imageArrayC3)[:,\n",
    "                                                                 None, :, :])),\n",
    "    (3, 3)).numpy()\n",
    "# imageArrayC3 = torch.from_numpy(\n",
    "#     utils.CenterCrop('NCHW', circlePoint=(244, 256),\n",
    "#                      radius=37)(np.concatenate(imageArrayC3)[:, None, :, :])).numpy()\n",
    "\n",
    "netC2 = model.model_defination.LeNet5()\n",
    "netC2.load_param(r'trainIssueLog/2022_10_13_22_40_16/parameters.pkl')\n",
    "netC3 = model.model_defination.LeNet5(size=224)\n",
    "netC3.load_param(r'trainIssueLog/2023_03_07_11_16_01/parameters.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedMapC2 = DDT.DDTThirdParty(imageArrayC2, netC2)  # 共定位图\n",
    "colormapC2 = DDT.toColorMap(projectedMapC2)  # 共定位图着色\n",
    "outputImageC2 = DDT.colorMapImage((imageArrayC2 * 255).astype('uint8'),\n",
    "                                  projectedMapC2)  # 共定位图叠加原图着色\n",
    "projectedMapC3 = DDT.DDTThirdParty(imageArrayC3, netC3)  # 共定位图\n",
    "colormapC3 = DDT.toColorMap(projectedMapC3)  # 共定位图着色\n",
    "outputImageC3 = DDT.colorMapImage((imageArrayC3 * 255).astype('uint8'),\n",
    "                                  projectedMapC3)  # 共定位图叠加原图着色\n",
    "\n",
    "largestcompC2 = DDT.getLargestComponent(projectedMapC2)  # 共定位图最大连接分量\n",
    "maskedprojectedmapC2 = applyMaskOnImage(projectedMapC2.squeeze(),\n",
    "                                        largestcompC2)  # 共定位图最大连接部分\n",
    "largestcompC3 = DDT.getLargestComponent(projectedMapC3)  # 共定位图最大连接分量\n",
    "maskedprojectedmapC3 = applyMaskOnImage(projectedMapC3.squeeze(),\n",
    "                                        largestcompC3)  # 共定位图最大连接部分\n",
    "threC2, otsuBinaryMapC2 = otsuThreshold(\n",
    "    projectedMapC2.squeeze().astype('uint8'))  # 共定位图otsu\n",
    "threC3, otsuBinaryMapC3 = otsuThreshold(\n",
    "    projectedMapC3.squeeze().astype('uint8'))  # 共定位图otsu\n",
    "otsuCutC2 = maskImage(  # 最大连接遮罩共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * imageArrayC2).astype('uint8'),\n",
    "    (otsuBinaryMapC2 / 255).astype('uint8'))\n",
    "otsuCutC3 = maskImage(  # 最大连接遮罩共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * imageArrayC3).astype('uint8'),\n",
    "    (otsuBinaryMapC3 / 255).astype('uint8'))\n",
    "threC2, otsumaskedBinaryMapC2 = otsuThreshold(\n",
    "    maskedprojectedmapC2.squeeze().astype('uint8'))  # 共定位图最大连接分量otsu\n",
    "threC3, otsumaskedBinaryMapC3 = otsuThreshold(\n",
    "    maskedprojectedmapC3.squeeze().astype('uint8'))  # 共定位图最大连接分量otsu\n",
    "otsumaskedCutC2 = maskImage(  # 最大连接遮罩共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * imageArrayC2).astype('uint8'),\n",
    "    (otsumaskedBinaryMapC2 / 255).astype('uint8'))\n",
    "otsumaskedCutC3 = maskImage(  # 最大连接遮罩共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * imageArrayC3).astype('uint8'),\n",
    "    (otsumaskedBinaryMapC3 / 255).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'original', 'projectmap', 'projectmap>0', 'otsu projectmap','otsuCut',\n",
    "    'largest comp', 'mask projected map', 'otsu largest projectmap','otsu masked Cut'\n",
    "]\n",
    "utils.drawImageArrays(imageArrayC2.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC2.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC2.transpose(0, 2, 3, 1) > 0,\n",
    "                      otsuBinaryMapC2,\n",
    "                      otsuCutC2,\n",
    "                      largestcompC2,\n",
    "                      maskedprojectedmapC2,\n",
    "                      otsumaskedBinaryMapC2,\n",
    "                      otsumaskedCutC2,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'original', 'projectmap', 'projectmap>0', 'otsu projectmap', 'otsuCut',\n",
    "    'largest comp', 'mask projected map', 'otsu largest projectmap',\n",
    "    'otsu masked Cut'\n",
    "]\n",
    "utils.drawImageArrays(imageArrayC3.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC3.transpose(0, 2, 3, 1),\n",
    "                      projectedMapC3.transpose(0, 2, 3, 1) > 0,\n",
    "                      otsuBinaryMapC3,\n",
    "                      otsuCutC3,\n",
    "                      largestcompC3,\n",
    "                      maskedprojectedmapC3,\n",
    "                      otsumaskedBinaryMapC3,\n",
    "                      otsumaskedCutC3,\n",
    "                      title=ttle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跟踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryMapC3 = otsuBinaryMapC3\n",
    "binaryMapPolarC3 = warpCartToPolar(binaryMapC3, (360, 360), (256, 244), 245)\n",
    "trackerC3 = Tracker(binaryMapPolarC3, 1.7, 0)\n",
    "trackerC3.run()\n",
    "binaryMapC2 = otsuBinaryMapC2\n",
    "binaryMapPolarC2 = warpCartToPolar(binaryMapC2, (360, 360), (258, 243), 245)\n",
    "trackerC2 = Tracker(binaryMapPolarC2, 1.7, 0)\n",
    "trackerC2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrayInFlat(binaryMapPolarC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrayInFlat(binaryMapPolarC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerC3.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerC3.getTrackByIndex(0).drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerC2.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerC2.getTrackByIndex(0).drawTrack()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤日冕仪识别结果\n",
    "def filterCoronagraphResultList(coronagraphResults: List[CoronagraphResult]):\n",
    "    filtedInd = []\n",
    "    for i in range(1, len(coronagraphResults) - 1):\n",
    "        # 若连续两个识别高度都小于前一个，则从该识别结果开始后的每一个都需要去掉\n",
    "        if coronagraphResults[i].height < coronagraphResults[\n",
    "                i - 1].height and coronagraphResults[\n",
    "                    i + 1].height < coronagraphResults[i-1].height:\n",
    "            filtedInd.extend(list(range(i, len(coronagraphResults))))\n",
    "        # 若某一个识别高度小于前后两个，则该识别结果需要去掉\n",
    "        if coronagraphResults[i].height < coronagraphResults[\n",
    "                i - 1].height and coronagraphResults[\n",
    "                    i + 1].height > coronagraphResults[i].height:\n",
    "            filtedInd.append(i)\n",
    "    filteredResults = [\n",
    "        result for i, result in enumerate(coronagraphResults)\n",
    "        if i not in filtedInd\n",
    "    ]\n",
    "    return filteredResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTrackIndC2 = 1\n",
    "selectedTrackIndC3 = 0\n",
    "selectedTrackC2 = trackerC2.getTrackByIndex(selectedTrackIndC2)\n",
    "heightsC2 = np.array([\n",
    "    cnt.getContourHeight(6.2) for cnt in selectedTrackC2.contourList\n",
    "])  # 轮廓线顶点距离太阳表面的距离（单位为太阳半径）\n",
    "CPAsC2 = np.array(\n",
    "    list(\n",
    "        map(lambda x: x.getCentralPositionAngel(),\n",
    "            selectedTrackC2.contourList)))\n",
    "AWsC2 = np.array(\n",
    "    list(map(lambda x: x.getAngularWidth(), selectedTrackC2.contourList)))\n",
    "\n",
    "selectedTrackC3 = trackerC3.getTrackByIndex(selectedTrackIndC3)\n",
    "heightsC3 = np.array([\n",
    "    cnt.getContourHeight(30) for cnt in selectedTrackC3.contourList\n",
    "])  # 轮廓线顶点距离太阳表面的距离（单位为太阳半径）\n",
    "CPAsC3 = np.array(\n",
    "    list(\n",
    "        map(lambda x: x.getCentralPositionAngel(),\n",
    "            selectedTrackC3.contourList)))\n",
    "AWsC3 = np.array(\n",
    "    list(map(lambda x: x.getAngularWidth(), selectedTrackC3.contourList)))\n",
    "\n",
    "print(heightsC2)\n",
    "print(heightsC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronagraphResults: List[CoronagraphResult] = []\n",
    "coronagraphResults.extend(\n",
    "    summrizeFromContourlist(imageNamesC2, selectedTrackC2.contourList))\n",
    "coronagraphResults.extend(\n",
    "    summrizeFromContourlist(imageNamesC3, selectedTrackC3.contourList))\n",
    "coronagraphResults.sort(key=lambda x: x.time_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(coronagraphResults):\n",
    "    print(i + 1, result)\n",
    "# coronagraphResults = filterCoronagraphResultList(coronagraphResults)\n",
    "# print(coronagraphResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsun = 696300\n",
    "xTime = [result.time_ for result in coronagraphResults]\n",
    "xTime = np.array([(t - xTime[0]).seconds for t in xTime])\n",
    "heights = np.array([result.height for result in coronagraphResults])\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(xTime.reshape(-1, 1), heights * Rsun)\n",
    "print(f'线性拟合:y={reg.coef_[0]:.6f}x{reg.intercept_:+.6f}')\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1))))}'\n",
    ")\n",
    "print(f'R^2={reg.score(xTime.reshape(-1,1),heights)}')\n",
    "\n",
    "# 进行二次拟合\n",
    "xTimeQuad = PolynomialFeatures(degree=2).fit_transform(xTime.reshape(-1, 1))\n",
    "regQuad = LinearRegression().fit(xTimeQuad, heights * Rsun)\n",
    "print(\n",
    "    f'二次拟合:y={regQuad.intercept_:.6f}{+-regQuad.coef_[0]:+.2f}{+-regQuad.coef_[1]:+.6f}x{+-regQuad.coef_[2]:+.6f}x^2'\n",
    ")\n",
    "print(\n",
    "    f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad)))}')\n",
    "print(f'R^2={regQuad.score(xTimeQuad,heights)}')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(xTime, heights * Rsun, label='Result')\n",
    "plt.plot(xTime, reg.predict(xTime.reshape(-1, 1)), label='Linear model')\n",
    "plt.plot(xTime, regQuad.predict(xTimeQuad), label='Quad model')\n",
    "textString = f'Width={AWs[-1]}\\n'\n",
    "textString += f'CPAs={CPAs[-1]}\\n'\n",
    "textString += f'velocity={reg.coef_[0]*Rsun:.3f}km/s\\n'\n",
    "textString += f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\n'\n",
    "textString += f'R$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}\\n'\n",
    "textString += f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\n'\n",
    "textString += f'R$^2$={regQuad.score(xTimeQuad,heights):.3f}\\n'\n",
    "textString += f'RMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "plt.text(xTime.max() * 0.7, Rsun * heights.mean()*0.5, textString)\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2],\n",
    "#     f'y={reg.coef_[0]:.3f}x{reg.intercept_:+.3f}\\nR$^2$={reg.score(xTime.reshape(-1,1),heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,reg.predict(xTime.reshape(-1,1)))):.2f}'\n",
    "# )\n",
    "# plt.text(\n",
    "#     xTime[-4], heights[2] - 0.3,\n",
    "#     f'y={regQuad.coef_[0]+regQuad.intercept_:-.2f}{+-regQuad.coef_[1]:+.2f}x{+-regQuad.coef_[2]:+.2f}x**2\\nR$^2$={regQuad.score(xTimeQuad,heights):.3f}\\nRMSE={np.sqrt(mean_squared_error(heights,regQuad.predict(xTimeQuad))):.2f}'\n",
    "# )\n",
    "\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Height(km)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进想法\n",
    "1. 由于极坐标转换会将原本连在一块的CME分开为两个，影响识别，尝试将被分开的CME轮廓线加在一起看能否正常工作  \n",
    "   已实现CME轮廓线组合，但组合后的轮廓线之间的距离过大，往往不能实现连续匹配\n",
    "2. 若检测到的CME高度在上升后却出现下降，则抛弃下降的高度\n",
    "3. C3日冕仪图像识别结果中出现横杆，产生噪音干扰识别，需要去除  \n",
    "   逐步增大CME样本量，横杆的影响逐渐减小\n",
    "4. 极坐标转换应以正北为极轴，逆时针方向转换\n",
    "   已解决"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 轮廓线合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将轮廓线合为一个整体\n",
    "class ConnectedContour(Contour):\n",
    "    '''用以描述被视为一个整体的两个原本分开的轮廓线\n",
    "    '''\n",
    "    def __init__(self, cntA: Contour, cntB: Contour):\n",
    "        connectedArray = np.concatenate((cntA.array, cntB.array), axis=0)\n",
    "        if cntA.binaryProjectMapIndex != cntB.binaryProjectMapIndex:\n",
    "            raise ValueError(\n",
    "                'Contours connected should be in the SAME binary map')\n",
    "        xa, ya, wa, ha = cv2.boundingRect(cntA.array)\n",
    "        xb, yb, wb, hb = cv2.boundingRect(cntB.array)\n",
    "        # 要求组合为整体的两个轮廓线必须是被极坐标转换而分开的\n",
    "        # 这要求左侧的轮廓线最左侧坐标为0，右侧轮廓线最右侧坐标为360\n",
    "        if (xa != 0 or xb + wb != 360) and (xb != 0 and xa + wa != 360):\n",
    "            raise ValueError(\n",
    "                'Contours are not seprated by polar transformation,xa,wa,xb,wb={},{},{},{}'\n",
    "                .format(xa, wa, xb, wb))\n",
    "        super().__init__(connectedArray, cntA.binaryProjectMap,\n",
    "                         cntA.contourIndex, cntA.binaryProjectMapIndex)\n",
    "        self._cntA = cntA\n",
    "        self._cntB = cntB\n",
    "\n",
    "    def getPositionalAngel(self) -> List:\n",
    "        '''返回组合轮廓线的位置角,返回值为[0,a,b,359]的形式\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        isWarpToNorth : bool\n",
    "            若为True，则将角度转化为以图像北侧为0度，逆时针方式测量的值\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            由两个子轮廓线左侧和右侧位置角构成的数组，分别为\n",
    "            [0,cntARight,cntBLeft,359]\n",
    "        '''\n",
    "        cntAPA = self._cntA.getPositionalAngel()\n",
    "        cntBPA = self._cntB.getPositionalAngel()\n",
    "        if 0 in cntAPA:\n",
    "            PA = cntAPA + cntBPA\n",
    "        else:\n",
    "            PA = cntBPA + cntAPA\n",
    "\n",
    "        return PA\n",
    "\n",
    "    def getCentralPositionAngel(self) -> float:\n",
    "        '''获取CME轮廓线中央位置角度\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        isWarpToNorth : bool\n",
    "            若为True，则将角度转化为以图像北侧为0度，逆时针方式测量的值\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            该CME轮廓线的中央位置角\n",
    "        '''\n",
    "        cntARight = self.getPositionalAngel()[1]\n",
    "        cntBLeft = self.getPositionalAngel()[2]\n",
    "        CPA = int(((cntARight + cntBLeft) / 2 + 180) % 360)\n",
    "        return CPA\n",
    "\n",
    "    def getAngularWidth(self) -> int:\n",
    "        '''返回角宽度\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            角宽度\n",
    "        '''\n",
    "        cntARight = self.getPositionalAngel()[1]\n",
    "        cntBLeft = self.getPositionalAngel()[2]\n",
    "        return 360 - cntBLeft + cntARight\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤轮廓线识别结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤日冕仪识别结果\n",
    "def filterCoronagraphResultList(coronagraphResults: List[CoronagraphResult]):\n",
    "    filtedInd = []\n",
    "    for i in range(1, len(coronagraphResults) - 1):\n",
    "        # 若连续两个识别高度都小于前一个，则从该识别结果开始后的每一个都需要去掉\n",
    "        if coronagraphResults[i].height < coronagraphResults[\n",
    "                i - 1].height and coronagraphResults[\n",
    "                    i + 1].height < coronagraphResults[i - 1].height:\n",
    "            filtedInd.extend(list(range(i, len(coronagraphResults))))\n",
    "        # 若某一个识别高度小于前后两个，则该识别结果需要去掉\n",
    "        if coronagraphResults[i].height < coronagraphResults[\n",
    "                i - 1].height and coronagraphResults[\n",
    "                    i + 1].height > coronagraphResults[i].height:\n",
    "            filtedInd.append(i)\n",
    "    filteredResults = [\n",
    "        result for i, result in enumerate(coronagraphResults)\n",
    "        if i not in filtedInd\n",
    "    ]\n",
    "    return filteredResults\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 轮廓线识别噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVisualization:\n",
    "    '''对神经网络最后一层的特征图进行展示\n",
    "    '''\n",
    "    def __init__(self, model: torch.nn.Module, data: np.ndarray) -> None:\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.featureMap = DDT.getActivation(self.model,\n",
    "                                            self.data).transpose(0, 3, 1, 2)\n",
    "\n",
    "    def display(self, imageIndex: int):\n",
    "        '''对神经网络特征图最后一层进行展示\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imageIndex : int\n",
    "            需要展示特征图的图片的索引\n",
    "        '''\n",
    "        from math import ceil\n",
    "        cols = 10\n",
    "        nums = self.featureMap[imageIndex].shape[0]\n",
    "        rows = ceil(nums / cols)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(self.data[imageIndex, 0], cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.figure(figsize=(cols * 5, rows * 5))\n",
    "        ind = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ax = plt.subplot(rows, cols, ind + 1)\n",
    "                feature = self.featureMap[imageIndex, ind]\n",
    "                feature = (feature - np.amin(feature)) / (\n",
    "                    np.amax(feature) - np.amin(feature) + 1e-5)\n",
    "                feature = np.round(feature * 255)\n",
    "                ax.imshow(feature, cmap='gray')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ind += 1\n",
    "                if ind >= nums:\n",
    "                    break\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加数据集前后训练的网络对于挡杆的效果对比\n",
    "ind = 0\n",
    "featureVisul = FeatureVisualization(netC3, imageArrayC3)\n",
    "featureVisul.display(ind)\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(projectedMapC3[ind].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "tC3 = model.model_defination.LeNet5(size=224)\n",
    "tC3.load_param(r'trainIssueLog\\2023_02_14_15_58_32\\parameters.pkl')\n",
    "featureVisul = FeatureVisualization(tC3, imageArrayC3)\n",
    "featureVisul.display(ind)\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(DDT.DDTThirdParty(imageArrayC3, tC3)[ind].squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rightInd = 10\n",
    "leftInd = rightInd + 1\n",
    "plt.imshow(np.abs(imageArrayC3[rightInd] - imageArrayC3[leftInd]).squeeze(),\n",
    "           cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极坐标转换极轴位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_polar_mapping(output_coords, k_angle, k_radius, center):\n",
    "    \"\"\"从欧几里得坐标系转换为极坐标系的反向坐标变换函数  \n",
    "    \n",
    "    为自行修改的版本，是以图像北侧为极轴，逆时针测量的变换。配合skimage.transform.warp使用\n",
    "    skimage.transform中原本的函数是以图像东侧为极轴，顺时针测量。\n",
    "    \"\"\"\n",
    "    angle = output_coords[:, 1] / k_angle\n",
    "    rr = ((output_coords[:, 0] / k_radius) * -np.cos(angle)) + center[0]\n",
    "    cc = ((output_coords[:, 0] / k_radius) * -np.sin(angle)) + center[1]\n",
    "    coords = np.column_stack((cc, rr))\n",
    "    return coords\n",
    "\n",
    "\n",
    "def safeAsInt(val, atol=1e-3):\n",
    "    \"\"\"\n",
    "    安全地将值转化为整形\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    val : scalar or iterable of scalars\n",
    "        Number or container of numbers which are intended to be interpreted as\n",
    "        integers, e.g., for indexing purposes, but which may not carry integer\n",
    "        type.\n",
    "    atol : float\n",
    "        Absolute tolerance away from nearest integer to consider values in\n",
    "        ``val`` functionally integers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    val_int : NumPy scalar or ndarray of dtype `np.int64`\n",
    "        Returns the input value(s) coerced to dtype `np.int64` assuming all\n",
    "        were within ``atol`` of the nearest integer.\n",
    "    \"\"\"\n",
    "    mod = np.asarray(val) % 1  # Extract mantissa\n",
    "\n",
    "    # Check for and subtract any mod values > 0.5 from 1\n",
    "    if mod.ndim == 0:  # Scalar input, cannot be indexed\n",
    "        if mod > 0.5:\n",
    "            mod = 1 - mod\n",
    "    else:  # Iterable input, now ndarray\n",
    "        mod[mod > 0.5] = 1 - mod[mod > 0.5]  # Test on each side of nearest int\n",
    "\n",
    "    try:\n",
    "        np.testing.assert_allclose(mod, 0, atol=atol)\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'Integer argument required but received '\n",
    "                         f'{val}, check inputs.')\n",
    "\n",
    "    return np.round(val).astype(np.int64)\n",
    "\n",
    "\n",
    "def warpPolar(image: np.ndarray,\n",
    "              center: Optional[Tuple[float, float]] = None,\n",
    "              *,\n",
    "              radius: Optional[int] = None,\n",
    "              output_shape: Optional[Tuple[int, int]] = None,\n",
    "              channel_axis=None,\n",
    "              **kwargs):\n",
    "    \"\"\"将图像重新映射至极坐标\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ndarray\n",
    "        Input image. Only 2-D arrays are accepted by default. 3-D arrays are\n",
    "        accepted if a `channel_axis` is specified.\n",
    "    center : tuple (row, col), optional\n",
    "        Point in image that represents the center of the transformation (i.e.,\n",
    "        the origin in cartesian space). Values can be of type `float`.\n",
    "        If no value is given, the center is assumed to be the center point\n",
    "        of the image.\n",
    "    radius : float, optional\n",
    "        Radius of the circle that bounds the area to be transformed.\n",
    "    output_shape : tuple (row, col), optional\n",
    "    multichannel : bool, optional\n",
    "        Whether the image is a 3-D array in which the third axis is to be\n",
    "        interpreted as multiple channels. If set to `False` (default), only 2-D\n",
    "        arrays are accepted. This argument is deprecated: specify\n",
    "        `channel_axis` instead.\n",
    "    channel_axis : int or None, optional\n",
    "        If None, the image is assumed to be a grayscale (single channel) image.\n",
    "        Otherwise, this parameter indicates which axis of the array corresponds\n",
    "        to channels.\n",
    "    **kwargs : keyword arguments\n",
    "        Passed to `transform.warp`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    warped : ndarray\n",
    "        The polar or log-polar warped image.\n",
    "    \"\"\"\n",
    "    multichannel = channel_axis is not None\n",
    "    if image.ndim != 2 and not multichannel:\n",
    "        raise ValueError(f'Input array must be 2-dimensional when '\n",
    "                         f'`channel_axis=None`, got {image.ndim}')\n",
    "\n",
    "    if image.ndim != 3 and multichannel:\n",
    "        raise ValueError(f'Input array must be 3-dimensional when '\n",
    "                         f'`channel_axis` is specified, got {image.ndim}')\n",
    "\n",
    "    if center is None:\n",
    "        center = (np.array(image.shape)[:2] / 2) - 0.5\n",
    "\n",
    "    if radius is None:\n",
    "        w, h = np.array(image.shape)[:2] / 2\n",
    "        radius = np.sqrt(w**2 + h**2)\n",
    "\n",
    "    if output_shape is None:\n",
    "        height = 360\n",
    "        width = int(np.ceil(radius))\n",
    "        output_shape = (height, width)\n",
    "    else:\n",
    "        output_shape = safeAsInt(output_shape)\n",
    "        height = output_shape[0]\n",
    "        width = output_shape[1]\n",
    "\n",
    "    k_radius = width / radius\n",
    "    map_func = _linear_polar_mapping\n",
    "\n",
    "    k_angle = height / (2 * np.pi)\n",
    "    warp_args = {'k_angle': k_angle, 'k_radius': k_radius, 'center': center}\n",
    "\n",
    "    from skimage.transform import warp\n",
    "    warped = warp(image,\n",
    "                  map_func,\n",
    "                  map_args=warp_args,\n",
    "                  output_shape=output_shape,\n",
    "                  preserve_range=True,\n",
    "                  order=0,\n",
    "                  **kwargs)\n",
    "    return warped\n",
    "\n",
    "\n",
    "def warpCartToPolar(img: np.ndarray, dsize: tuple[int, int],\n",
    "                    center: tuple[int, int], maxRadius: float) -> np.ndarray:\n",
    "    '''将数组转换为极坐标形式\n",
    "\n",
    "    坐标变换会改变图像像素值分布\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        需要转换的数组，形状为NHW或NHWC\n",
    "    dsize : tuple\n",
    "        输出的图像大小，格式为(列,行)\n",
    "    center : tuple\n",
    "        转换中心，格式为(列,行)\n",
    "    maxRadius : float\n",
    "        最大半径，决定转换的最大范围\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        转换后的图片数组，类型同输入图像数组img相同\n",
    "    '''\n",
    "    # skimage接受的中心坐标和图像尺寸都是(行,列)，需要进行转换\n",
    "    center = (center[1], center[0])\n",
    "    dsize = (dsize[1], dsize[0])\n",
    "    if img.ndim != 3 and img.ndim != 4:\n",
    "        raise ValueError('Input image shape must be 3 or 4 ,got {}'.format(\n",
    "            img.ndim))\n",
    "    if img.ndim == 3:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1]),\n",
    "                              dtype=img.dtype)\n",
    "    else:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w,C)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1], img.shape[3]),\n",
    "                              dtype=img.dtype)\n",
    "    channel_axis = None if img.ndim == 3 else 3\n",
    "    from skimage.transform import rotate\n",
    "    for i in range(img.shape[0]):\n",
    "        imgPol = warpPolar(img[i],\n",
    "                           center,\n",
    "                           radius=maxRadius,\n",
    "                           output_shape=dsize,\n",
    "                           channel_axis=channel_axis)\n",
    "        imgPol = rotate(imgPol, 90, order=0, preserve_range=True)\n",
    "        # imgPol = cv2.warpPolar(img[i], dsize, center, maxRadius,\n",
    "        #                        cv2.INTER_NEAREST + cv2.WARP_POLAR_LINEAR)\n",
    "        # imgPol = cv2.rotate(imgPol, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        imgInPolar[i] = imgPol\n",
    "    return imgInPolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = warpPolar(imageArray[3].squeeze(), (243, 258),\n",
    "              radius=245,\n",
    "              output_shape=(360, 360))\n",
    "w = skimage.transform.rotate(w, 90)\n",
    "plt.imshow(w, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.transform import warp_polar, rotate\n",
    "import skimage\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "image = np.array(Image.open(r'C:\\Users\\lenovo\\Desktop\\tmp2278.jpg'))\n",
    "imgPol = cv2.warpPolar(image, (360, 360), (138, 138), 138,\n",
    "                       cv2.INTER_NEAREST + cv2.WARP_POLAR_LINEAR)\n",
    "imgPol = cv2.rotate(imgPol, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "# image = data.astronaut()\n",
    "# warped = warp_polar(image)\n",
    "warped = warpPolar(image, (138,138), radius=138, output_shape=(360, 360),channel_axis=-1)\n",
    "warped = rotate(warped, 90)\n",
    "\n",
    "# image = data.astronaut()\n",
    "# warped = warp_polar(image, scaling='log', channel_axis=-1)\n",
    "plt.figure()\n",
    "plt.imshow(warped, cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(imgPol, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image,cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb6408f9acc94ef862c1000d2f7ed73b15d48594c9d0937a8093d9903abff190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
