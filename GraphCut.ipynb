{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import model.train_schedule\n",
    "import model.model_defination\n",
    "import model\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normoalizeArray(array: np.ndarray, newmin, newmax) -> np.ndarray:\n",
    "    '''\n",
    "    将array中的最大值变为max，最小值变为min，其他的数值按照平均原则计算。\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:需要改变的的数组\n",
    "    newmin:最小值\n",
    "    newmax:最大值\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    值改变后的数组\n",
    "    '''\n",
    "    originMax = array.max()\n",
    "    originMin = array.min()\n",
    "    difference = originMax - originMin\n",
    "    newArray = (array - originMin) / difference * (newmax - newmin) + newmin\n",
    "    return newArray\n",
    "\n",
    "\n",
    "def arrayToImg(arr: np.ndarray) -> np.ndarray:\n",
    "    '''将数组的值改为[0,255]范围内\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        被改变的数组\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        修改后的数组\n",
    "    '''\n",
    "    return normoalizeArray(arr.astype('np.uint8'), 0, 255)\n",
    "\n",
    "\n",
    "def saveImg(arr: np.ndarray, filename: str):\n",
    "    '''保存图片\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        被保存的数组\n",
    "    filename : str\n",
    "        图片的文件名\n",
    "    '''\n",
    "    Image.fromarray(arrayToImg(arr)).save(filename)\n",
    "\n",
    "\n",
    "def showImg(arr: np.ndarray):\n",
    "    '''将数组作为图片展示\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        需要展示的数组\n",
    "    '''\n",
    "    Image.fromarray(arrayToImg(arr)).show()\n",
    "\n",
    "\n",
    "def grayImageToRGB(arr: np.ndarray) -> np.ndarray:\n",
    "    '''将灰度图像的复制三份，成为RGB图像\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        灰度图像，形状为HW或者NHW\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        RGB三通道图像，形状为HWC或者NHWC\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        图像的维度必须为2，否则会引发ValueError\n",
    "    '''\n",
    "    if arr.ndim == 3:\n",
    "        return np.concatenate((np.expand_dims(arr, 3), np.expand_dims(\n",
    "            arr, 3), np.expand_dims(arr, 3)),\n",
    "                              axis=3)\n",
    "    elif arr.ndim == 2:\n",
    "        return np.concatenate((np.expand_dims(arr, 2), np.expand_dims(\n",
    "            arr, 2), np.expand_dims(arr, 2)),\n",
    "                              axis=2)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Dimensions of input array must be 3(NHW) or 2(HW),got {}'.format(\n",
    "                arr.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互式GraphCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现交互式的Graph Cut算法\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124723416\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "drawing = False\n",
    "mode = False\n",
    "\n",
    "\n",
    "class GraphCutXupt:\n",
    "    def __init__(self, t_img):\n",
    "        self.img = t_img\n",
    "        self.img_raw = img.copy()\n",
    "        self.img_width = img.shape[0]\n",
    "        self.img_height = img.shape[1]\n",
    "        self.scale_size = 640 * self.img_width // self.img_height\n",
    "        if self.img_width > 640:\n",
    "            self.img = cv2.resize(self.img, (640, self.scale_size),\n",
    "                                  interpolation=cv2.INTER_AREA)\n",
    "        self.img_show = self.img.copy()\n",
    "        self.img_gc = self.img.copy()\n",
    "        self.img_gc = cv2.GaussianBlur(self.img_gc, (3, 3), 0)\n",
    "        self.lb_up = False\n",
    "        self.rb_up = False\n",
    "        self.lb_down = False\n",
    "        self.rb_down = False\n",
    "        self.mask = np.full(self.img.shape[:2], 2, dtype=np.uint8)\n",
    "        self.firt_choose = True\n",
    "\n",
    "\n",
    "# 鼠标的回调函数\n",
    "def mouse_event2(event, x, y, flags, param):\n",
    "    global drawing, last_point, start_point\n",
    "    # 左键按下：开始画图\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        last_point = (x, y)\n",
    "        start_point = last_point\n",
    "        param.lb_down = True\n",
    "        print('mouse lb down')\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        drawing = True\n",
    "        last_point = (x, y)\n",
    "        start_point = last_point\n",
    "        param.rb_down = True\n",
    "        print('mouse rb down')\n",
    "    # 鼠标移动，画图\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            if param.lb_down:\n",
    "                cv2.line(param.img_show, last_point, (x, y), (0, 0, 255), 2,\n",
    "                         -1)\n",
    "                cv2.rectangle(param.mask, last_point, (x, y), 1, -1, 4)\n",
    "            else:\n",
    "                cv2.line(param.img_show, last_point, (x, y), (255, 0, 0), 2,\n",
    "                         -1)\n",
    "                cv2.rectangle(param.mask, last_point, (x, y), 0, -1, 4)\n",
    "            last_point = (x, y)\n",
    "    # 左键释放：结束画图\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        param.lb_up = True\n",
    "        param.lb_down = False\n",
    "        cv2.line(param.img_show, last_point, (x, y), (0, 0, 255), 2, -1)\n",
    "        if param.firt_choose:\n",
    "            param.firt_choose = False\n",
    "        cv2.rectangle(param.mask, last_point, (x, y), 1, -1, 4)\n",
    "        print('mouse lb up')\n",
    "    elif event == cv2.EVENT_RBUTTONUP:\n",
    "        drawing = False\n",
    "        param.rb_up = True\n",
    "        param.rb_down = False\n",
    "        cv2.line(param.img_show, last_point, (x, y), (255, 0, 0), 2, -1)\n",
    "        if param.firt_choose:\n",
    "            param.firt_choose = False\n",
    "            param.mask = np.full(param.img.shape[:2], 3, dtype=np.uint8)\n",
    "        cv2.rectangle(param.mask, last_point, (x, y), 0, -1, 4)\n",
    "        print('mouse rb up')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = cv2.imread(\n",
    "        \"MiddleData/original/20130820_092405_lasc2rdf_aia193rdf.png\",\n",
    "        flags=1)  # 读取彩色图像(Youcans)\n",
    "    g_img = GraphCutXupt(img)\n",
    "\n",
    "    cv2.namedWindow('image')\n",
    "    # 定义鼠标的回调函数\n",
    "    cv2.setMouseCallback('image', mouse_event2, g_img)\n",
    "    while (True):\n",
    "        cv2.imshow('image', g_img.img_show)\n",
    "        if g_img.lb_up or g_img.rb_up:\n",
    "            g_img.lb_up = False\n",
    "            g_img.rb_up = False\n",
    "            bgdModel = np.zeros((1, 65), np.float64)\n",
    "            fgdModel = np.zeros((1, 65), np.float64)\n",
    "            rect = (1, 1, g_img.img.shape[1], g_img.img.shape[0])\n",
    "            print(g_img.mask)\n",
    "            mask = g_img.mask\n",
    "            g_img.img_gc = g_img.img.copy()\n",
    "            cv2.grabCut(g_img.img_gc, mask, rect, bgdModel, fgdModel, 5,\n",
    "                        cv2.GC_INIT_WITH_MASK)\n",
    "            mask2 = np.where((mask == 2) | (mask == 0), 0,\n",
    "                             1).astype('uint8')  # 0和2做背景\n",
    "            g_img.img_gc = g_img.img_gc * mask2[:, :,\n",
    "                                                np.newaxis]  # 使用蒙板来获取前景区域\n",
    "            cv2.imshow('youcans', g_img.img_gc)\n",
    "        # 按下ESC键退出\n",
    "        if cv2.waitKey(20) == 27:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.subplot(221), plt.axis('off'), plt.title(\"xupt\")\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "    plt.subplot(222), plt.axis('off'), plt.title(\"mask\")\n",
    "    plt.imshow(mask, 'gray')\n",
    "    plt.subplot(223), plt.axis('off'), plt.title(\"mask2\")\n",
    "    plt.imshow(mask2, 'gray')\n",
    "    plt.subplot(224), plt.axis('off'), plt.title(\"Grab Cut\")\n",
    "    plt.imshow(cv2.cvtColor(g_img.img_gc, cv2.COLOR_BGR2RGB))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用前景选框的GrabCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现GrabCut算法(前景选框)\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124744467\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"/home/lin/Pictures/R-C.jpg\", flags=1)  # 读取彩色图像(BGR)\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "# 定义矩形框，框选目标前景\n",
    "# rect = (118, 125, 220, 245)  # 直接设置矩形的位置参数，也可以鼠标框选 ROI\n",
    "print(\"Select a ROI and then press SPACE or ENTER button!\\n\")\n",
    "roi = cv2.selectROI(image, showCrosshair=True, fromCenter=False)\n",
    "xmin, ymin, w, h = roi  # 矩形裁剪区域 (ymin:ymin+h, xmin:xmin+w) 的位置参数\n",
    "rect = (xmin, ymin, w, h)  # 边界框矩形的坐标和尺寸\n",
    "imgROI = np.zeros_like(image)  # 创建与 image 相同形状的黑色图像\n",
    "imgROI[ymin:ymin + h, xmin:xmin + w] = image[ymin:ymin + h,\n",
    "                                             xmin:xmin + w].copy()\n",
    "print(xmin, ymin, w, h)\n",
    "\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(image,\n",
    "                                       mask,\n",
    "                                       rect,\n",
    "                                       bgModel,\n",
    "                                       fgModel,\n",
    "                                       iter,\n",
    "                                       mode=cv2.GC_INIT_WITH_RECT)  # 框选前景分割模式\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(231), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(232), plt.axis('off'), plt.title(\"Bounding box\")\n",
    "plt.imshow(cv2.cvtColor(imgROI, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(233), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite background(White area)\")\n",
    "maskBGD = (mask == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "plt.subplot(234), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable background(white area)\")\n",
    "maskPBGD = (mask == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "plt.subplot(235), plt.axis('off'), plt.title(\n",
    "    \"GrabCut Mask(White for Foreground,Black for Background)\")\n",
    "# maskGrabCut = np.where((mask==cv2.GC_BGD) | (mask==cv2.GC_PR_BGD), 0, 1)\n",
    "plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(236), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(maskImg, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用掩膜图像的GrabCut算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用OpenCV实现GrabCut算法（掩模图像）\n",
    "# 来自https://blog.csdn.net/youcans/article/details/124744517\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# cv::GrabCutClasses {\n",
    "#   cv::GC_BGD = 0,\n",
    "#   cv::GC_FGD = 1,\n",
    "#   cv::GC_PR_BGD = 2,\n",
    "#   cv::GC_PR_FGD = 3\n",
    "# }\n",
    "image = cv2.imread(\n",
    "    r\"C:\\Programing\\MiddleData/original/20130820_102405_lasc2rdf_aia193rdf.png\",\n",
    "    flags=1)  # 读取彩色图像(BGR)\n",
    "# *对于CME灰度图像，三个通道的值完全相同\n",
    "maskImg = cv2.imread(r\"C:\\Programing\\MiddleData/cropresult/crop9.png\",\n",
    "                     flags=0)[:, 512:]  # 读取掩模图像(xupt) TODO need modification\n",
    "\n",
    "# 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "mask[maskImg == 0] = cv2.GC_BGD\n",
    "# print(mask.shape, maskInv.shape)\n",
    "# apply GrabCut using the the mask segmentation method\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(image,\n",
    "                                       mask,\n",
    "                                       None,\n",
    "                                       bgModel,\n",
    "                                       fgModel,\n",
    "                                       iter,\n",
    "                                       mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "# //原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "# //否则显示的区域就成了背景，所以进行修改\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(231), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "plt.subplot(232), plt.axis('off'), plt.title(\"Mask image\")\n",
    "plt.imshow(maskImg, 'gray')  # definite background\n",
    "plt.subplot(233), plt.axis('off'), plt.title(\"GrabCut mask\")\n",
    "plt.imshow(mask, 'gray')\n",
    "plt.subplot(234), plt.axis('off'), plt.title(\"Mask for definite background\")\n",
    "maskBGD = (mask == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "plt.subplot(235), plt.axis('off'), plt.title(\"Mask for probable background\")\n",
    "maskPBGD = (mask == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "# plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "# plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(236), plt.axis('off'), plt.title(\"Youcans Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对掩膜图像的GrabCut进行了自行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# cv::GrabCutClasses {\n",
    "#   cv::GC_BGD = 0,\n",
    "#   cv::GC_FGD = 1,\n",
    "#   cv::GC_PR_BGD = 2,\n",
    "#   cv::GC_PR_FGD = 3\n",
    "# }\n",
    "image = cv2.imread(\n",
    "    r\"C:\\Programing\\MiddleData/original/20130820_102405_lasc2rdf_aia193rdf.png\",\n",
    "    flags=1)  # 读取彩色图像(BGR)\n",
    "# *对于CME灰度图像，三个通道的值完全相同\n",
    "maskImg = cv2.imread(r\"C:\\Programing\\MiddleData/cropresult/crop9.png\",\n",
    "                     flags=0)[:, 512:]  # 读取掩模图像(xupt) TODO need modification\n",
    "\n",
    "# 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "mask[maskImg == 0] = cv2.GC_BGD\n",
    "# print(mask.shape, maskInv.shape)\n",
    "# apply GrabCut using the the mask segmentation method\n",
    "fgModel = np.zeros((1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "bgModel = np.zeros((1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "iter = 5\n",
    "(maskAfter, bgModel, fgModel) = cv2.grabCut(\n",
    "    image,\n",
    "    mask,\n",
    "    None,\n",
    "    bgModel,\n",
    "    fgModel,\n",
    "    # None,\n",
    "    # None,\n",
    "    iter,\n",
    "    mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "\n",
    "# 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "maskOutput = np.where((maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD),\n",
    "                      0, 1)\n",
    "# *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "# *否则显示的区域就成了背景，所以进行修改\n",
    "maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "imgGrabCut = cv2.bitwise_and(image, image, mask=maskGrabCut)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "plt.subplot(332), plt.axis('off'), plt.title(\"Mask image\")\n",
    "plt.imshow(maskImg, 'gray')  # definite background\n",
    "\n",
    "plt.subplot(333), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "plt.subplot(334), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite background(white)\")\n",
    "maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "plt.subplot(335), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable background(white)\")\n",
    "maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "# plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "# plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "plt.subplot(336), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "plt.subplot(337), plt.axis('off'), plt.title(\n",
    "    \"Mask for definite foreground(white)\")\n",
    "maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "plt.subplot(338), plt.axis('off'), plt.title(\n",
    "    \"Mask for probable foreground(white)\")\n",
    "maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#原图 掩膜图 grabcut生成的mask 前景 背景 绝对前景 绝对背景 grabcut后的图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩膜图像的GrabCut方法封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import DDT\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "def cutWithMask(image: np.ndarray,\n",
    "                maskImg: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对image进行GrabCut，使用maskImg作为掩膜\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        需要进行GrabCut的图像，为BGR图像，形状为NHWC，类型必须为uint8\n",
    "    maskImg : np.ndarray\n",
    "        作为掩膜的图像，为灰度图，形状为NHW\n",
    "    Returns\n",
    "    ----------\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理过后的掩膜，该数组的值为[0,1,2,3]其中之一，其定义为\n",
    "        cv::GC_BGD = 0,\n",
    "        cv::GC_FGD = 1,\n",
    "        cv::GC_PR_BGD = 2,\n",
    "        cv::GC_PR_FGD = 3\n",
    "    imgGrabCut : np.ndarray\n",
    "        经过GrabCut算法处理过后的图像，形状为NHWC\n",
    "    '''\n",
    "    #image = cv2.imread(imagePath, flags=1)  # 读取彩色图像(BGR)\n",
    "    # *CV2中，图片默认为BGR格式\n",
    "    for i in range(image.shape[0]):\n",
    "        image[i] = cv2.cvtColor(image[i], cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 对于CME灰度图像，三个通道的值完全相同\n",
    "    #maskImg = cv2.imread(maskImgPath, flags=0)[:, 512:]  # 读取掩模图像(xupt)\n",
    "\n",
    "    # 生成掩模图像 mask，大于 0 的像素设为可能前景\n",
    "    mask = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    mask[maskImg > 0] = cv2.GC_PR_FGD\n",
    "    mask[maskImg == 0] = cv2.GC_BGD\n",
    "    # print(mask.shape, maskInv.shape)\n",
    "    # apply GrabCut using the the mask segmentation method\n",
    "    fgModel = np.zeros((image.shape[0], 1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "    bgModel = np.zeros((image.shape[0], 1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "    iter = 5\n",
    "    maskAfter = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    imgGrabCut = np.zeros_like(image)\n",
    "    for i in range(image.shape[0]):\n",
    "        (maskAfter[i], bgModel[i],\n",
    "         fgModel[i]) = cv2.grabCut(image[i],\n",
    "                                   mask[i],\n",
    "                                   None,\n",
    "                                   bgModel[i],\n",
    "                                   fgModel[i],\n",
    "                                   iter,\n",
    "                                   mode=cv2.GC_INIT_WITH_MASK)  # 基于掩模图像初始化\n",
    "        maskOutput = np.where(\n",
    "            (maskAfter[i] == cv2.GC_BGD) | (maskAfter[i] == cv2.GC_PR_BGD), 0,\n",
    "            1)\n",
    "        # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "        # *否则显示的区域就成了背景，所以进行修改\n",
    "        maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "        imgGrabCut[i] = cv2.bitwise_and(image[i], image[i],\n",
    "                                        mask=maskGrabCut).astype('uint8')\n",
    "    return maskAfter, imgGrabCut\n",
    "\n",
    "\n",
    "def showGrabCutWithMask(image: np.ndarray, maskImg: np.ndarray,\n",
    "                        maskAfter: np.ndarray):\n",
    "    '''绘制GrabCut的结果\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        经GrabCut处理之前的原图像，形状为HWC\n",
    "    maskImg : np.ndarray\n",
    "        掩膜图像，形状为HW\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理之后的mask，形状为HW\n",
    "    '''\n",
    "    # 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "    maskOutput = np.where(\n",
    "        (maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD), 0, 1)\n",
    "    # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "    # *否则显示的区域就成了背景，所以进行修改\n",
    "    maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "    imgGrabCut = cv2.bitwise_and(image, image,\n",
    "                                 mask=maskGrabCut).astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "    plt.imshow(cv2.cvtColor(image.astype('uint8'),\n",
    "                            cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "    plt.subplot(332), plt.axis('off'), plt.title(\"Mask image\")\n",
    "    plt.imshow(maskImg, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(333), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "    plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "    plt.subplot(334), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "    plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "    plt.subplot(335), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite background(white)\")\n",
    "    maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(336), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable background(white)\")\n",
    "    maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "    # plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "    # plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "\n",
    "    plt.subplot(337), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite foreground(white)\")\n",
    "    maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "    plt.subplot(338), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable foreground(white)\")\n",
    "    maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.subplots_adjust(hspace=0.2,wspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from DDT import getLargestComponent\n",
    "\n",
    "\n",
    "def maskImage(imgs: np.ndarray, mask: np.ndarray, mode=cv2.COLORMAP_AUTUMN):\n",
    "    '''以mask为遮罩，得到img被遮罩后的数组\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : np.ndarray, uint8\n",
    "        原图像，形状为NHWC\n",
    "    mask : np.ndarray, uint8\n",
    "        遮罩，形状为NHW，值为0或1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        原图被遮罩遮盖后的数组\n",
    "    '''\n",
    "    if not np.issubsctype(mask.dtype, np.uint8) or not np.issubdtype(\n",
    "            imgs.dtype, np.uint8):\n",
    "        raise ValueError(\n",
    "            'Input img and mask dtype both should be uint8, got {} and {}'.\n",
    "            format(imgs.dtype, mask.dtype))\n",
    "    imgAfterMask = np.zeros((imgs.shape[0], imgs.shape[1], imgs.shape[2], 3),\n",
    "                            dtype=np.uint8)\n",
    "    mask = mask * 255\n",
    "    for i in range(imgs.shape[0]):\n",
    "        maskColored = np.expand_dims(mask[i], axis=0).transpose(1, 2, 0)\n",
    "        maskColored = cv2.cvtColor(cv2.applyColorMap(maskColored, mode),\n",
    "                                   cv2.COLOR_BGR2RGB)\n",
    "        img = np.tile(imgs[i], (1, 1, 3))\n",
    "        imgAfterMask[i] = cv2.addWeighted(img, 0.5, maskColored, 0.5, 0.0)\n",
    "    return imgAfterMask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前景选框的GrabCut方法封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWithROI(image: np.ndarray) -> tuple:\n",
    "    '''对image进行GrabCut，采用框选的方式选择前景\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        待分割的图像数组，形状为NHWC\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    maskAfter : np.ndarray\n",
    "        经过GrabCut处理过后的掩膜，该数组的值为[0,1,2,3]其中之一，其定义为\n",
    "        cv::GC_BGD = 0,\n",
    "        cv::GC_FGD = 1,\n",
    "        cv::GC_PR_BGD = 2,\n",
    "        cv::GC_PR_FGD = 3\n",
    "    '''\n",
    "    # *CV2中，图片默认为BGR格式\n",
    "    picNums = image.shape[0]\n",
    "    for i in range(image.shape[0]):\n",
    "        image[i] = cv2.cvtColor(image[i], cv2.COLOR_RGB2BGR)\n",
    "    mask = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "\n",
    "    # 定义矩形框，框选目标前景\n",
    "    # rect = (118, 125, 220, 245)  # 直接设置矩形的位置参数，也可以鼠标框选 ROI\n",
    "    rois = []\n",
    "    # imgsROI = np.zeros_like(image)  # 创建与 image 相同形状的黑色图像，方便展示选框框住的物体\n",
    "    for i in range(picNums):\n",
    "        windowName = 'Selecting ROI {}/{}'.format(i, picNums)\n",
    "        roi = cv2.selectROI(windowName,\n",
    "                            image[i],\n",
    "                            showCrosshair=True,\n",
    "                            fromCenter=False)\n",
    "        rois.append(roi)\n",
    "        # xmin, ymin, w, h = roi\n",
    "        # imgsROI[i, ymin:ymin + h, xmin:xmin + w] = image[i, ymin:ymin + h,\n",
    "        #                                                  xmin:xmin + w].copy()\n",
    "\n",
    "    fgModel = np.zeros((picNums, 1, 65), dtype=\"float\")  # 前景模型, 13*5\n",
    "    bgModel = np.zeros((picNums, 1, 65), dtype=\"float\")  # 背景模型, 13*5\n",
    "    iter = 5\n",
    "    maskAfter = np.zeros(image.shape[:3], dtype=\"uint8\")\n",
    "    for i in range(picNums):\n",
    "        (maskAfter[i], bgModel[i],\n",
    "         fgModel[i]) = cv2.grabCut(image[i],\n",
    "                                   mask[i],\n",
    "                                   rois[i],\n",
    "                                   bgModel[i],\n",
    "                                   fgModel[i],\n",
    "                                   iter,\n",
    "                                   mode=cv2.GC_INIT_WITH_RECT)  # 框选前景分割模式\n",
    "    return maskAfter, rois\n",
    "\n",
    "\n",
    "def showGrabCutWithROI(image: np.ndarray, roi: tuple, maskAfter: np.ndarray):\n",
    "    '''绘制利用ROI框选前景的GrabCut结果\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        待分割的原图片数组,形状为HW\n",
    "    rois : list\n",
    "        图片选框的四元组，包含xmin,ymin,w,h\n",
    "    maskAfter : np.ndarray\n",
    "        GrabCut输出的mask\n",
    "    '''\n",
    "    # 将所有确定背景和可能背景像素设置为 0，而确定前景和可能前景像素设置为 1\n",
    "    maskOutput = np.where(\n",
    "        (maskAfter == cv2.GC_BGD) | (maskAfter == cv2.GC_PR_BGD), 0, 1)\n",
    "    # *原为maskGrabCut = 255 - (maskOutput * 255).astype(\"uint8\")，但其中不应为255减去\n",
    "    # *否则显示的区域就成了背景，所以进行修改\n",
    "    maskGrabCut = (maskOutput * 255).astype(\"uint8\")\n",
    "    imgGrabCut = cv2.bitwise_and(image, image,\n",
    "                                 mask=maskGrabCut).astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(331), plt.axis('off'), plt.title(\"Origin image\")\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # 显示 img(RGB)\n",
    "\n",
    "    imgROI = np.zeros_like(image)\n",
    "    xmin, ymin, w, h = roi\n",
    "    imgROI[ymin:ymin + h, xmin:xmin + w] = image[ymin:ymin + h,\n",
    "                                                 xmin:xmin + w].copy()\n",
    "    plt.subplot(332), plt.axis('off'), plt.title(\"Bounding Box\")\n",
    "    plt.imshow(imgROI, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(333), plt.axis('off'), plt.title(\"GrabCut Output\")\n",
    "    plt.imshow(cv2.cvtColor(imgGrabCut, cv2.COLOR_BGR2RGB))  # GrabCut Output\n",
    "\n",
    "    plt.subplot(334), plt.axis('off'), plt.title(\"mask after GrabCut\")\n",
    "    plt.imshow(maskAfter, 'gray')\n",
    "\n",
    "    plt.subplot(335), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite background(white)\")\n",
    "    maskBGD = (maskAfter == cv2.GC_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskBGD, 'gray')  # definite background\n",
    "\n",
    "    plt.subplot(336), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable background(white)\")\n",
    "    maskPBGD = (maskAfter == cv2.GC_PR_BGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBGD, 'gray')  # probable background\n",
    "\n",
    "    # plt.subplot(235), plt.axis('off'), plt.title(\"GrabCut Mask\")\n",
    "    # plt.imshow(maskGrabCut, 'gray')  # mask generated by GrabCut\n",
    "\n",
    "    plt.subplot(337), plt.axis('off'), plt.title(\n",
    "        \"Mask for definite foreground(white)\")\n",
    "    maskFGD = (maskAfter == cv2.GC_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskFGD, 'gray')  # definite foreground\n",
    "\n",
    "    plt.subplot(338), plt.axis('off'), plt.title(\n",
    "        \"Mask for probable foreground(white)\")\n",
    "    maskPBFD = (maskAfter == cv2.GC_PR_FGD).astype(\"uint8\") * 255\n",
    "    plt.imshow(maskPBFD, 'gray')  # probable foreground\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrabCut算法测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一 20140106_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "crop = utils.CenterCrop('NCHW')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_1 = r'CME_data\\20140106_2'\n",
    "imageArray20140106_1 = utils.loadImageFolder(PathCME20140106_1)\n",
    "cropimageArray20140106_1 = crop(imageArray20140106_1)\n",
    "cropnet.predict(cropimageArray20140106_1)\n",
    "cropimageArray20140106_1 = cropimageArray20140106_1[cropnet.predict(\n",
    "    cropimageArray20140106_1) == 1]\n",
    "largestComp20140106_1 = DDT.DDT(cropimageArray20140106_1, cropnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropimageArray20140106_1 = cropimageArray20140106_1.squeeze()\n",
    "stackcropimageArray20140106_1 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1).astype('uint8')\n",
    "#!最大连接分量存在翻转情况\n",
    "reverlargestComp20140106_1 = 1 - largestComp20140106_1\n",
    "# *CutWithMask所接受的Image图片必须为uint8类型\n",
    "maskafter20140106_1 = cutWithMask(stackcropimageArray20140106_1,\n",
    "                                  reverlargestComp20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawLargestComp(cropimageArray20140106_1, reverlargestComp20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskafterROI, rois = cutWithROI(stackcropimageArray20140106_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 1\n",
    "showGrabCutWithMask(cropimageArray20140106_1[picIndex],\n",
    "                    reverlargestComp20140106_1[picIndex],\n",
    "                    maskafter20140106_1[picIndex])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 8\n",
    "showGrabCutWithROI(cropimageArray20140106_1[picIndex], rois[picIndex],\n",
    "                   maskafterROI[picIndex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试二 20140106_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "crop = utils.CenterCrop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_2 = 'CME_data/20140106_2'\n",
    "imageArray20140106_2 = utils.loadImageFolder(PathCME20140106_2)\n",
    "cropimageArray20140106_2 = crop(imageArray20140106_2)\n",
    "print('cropnet predict:', cropnet.predict(cropimageArray20140106_2))\n",
    "cropimageArray20140106_2 = cropimageArray20140106_2[cropnet.predict(\n",
    "    cropimageArray20140106_2) == 1]\n",
    "largestComp20140106_2 = DDT.DDT(cropimageArray20140106_2, cropnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawLargestComp(cropimageArray20140106_2, largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropimageArray20140106_2 = cropimageArray20140106_2.squeeze()\n",
    "stackcropimageArray20140106_2 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_2).astype('uint8')\n",
    "# *CutWithMask所接受的Image图片必须为uint8类型\n",
    "maskafter20140106_2 = cutWithMask(stackcropimageArray20140106_2,\n",
    "                                  largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT.drawImageArrays(cropimageArray20140106_2, largestComp20140106_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 2\n",
    "showGrabCutWithMask(cropimageArray20140106_2[picIndex],\n",
    "                    largestComp20140106_2[picIndex],\n",
    "                    maskafter20140106_2[picIndex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更换第三方DDT算法的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一 20140106_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model_defination\n",
    "import utils\n",
    "import DDT\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "net = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_04_13_20_00_25/parameters.pkl'\n",
    "net.load_param(parameter_path)\n",
    "cropnet = model.model_defination.LeNet5()\n",
    "parameter_path = 'log/2022_06_22_17_59_11/parameters.pkl'\n",
    "cropnet.load_param(parameter_path)\n",
    "# 使用ToTensor变换的网络\n",
    "cropnet_new = model.model_defination.LeNet5()\n",
    "parameter_path_new = 'trainIssueLog/2022_10_13_22_40_16/parameters.pkl'\n",
    "cropnet_new.load_param(parameter_path_new)\n",
    "crop = utils.CenterCrop('NCHW', value=127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathCME20140106_1 = r'CME_data\\20140602'\n",
    "imageArray20140106_1 = utils.loadImageFolder(PathCME20140106_1)\n",
    "cropimageArray20140106_1 = crop(imageArray20140106_1)\n",
    "projectedMap = DDT.DDTThirdParty(cropimageArray20140106_1, cropnet)\n",
    "colormap = DDT.toColorMap(projectedMap)\n",
    "outputImage = DDT.colorMapImage(cropimageArray20140106_1.astype('uint8'),\n",
    "                                projectedMap)\n",
    "\n",
    "\n",
    "def divide255(image):\n",
    "    return image / 255\n",
    "\n",
    "\n",
    "# 筛选了新的数据集，读取的图片默认归为[0,1]的网络结果\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_new = utils.loadImageFolder(PathCME20140106_1, trans)\n",
    "projectedMap_new = DDT.DDTThirdParty(cropimageArray20140106_1_new, cropnet_new)\n",
    "colormap_new = DDT.toColorMap(projectedMap_new)\n",
    "# 255*cropimageArray20140106_1_new.astype('uint8')=255*(cropimageArray20140106_1_new.astype('uint8'))\n",
    "outputImage_new = DDT.colorMapImage(\n",
    "    (cropimageArray20140106_1_new * 255).astype('uint8'), projectedMap_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp = getLargestComponent(projectedMap)\n",
    "imgConnectCut = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1).astype('uint8'),\n",
    "    largestcomp)\n",
    "largestcomp_new = getLargestComponent(projectedMap_new)\n",
    "imgConnectCut_new = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    largestcomp_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于进行着色的测试\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = cropimageArray20140106_1_new\n",
    "project_map = projectedMap_new\n",
    "mode = cv2.COLORMAP_JET\n",
    "output_imgs = np.zeros((imgs.shape[0], imgs.shape[2], imgs.shape[3], 3),\n",
    "                       dtype=np.uint8)\n",
    "masks = np.zeros((imgs.shape[0], imgs.shape[2], imgs.shape[3], 3),\n",
    "                 dtype=np.uint8)\n",
    "for i in range(imgs.shape[0]):\n",
    "    # img = cv2.resize(cv2.imread(os.path.join('./data', name)), (224, 224)) #读取为BGR格式\n",
    "    # 将project_map repeat为(3,H,W)再转置为(H,W,3)\n",
    "    # 这里的mask类似于自己的IndicatorMat\n",
    "    mask = np.tile(project_map[i], reps=(3, 1, 1)).transpose(1, 2, 0)\n",
    "    mask = cv2.cvtColor(cv2.applyColorMap(mask.astype(np.uint8), mode),\n",
    "                        cv2.COLOR_BGR2RGB)\n",
    "    masks[i] = mask\n",
    "    # addWeighted接受的两个数组应当为同样形状：HWC，因此将imgs[i]转为HWC形状\n",
    "    img = np.tile(imgs[i] * 255, (3, 1, 1)).transpose(1, 2, 0).astype('uint8')\n",
    "    output_img = cv2.addWeighted(img, 0.5, mask, 0.5, 0.0)\n",
    "    output_imgs[i] = output_img\n",
    "plt.imshow(output_imgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组会绘图\n",
    "ttle = [\n",
    "    'original image', 'original project map', 'mask image', 'Lenet(224) image',\n",
    "    'Lenet(224) project map', 'Lenet(224) mask image'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(\n",
    "    cropimageArray20140106_1.astype('uint8'))[6:9],\n",
    "                      utils.NCHWtoNHWC(projectedMap)[6:9],\n",
    "                      outputImage[6:9],\n",
    "                      utils.NCHWtoNHWC(cropimageArray20140106_1_new)[6:9],\n",
    "                      utils.NCHWtoNHWC(projectedMap_new)[6:9],\n",
    "                      outputImage_new[6:9],\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧网络的project_map产生亮圆环的原因是旧网络的图片中间填充的灰色而不是黑色\n",
    "colormap = DDT.toColorMap(projectedMap)\n",
    "colormap_new = DDT.toColorMap(projectedMap_new)\n",
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1.astype('uint8')),\n",
    "    utils.NCHWtoNHWC(projectedMap), outputImage, colormap,\n",
    "    largestcomp[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "    utils.NCHWtoNHWC(projectedMap_new), outputImage_new, colormap_new,\n",
    "    largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原网络的效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1.squeeze()).astype('uint8')\n",
    "maskafter20140106_1, imggrabcut20140106_1 = cutWithMask(\n",
    "    stackcropimageArray20140106_1, projectedMap.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 2\n",
    "showGrabCutWithMask(cropimageArray20140106_1[picIndex].squeeze(),\n",
    "                    projectedMap.squeeze()[picIndex],\n",
    "                    maskafter20140106_1[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(224)效果测试(ToTensor reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !新网络读取的图片数据为[0,1]内，因此需要转换至[0,255]\n",
    "stackcropimageArray20140106_1_new = (utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_new.squeeze() * 255)).astype('uint8')\n",
    "maskafter20140106_1_new, imggrabcut20140106_1_new = cutWithMask(\n",
    "    stackcropimageArray20140106_1_new, projectedMap_new.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 9\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_new[picIndex].squeeze(),\n",
    "                    projectedMap_new.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_new[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(112)效果测试(ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropnet_112 = model.model_defination.LeNet5(size=112)\n",
    "cropnet_112.load_param(r'trainIssueLog\\2022_10_16_20_12_25_112\\parameters.pkl')\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_112 = utils.loadImageFolder(PathCME20140106_1, trans)\n",
    "projectedMap_112 = DDT.DDTThirdParty(cropimageArray20140106_1_112, cropnet_112)\n",
    "colormap_112 = DDT.toColorMap(projectedMap_112)\n",
    "outputImage_112 = DDT.colorMapImage(\n",
    "    (255 * cropimageArray20140106_1_112).astype('uint8'), projectedMap_112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp_112 = getLargestComponent(projectedMap_112)\n",
    "imgConnectCut_112 = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_112).astype('uint8'),\n",
    "    largestcomp_112)\n",
    "colormap_112 = DDT.toColorMap(projectedMap_112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "    utils.NCHWtoNHWC(projectedMap_new), outputImage_new, colormap_new,\n",
    "    largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_new,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112),\n",
    "    utils.NCHWtoNHWC(projectedMap_112), outputImage_112, colormap_112,\n",
    "    largestcomp_112[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1_112 = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_112.squeeze() * 255).astype('uint8')\n",
    "maskafter20140106_1_112, imggrabcut20140106_1_112 = cutWithMask(\n",
    "    stackcropimageArray20140106_1_112, projectedMap_112.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 10\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_112[picIndex].squeeze(),\n",
    "                    projectedMap_112.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_112[picIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5(112)效果测试(ToTensor BoxBlur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from kornia.filters import box_blur\n",
    "import torch\n",
    "\n",
    "cropnet_112B = model.model_defination.LeNet5(size=112)\n",
    "cropnet_112B.load_param(\n",
    "    r'trainIssueLog\\2022_10_17_01_08_17_112_blur\\parameters.pkl')\n",
    "trans = torchvision.transforms.Compose([divide255, utils.CenterCrop('NCHW')])\n",
    "cropimageArray20140106_1_112B = box_blur(\n",
    "    torch.from_numpy(utils.loadImageFolder(PathCME20140106_1, trans)),\n",
    "    (3, 3)).numpy()\n",
    "projectedMap_112B = DDT.DDTThirdParty(cropimageArray20140106_1_112B,\n",
    "                                      cropnet_112B)\n",
    "colormap_112B = DDT.toColorMap(projectedMap_112B)\n",
    "outputImage_112B = DDT.colorMapImage(\n",
    "    (255 * cropimageArray20140106_1_112B).astype('uint8'), projectedMap_112B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestcomp_112B = getLargestComponent(projectedMap_112B)\n",
    "imgConnectCut_112B = maskImage(\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_112B).astype('uint8'),\n",
    "    largestcomp_112B)\n",
    "colormap_112B = DDT.toColorMap(projectedMap_112B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.drawImageArrays(\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112),\n",
    "    utils.NCHWtoNHWC(projectedMap_112), outputImage_112, colormap_112,\n",
    "    largestcomp_112[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112,\n",
    "    utils.NCHWtoNHWC(cropimageArray20140106_1_112B),\n",
    "    utils.NCHWtoNHWC(projectedMap_112B), outputImage_112B, colormap_112B,\n",
    "    largestcomp_112B[:, None, :, :].transpose(0, 2, 3, 1), imgConnectCut_112B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackcropimageArray20140106_1_112B = utils.grayImageToRGB(\n",
    "    cropimageArray20140106_1_112B.squeeze() * 255).astype('uint8')\n",
    "maskafter20140106_1_112B, imggrabcut20140106_1_112B = cutWithMask(\n",
    "    stackcropimageArray20140106_1_112B, projectedMap_112B.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picIndex = 14\n",
    "showGrabCutWithMask(stackcropimageArray20140106_1_112B[picIndex].squeeze(),\n",
    "                    projectedMap_112B.squeeze()[picIndex],\n",
    "                    maskafter20140106_1_112B[picIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttle = [\n",
    "    'Origin pic', 'origin mask', 'origin cut', 'reduced mask', 'reduced cut',\n",
    "    '112 mask', '112 cut', '112B mask', '112B cut'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(cropimageArray20140106_1_new),\n",
    "                      outputImage,\n",
    "                      imggrabcut20140106_1,\n",
    "                      outputImage_new,\n",
    "                      imggrabcut20140106_1_new,\n",
    "                      outputImage_112,\n",
    "                      imggrabcut20140106_1_112,\n",
    "                      outputImage_112B,\n",
    "                      imggrabcut20140106_1_112B,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 边缘检测与二值化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对得到的projectMap利用Canny算子计算得到边缘，将值范围为[0,255]的projectMap变为值为0或1的二值图，以便得到CME区域的位置。\n",
    "\n",
    "Canny算子需要给定低阈值和高阈值，一般把高阈值定为低阈值的2或3倍。因此，需要给定低阈值。梯度大小小于低阈值的边缘将被直接放弃，梯度大小大于高阈值的边缘被保留，梯度大小在低阈值和高阈值之间的边缘，若它连接着高于高阈值的边缘则会保留。选择一个合适的阈值可以使得Canny算法效果最优。\n",
    "\n",
    "**在进行canny算子边缘检测之前，以最大连接分量作为mask，遮罩共定位图，可以排除一些无关的图片部分**\n",
    "\n",
    "otsu算法可以自适应地选择阈值，将共定位图转化为0/1二值图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "\n",
    "def cannyThreshold(\n",
    "    src: np.ndarray,\n",
    "    lowThreshold: float,\n",
    "    kernelSize: int,\n",
    "    ratio=3,\n",
    "    color: Tuple[int, int, int] = (255, 0, 0)\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对src使用Canny边缘检测算法\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src : np.ndarray,uint8\n",
    "        需要进行处理的原图像，形状为NHWC\n",
    "    lowThreshold : float\n",
    "        低阈值\n",
    "    kernelSize : int\n",
    "        滤波器核大小\n",
    "    ratio : int, optional\n",
    "        高阈值与低阈值之比, by default 3\n",
    "    color : Tuple[int,int,int], optional\n",
    "        对检测出的边缘所赋的颜色\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    detecetedEdges : np.ndarray\n",
    "        算法输出的边缘，形状为NHW\n",
    "    dst : np.ndarray\n",
    "        将原图上的边缘标明后的图像,形状为NHWC\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        当src不为Uint8时抛出\n",
    "    '''\n",
    "    if not np.issubsctype(src.dtype, np.uint8):\n",
    "        raise ValueError(\n",
    "            'Expected input src dtype shouble uint8 ,got {}'.format(src.dtype))\n",
    "    detectedEdges = np.zeros((src.shape[0], src.shape[1], src.shape[2]),\n",
    "                             dtype=src.dtype)\n",
    "    dst = np.zeros_like(src, dtype=src.dtype)\n",
    "    for i in range(src.shape[0]):\n",
    "        srcGray = cv2.cvtColor(src[i], cv2.COLOR_RGB2GRAY)\n",
    "        imgBlur = cv2.blur(srcGray, (3, 3))\n",
    "        detectedEdges = cv2.Canny(imgBlur, lowThreshold, lowThreshold * ratio,\n",
    "                                  kernelSize)\n",
    "        mask = detectedEdges != 0\n",
    "        srciCopy = np.copy(src[i])\n",
    "        srciCopy[mask] = np.array(color)\n",
    "        dst[i] = srciCopy\n",
    "    return detectedEdges, dst\n",
    "\n",
    "\n",
    "from utils import applyMaskOnImage\n",
    "\n",
    "\n",
    "def otsuThreshold(img: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''对图片数组应用otsu二值化\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        需要进行二值化的数组，形状为NHW，形状为uint8\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thre : float\n",
    "        每一张图片分割的阈值\n",
    "    binarypic : np.ndarray\n",
    "        图片应用otsu法产生的阈值后产生的二值图，值为0或255\n",
    "    '''\n",
    "    thre = np.zeros((img.shape[0], ))\n",
    "    binarypic = np.zeros_like(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        # 以下threshold参数第三项不能为img[i].max()，而应该直接给定255，\n",
    "        # 因为对于某些数组，最大值若不为255，\n",
    "        # 二值化后大于阈值的元素将变为原本的最大值，而不是255\n",
    "        thre[i], binarypic[i] = cv2.threshold(\n",
    "            img[i], 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thre, binarypic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "from typing import Tuple\n",
    "\n",
    "max_lowThreshold = 300\n",
    "window_name = 'Edge Map'\n",
    "title_trackbar = 'Min Threshold:'\n",
    "ratio = 3\n",
    "kernel_size = 3\n",
    "\n",
    "\n",
    "def CannyThreshold(val):\n",
    "    low_threshold = val\n",
    "    img_blur = cv.blur(src_gray, (3, 3))\n",
    "    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold * ratio,\n",
    "                              kernel_size)\n",
    "    mask = detected_edges != 0\n",
    "    # dst = src * (mask[:,:,None].astype(src.dtype))\n",
    "    dst = np.copy(image)\n",
    "    dst[mask] = np.array([0, 0, 255])\n",
    "    cv.imshow(window_name, dst)\n",
    "    return detected_edges, dst[:, :, ::-1]\n",
    "\n",
    "\n",
    "args = edict()\n",
    "args.input = r'C:\\Programing\\CMEclassfication\\CME_data\\20111122\\20111122_212813_lasc2rdf_aia193rdf.png'\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Code for Canny Edge Detector tutorial.')\n",
    "# parser.add_argument('--input',\n",
    "#                     help='Path to input image.',\n",
    "#                     default='fruits.jpg')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# src = cv.imread(cv.samples.findFile(args.input))\n",
    "# if src is None:\n",
    "#     print('Could not open or find the image: ', args.input)\n",
    "#     exit(0)\n",
    "# src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "picindex = 5\n",
    "maskedprojectmap_new = np.concatenate(tuple(\n",
    "    cv2.bitwise_and(projectedMap_new[i, 0].astype('uint8'),\n",
    "                    projectedMap_new[i, 0].astype('uint8'),\n",
    "                    mask=largestcomp_new[i])[None, :, :]\n",
    "    for i in range(projectedMap_new.shape[0])),\n",
    "                                      axis=0)\n",
    "image = (cropimageArray20140106_1_new[picindex] * 255).astype('uint8')\n",
    "image = np.tile(image.transpose(1, 2, 0), reps=3)\n",
    "src = np.tile(maskedprojectmap_new[picindex].astype('uint8'),\n",
    "              reps=(3, 1, 1)).transpose(1, 2, 0)\n",
    "src_gray = maskedprojectmap_new[picindex].astype('uint8')\n",
    "cv2.namedWindow(window_name)\n",
    "cv2.createTrackbar(title_trackbar, window_name, 0, max_lowThreshold,\n",
    "                   CannyThreshold)\n",
    "edge, dst = CannyThreshold(65)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 7))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.imshow(dst)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.imshow(edge, cmap='gray')\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(src)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(projectedMap_new[picindex, 0].astype('uint8'), cmap='gray')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(colormap_new[picindex].astype('uint8'))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(largestcomp_new[picindex].astype('uint8'), cmap='gray')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(maskedprojectmap_new[picindex].astype('uint8'), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阈值的选择\n",
    "\n",
    "观察得到的CME共定位图，相关性最高的地方应该是CME的核心区域，在此处，图片的亮度最高。在明亮区域的四周分布着低亮度的区域。首先，可以尝试使用最大连接分量作为mask，去掉那些零散的部分。其次，从共定位图的低相关性部分到CME高相关性部分，在接近CME时，梯度应当有着巨大的改变。选取这样的一个梯度值，可以使得边缘检测分割效果最佳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图像直方图\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Code for Histogram Calculation tutorial.')\n",
    "# parser.add_argument('--input', help='Path to input image.', default='lena.jpg')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args = edict()\n",
    "# args.input = r'C:\\Users\\lenovo\\Pictures\\th5.jfif'\n",
    "# src = cv.imread(cv.samples.findFile(args.input))\n",
    "# if src is None:\n",
    "#     print('Could not open or find the image:', args.input)\n",
    "#     exit(0)\n",
    "\n",
    "src = maskedprojectmap_new[picindex].astype('uint8')\n",
    "histSize = 10\n",
    "histRange = (0, 256)  # the upper boundary is exclusive\n",
    "accumulate = False\n",
    "hist = cv.calcHist([src], [0],\n",
    "                   None, [histSize],\n",
    "                   histRange,\n",
    "                   accumulate=accumulate)\n",
    "hist_w = 512\n",
    "hist_h = 400\n",
    "bin_w = int(round(hist_w / histSize))\n",
    "histImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n",
    "cv.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n",
    "for i in range(1, histSize):\n",
    "    cv.line(histImage, (bin_w * (i - 1), hist_h - int(hist[i - 1])),\n",
    "            (bin_w * (i), hist_h - int(hist[i])), (255, 0, 0),\n",
    "            thickness=2)\n",
    "cv.imshow('Source image', src)\n",
    "cv.imshow('calcHist Demo', histImage)\n",
    "cv.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图通过梯度直方图来找到一个合适的阈值，以便分割图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制梯度直方图\n",
    "src = maskedprojectmap_new[picindex].astype('uint8')\n",
    "# src = projectedMap[picindex].astype('uint8').squeeze()\n",
    "gx = cv.Sobel(src, cv.CV_32F, 1, 0, ksize=1)\n",
    "gy = cv.Sobel(src, cv.CV_32F, 0, 1, ksize=1)\n",
    "abs_gx = cv.convertScaleAbs(gx)\n",
    "abs_gy = cv.convertScaleAbs(gy)\n",
    "grad = cv.addWeighted(abs_gx, 0.5, abs_gy, 0.5, 0)\n",
    "\n",
    "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "histSize = 10\n",
    "histRange = (0, 256)  # the upper boundary is exclusive\n",
    "accumulate = False\n",
    "hist = cv.calcHist([mag], [0],\n",
    "                   None, [histSize],\n",
    "                   histRange,\n",
    "                   accumulate=accumulate)\n",
    "hist_w = 512\n",
    "hist_h = 400\n",
    "bin_w = int(round(hist_w / histSize))\n",
    "histImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n",
    "cv.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n",
    "for i in range(1, histSize):\n",
    "    cv.line(histImage, (bin_w * (i - 1), hist_h - int(hist[i - 1])),\n",
    "            (bin_w * (i), hist_h - int(hist[i])), (255, 0, 0),\n",
    "            thickness=2)\n",
    "# cv.imshow('Source image', src)\n",
    "# cv.imshow('Gradient hist', histImage)\n",
    "# cv.waitKey()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(histImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试使用otsu算法直接分割共定位图和梯度强度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otsu算法直接作用于梯度强度图和共定位图的效果\n",
    "gradthreshold, gradbinarypic = cv2.threshold(\n",
    "    grad, 0, grad.max(), cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "srcthreshold, srcbinarypic = cv2.threshold(src, 0, 1,\n",
    "                                           cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "srcbinarypic = srcbinarypic.astype('uint8')\n",
    "imgthreshold = maskImage(\n",
    "    utils.NCHWtoNHWC(\n",
    "        255 *\n",
    "        cropimageArray20140106_1_new[picindex][None, :, :, :]).astype('uint8'),\n",
    "    srcbinarypic[None, :, :])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(src, cmap='gray')\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(srcbinarypic, cmap='gray')  # otsu作用于projectMap\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(gradbinarypic, cmap='gray')  # otsu作用于梯度强度图\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(imgthreshold[0])  # 将projectmap的otsu结果遮罩到原图上\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(dst)  # 使用对projectMap使用canny算子，再遮罩到原图上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(src, cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mag > 0.25 * mag.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histnp, binnp = np.histogram(mag, bins=20, range=(0, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试otsuThreshold函数\n",
    "maskedprojectedMap_new = applyMaskOnImage(\n",
    "    projectedMap_new.squeeze(),  #共定位图使用最大连接分量遮罩后\n",
    "    largestcomp_new)\n",
    "thre_new, otsuprojectMap_new = otsuThreshold(  # 直接对共定位图使用otsu算法\n",
    "    projectedMap_new.squeeze().astype('uint8'))\n",
    "maskedthre_new, otsumaskedprojectedMap_new = otsuThreshold(  # 对遮罩的共定位图使用otsu算法\n",
    "    maskedprojectedMap_new.squeeze().astype('uint8'))\n",
    "otsuCut = maskImage(  # 共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    (otsuprojectMap_new / 255).astype('uint8'))\n",
    "maskedotsuCut = maskImage(  # 遮罩后的共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    (otsumaskedprojectedMap_new / 255).astype('uint8'))\n",
    "ttle = [\n",
    "    'projectedMap_new', 'largestcomp_new', 'otsuprojectMap_new',\n",
    "    'outputImage_new', 'otsuCut', 'maskedprojectedMap_new',\n",
    "    'otsumaskedprojectedMap_new', 'maskedotsuCut'\n",
    "]\n",
    "utils.drawImageArrays(utils.NCHWtoNHWC(projectedMap_new),\n",
    "                      largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "                      otsuprojectMap_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "                      outputImage_new,\n",
    "                      otsuCut,\n",
    "                      maskedprojectedMap_new[:,\n",
    "                                             None, :, :].transpose(0, 2, 3, 1),\n",
    "                      otsumaskedprojectedMap_new[:, None, :, :].transpose(\n",
    "                          0, 2, 3, 1),\n",
    "                      maskedotsuCut,\n",
    "                      title=ttle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 物理参数提取测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论文中的方法\n",
    "1. 将得到的CME区域遮罩转变为极坐标形式 warpToPolar\n",
    "2. 得到区域遮罩极坐标形势下每一列的尖端位置 getTipLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换极坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = outputImage_new[5]\n",
    "\n",
    "h, w = img.shape[:2]  # 图片的高度和宽度\n",
    "cx, cy = 258, 243  # 以图像中心点作为变换中心\n",
    "maxR = max(cx, cy)  # 最大变换半径\n",
    "\n",
    "# img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "# imgPolar = cv2.linearPolar(img, (258,243), maxR, cv2.INTER_LINEAR)\n",
    "imgPolar = cv2.warpPolar(img, (360, 360), (258, 243), 254,\n",
    "                         cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPR = cv2.rotate(imgPolar, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(131), plt.imshow(img), plt.title(\"Original\")\n",
    "plt.subplot(132), plt.imshow(imgPR), plt.title(\"PolarTrans\")\n",
    "plt.show()\n",
    "\n",
    "img = largestcomp_new[5]\n",
    "# imgPolar = cv2.warpPolar(img, (512, 512), (258, 243), 254,\n",
    "#                          cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPolar = cv2.warpPolar(img, (360, 360), (258, 243), 240,\n",
    "                         cv2.INTER_LINEAR + cv2.WARP_POLAR_LINEAR)\n",
    "imgPR = cv2.rotate(imgPolar, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(131), plt.imshow(img), plt.title(\"Original\")\n",
    "plt.subplot(132), plt.imshow(imgPR), plt.title(\"PolarTrans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "极坐标转换，获取尖端位置，过滤尖端函数定义\n",
    "\n",
    "warpCartToPolar,getTipLocation,filterTip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "def warpCartToPolar(img: np.ndarray, dsize: tuple, center: tuple,\n",
    "                    maxRadius: float) -> np.ndarray:\n",
    "    '''将数组转换为极坐标形式\n",
    "\n",
    "    极坐标的极轴朝向东侧，也就是图片右侧，而不是正北。\\n\n",
    "    坐标变换会改变图像像素值分布\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        需要转换的数组，形状为NHW或NHWC\n",
    "    dsize : tuple\n",
    "        输出的图像大小\n",
    "    center : tuple\n",
    "        转换中心\n",
    "    maxRadius : float\n",
    "        最大半径，决定转换的最大范围\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        转换后的图片数组，类型同输入图像数组img相同\n",
    "    '''\n",
    "    if img.ndim != 3 and img.ndim != 4:\n",
    "        raise ValueError('Input image shape must be 3 or 4 ,got {}'.format(\n",
    "            img.ndim))\n",
    "    if img.ndim == 3:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1]),\n",
    "                              dtype=img.dtype)\n",
    "    else:  # 如果img是NHW，则imgInPolar形状为(N,dsize.h,dsize.w,C)\n",
    "        imgInPolar = np.zeros((img.shape[0], dsize[0], dsize[1], img.shape[3]),\n",
    "                              dtype=img.dtype)\n",
    "    for i in range(img.shape[0]):\n",
    "        imgPol = cv2.warpPolar(img[i], dsize, center, maxRadius,\n",
    "                               cv2.INTER_NEAREST + cv2.WARP_POLAR_LINEAR)\n",
    "        imgPol = cv2.rotate(imgPol, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        imgInPolar[i] = imgPol\n",
    "    return imgInPolar\n",
    "\n",
    "\n",
    "def getTipLocation(imgInPolar: np.ndarray, fromBottom=True) -> np.ndarray:\n",
    "    '''获取极坐标图像中的尖端位置\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgInPolar : np.ndarray\n",
    "        极坐标图像，形状为NHW，值为0或1，类型为uint8\n",
    "    fromBottom : bool\n",
    "        若为True，尖端位置从图像底部开始计数，若为False，则从图像顶部开始计数\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        尖端位置，形状为NW，当fromBottom为True时，表示第N张图片中横坐标为W的那一列中的尖端距离图像顶端的距离，\n",
    "        若为False，表示尖端距离图像底部的距离。\n",
    "        \n",
    "        若值为np.nan则表示该列不存在CME遮罩。\n",
    "        \n",
    "        存在距离顶端和距离底部两种距离的原因在于，\n",
    "        将图像数据用ndarray表示时，ndarray的索引原点在左上角，\n",
    "        但对于图像来说，原点在左下角。\\n\n",
    "        *****数组中存在nan，可能会导致意料之外的问题*****\n",
    "    '''\n",
    "    if imgInPolar.ndim != 3 and imgInPolar.ndim != 4:\n",
    "        raise ValueError('Input image shape must be 3 or 4 ,got {}'.format(\n",
    "            imgInPolar.ndim))\n",
    "    if not np.issubsctype(imgInPolar.dtype, np.uint8):\n",
    "        raise ValueError('Input array dtype must be np.uint8 , got {}'.format(\n",
    "            imgInPolar.dtype))\n",
    "    if not imgInPolar.max() == 1:\n",
    "        raise ValueError(\n",
    "            'Input array must be a 0/1 binary array , got input array max value {}'\n",
    "            .format(imgInPolar.max()))\n",
    "    # tipLocation的类型之前选择为uint8，导致超过256的数字会溢出\n",
    "    tipLocation = np.zeros((imgInPolar.shape[0], imgInPolar.shape[2]))\n",
    "    for i in range(imgInPolar.shape[0]):\n",
    "        for j in range(imgInPolar.shape[2]):\n",
    "            # 需要获得极坐标图像中CME每一个位置角上CME遮罩的最大高度\n",
    "            # 由于ndarray数组是以左上角为原点，两个维度为HW\n",
    "            # 因此需要求在W维度上(每一列)值为1的那些元素的纵坐标的最小值\n",
    "            if np.argwhere(imgInPolar[i][:, j] == 1).size == 0:\n",
    "                tipLocation[i, j] = np.nan\n",
    "            else:\n",
    "                if fromBottom:\n",
    "                    tipLocation[i, j] = imgInPolar.shape[1] - np.argwhere(\n",
    "                        imgInPolar[i][:, j] == 1).min() - 1\n",
    "                else:\n",
    "                    tipLocation[i,\n",
    "                                j] = np.argwhere(imgInPolar[i][:,\n",
    "                                                               j] == 1).min()\n",
    "    return tipLocation\n",
    "\n",
    "\n",
    "def filterTip(tip: np.ndarray, height: float) -> np.ndarray:\n",
    "    '''对包含尖端位置的数组进行过滤\n",
    "\n",
    "    过滤条件：\n",
    "    - 最大高度低于height的方位角将被抛弃\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tip : np.ndarray\n",
    "        包含尖端位置的数组\n",
    "    height : float\n",
    "        尖端位置低于height的位置角将被移除\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        经过过滤的数组\n",
    "    '''\n",
    "    tip = np.copy(tip)\n",
    "    tip[:, np.nanmax(tip, axis=0) < 180] = np.nan\n",
    "    return tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "binaryMap = otsumaskedprojectedMap_new\n",
    "binaryMap_polar = warpCartToPolar(binaryMap, (360, 360), (258, 243), 245)\n",
    "tipb = getTipLocation((binaryMap_polar / 255).astype('uint8'))\n",
    "tipu = getTipLocation((binaryMap_polar / 255).astype('uint8'), False)\n",
    "filtedtipb = filterTip(\n",
    "    tipb, binaryMap_polar.shape[1])  # binaryMap_polar.shape[1]即图像高度\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(binaryMap[ind])\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(binaryMap_polar[ind])\n",
    "tipImage = np.zeros_like(binaryMap_polar)\n",
    "for i in range(tipu.shape[0]):\n",
    "    for j in range(tipu.shape[1]):\n",
    "        if not np.isnan(tipu[i, j]):\n",
    "            tipImage[i, tipu[i, j].astype('uint'), j] = 1\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "# plt.plot(np.arange(512), 512-np.nan_to_num(tipu[ind]))\n",
    "plt.plot(np.arange(tipb.shape[1]), np.nan_to_num(tipb[ind]))\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "plt.plot(np.arange(tipu.shape[1]),\n",
    "         binaryMap_polar.shape[1] - np.nan_to_num(tipu[ind]))\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.xlim((0, 360))\n",
    "plt.ylim((0, 360))\n",
    "plt.plot(np.arange(tipb.shape[1]), np.nan_to_num(filtedtipb[ind]))\n",
    "# plt.imshow(tipImage[ind])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(binaryMap_polar[ind])\n",
    "img[:, np.nanmax(tipb, axis=0) < 180] = 0\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算中央位置角"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内部定义函数的修改\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "\n",
    "def findContinusNumbers(numList: List[int]) -> List[tuple]:\n",
    "    '''寻找列表中连续的数字段\n",
    "    \n",
    "    返回一个由元组构成的列表，每一个元组都是一段连续数字的起始数字和结束数字\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    numList : List[int]\n",
    "        连续的数字序列\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[tuple]\n",
    "        元组构成的列表，每一个元组都是一组连续数字的起始数字和结束数字\n",
    "    '''\n",
    "    numList.sort()\n",
    "    ind = 1\n",
    "    findList = []\n",
    "    while ind <= len(numList) - 1:\n",
    "        if numList[ind] - numList[ind - 1] == 1:\n",
    "            flag = ind - 1\n",
    "            while numList[ind] - numList[ind - 1] == 1:\n",
    "                ind += 1\n",
    "                if ind > len(numList) - 1:\n",
    "                    break\n",
    "            findList.append((numList[flag], numList[ind - 1]))\n",
    "        else:\n",
    "            ind += 1\n",
    "    return findList\n",
    "\n",
    "\n",
    "def calcCPA(tip: np.ndarray):\n",
    "    startEndPosition = []\n",
    "    for i in range(tip.shape[0]):\n",
    "        effectivePosition = np.argwhere(\n",
    "            ~np.isnan(filtedtipb[i])).squeeze().tolist()\n",
    "        currentStartEnd = findContinusNumbers(effectivePosition)\n",
    "        startEndPosition.append(currentStartEnd)\n",
    "    return startEndPosition\n",
    "\n",
    "\n",
    "a = calcCPA(filtedtipb)\n",
    "print(list(map(lambda x: len(x), a)))\n",
    "a[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算方位角\n",
    "effectivePA = np.argwhere(\n",
    "    ~np.isnan(filtedtipb[ind])).squeeze().tolist()  # tip不为nan的位置角\n",
    "\n",
    "\n",
    "def pixelToDegree(PAinPixel, width):\n",
    "    kangle = 360 / width\n",
    "    return kangle * PAinPixel % 360\n",
    "\n",
    "\n",
    "startEndPA = []\n",
    "start = effectivePA[0]\n",
    "end = effectivePA[1]\n",
    "lastValue = effectivePA[0]\n",
    "index = 1\n",
    "while index <= len(effectivePA) - 1:\n",
    "    if effectivePA[index] - effectivePA[index - 1] == 1:\n",
    "        flag = index - 1\n",
    "        while effectivePA[index] - effectivePA[index - 1] == 1:\n",
    "            index += 1\n",
    "            if index > len(effectivePA) - 1:\n",
    "                break\n",
    "        startEndPA.append([effectivePA[flag], effectivePA[index - 1]])\n",
    "    else:\n",
    "        index += 1\n",
    "\n",
    "if startEndPA[0][0] == 0 and startEndPA[-1][1] == 359:\n",
    "    startEndPA[-1][1] += startEndPA[0][1]\n",
    "    startEndPA\n",
    "startEndPA.pop(0)\n",
    "CPAs = []\n",
    "for PAs in startEndPA:\n",
    "    CPAs.append(\n",
    "        pixelToDegree(sum(PAs) / len(PAs), width=binaryMap_polar.shape[1]))\n",
    "CPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(binaryMap_polar[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试cv2 contours系列函数\n",
    "考虑到只得到CME尖端区域的坐标来计算CME物理参数会缺失很多信息，造成困难，因此尝试使用cv2 contours轮廓线系列函数，使用完整的CME轮廓信息来完成跟踪。\n",
    "\n",
    "预期步骤：\n",
    "1. 获取每一帧的CME区域\n",
    "2. 计算与前一帧所有的CME区域两两之间的形状相似度\n",
    "3. 根据计算得到的形状相似度进行分配，获知两帧之间哪些CME区域是相同的，哪些区域是新的，哪些区域消失了，并给每一个CME区域分配ID号用以标识\n",
    "4. 计算速度、角速度、角宽度等物理量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试使用cv2 contours系列函数\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind]).astype('uint8'), cv2.RETR_EXTERNAL, \n",
    "    cv2.CHAIN_APPROX_NONE)  # contours是个元组，每一个元素都是一个array，包含了轮廓点的x,y坐标，坐标系原点在左上角\n",
    "plt.imshow(cv2.drawContours(\n",
    "    cv2.merge((np.copy(binaryMap_polar[ind]), np.copy(binaryMap_polar[ind]),\n",
    "               np.copy(binaryMap_polar[ind]))),\n",
    "    contours,\n",
    "    contourIdx=0,\n",
    "    color=(255, 0, 0),\n",
    "    thickness=1,\n",
    "),\n",
    "           cmap='gray')\n",
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看contours的寻找是否正确\n",
    "img = np.zeros((360, 360))\n",
    "for i in contours[0].squeeze():\n",
    "    img[i[1], i[0]] = 1\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用模板匹配方法\n",
    "试图获得CME区域的边缘框，再利用cv2的模板匹配来获取两个帧之间CME区域的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得边缘框来选择CME区域\n",
    "Gcentera, Gcenterb, w, h = cv2.boundingRect(contours[0].squeeze())\n",
    "plt.imshow(\n",
    "    cv2.rectangle(\n",
    "        cv2.merge((binaryMap_polar[ind], binaryMap_polar[ind],\n",
    "                   binaryMap_polar[ind])), (Gcentera, Gcenterb),\n",
    "        (Gcentera + w, Gcenterb + h), (255, 255, 255), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行模板匹配\n",
    "imgtempl = binaryMap_polar[ind][Gcenterb:Gcenterb + h, Gcentera:Gcentera + w]\n",
    "plt.imshow(imgtempl, cmap='gray')\n",
    "# imgMatched = cv2.matchTemplate(binaryMap[ind],\n",
    "#                                imgtempl[y:y + h, x:x + w],\n",
    "#                                cv2.TM_CCOEFF_NORMED)\n",
    "imgMatched = cv2.matchTemplate(\n",
    "    binaryMap_polar[ind], binaryMap_polar[ind][Gcenterb:Gcenterb + h,\n",
    "                                               Gcentera:Gcentera + w],\n",
    "    cv2.TM_CCOEFF_NORMED)\n",
    "minval, maxval, minloc, maxloc = cv2.minMaxLoc(imgMatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgMatched, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    cv2.circle(\n",
    "        cv2.merge((binaryMap_polar[ind], binaryMap_polar[ind],\n",
    "                   binaryMap_polar[ind])),\n",
    "        (int(maxloc[0] + imgtempl.shape[1] / 2),\n",
    "         int(maxloc[1] + imgtempl.shape[0] / 2)), 20, (0, 0, 255), 2, 8, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用形状相似度方法\n",
    "1. 使用cv2.matchShape函数获取形状相似度\n",
    "2. 利用linear_sum_assignment将两帧中的CME区域进行匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取CME区域轮廓线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\"\n",
    "\n",
    "ind = 0\n",
    "contoursNext, hierarchyNext = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind + 1]).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_NONE)\n",
    "contoursNext = sorted(list(contoursNext),\n",
    "                      key=lambda x: x.shape[0],\n",
    "                      reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "print('ContoursNext:', list(map(lambda x: len(x), contoursNext)))\n",
    "contoursNumThre = contoursNext[0].shape[0] / 10\n",
    "contoursNext = list(\n",
    "    filter(lambda x: x.shape[0] > contoursNumThre,\n",
    "           contoursNext))  # 过滤掉点数小于最大轮廓线点数1/10的轮廓线\n",
    "\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    np.copy(binaryMap_polar[ind]).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_NONE)\n",
    "contours = sorted(list(contours), key=lambda x: x.shape[0],\n",
    "                  reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "print('Contours:', list(map(lambda x: len(x), contours)))\n",
    "contoursNumThre = contours[0].shape[0] / 10\n",
    "contours = list(filter(lambda x: x.shape[0] > contoursNumThre,\n",
    "                       contours))  # 过滤掉点数小于最大轮廓线点数1/10的轮廓线\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'Next Frame {ind+1}:total {len(contoursNext)} contours')\n",
    "imgNext = cv2.merge(\n",
    "    (np.copy(binaryMap_polar[ind + 1]), np.copy(binaryMap_polar[ind + 1]),\n",
    "     np.copy(binaryMap_polar[ind + 1])))\n",
    "for i in range(len(contoursNext)):\n",
    "    # orgx, orgy = contoursNext[i].squeeze(\n",
    "    # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "    orgx, orgy = int(contoursNext[i].squeeze().mean(axis=0)[0]), int(\n",
    "        contoursNext[i].squeeze().mean(axis=0)[1])\n",
    "    cv2.putText(imgNext,\n",
    "                str(i),\n",
    "                org=(orgx, orgy),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(14, 173, 238),\n",
    "                thickness=2)\n",
    "    cv2.drawContours(\n",
    "        imgNext,\n",
    "        contoursNext,\n",
    "        contourIdx=i,\n",
    "        color=(127, 127, 127),\n",
    "        thickness=1,\n",
    "    )\n",
    "plt.imshow(imgNext)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'Curr Frame {ind} :total {len(contours)} contours')\n",
    "img = cv2.merge((np.copy(binaryMap_polar[ind]), np.copy(binaryMap_polar[ind]),\n",
    "                 np.copy(binaryMap_polar[ind])))\n",
    "for i in range(len(contours)):\n",
    "    # orgx, orgy = contoursNext[i].squeeze(\n",
    "    # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "    orgx, orgy = int(contours[i].squeeze().mean(axis=0)[0]), int(\n",
    "        contours[i].squeeze().mean(axis=0)[1])\n",
    "    cv2.putText(img,\n",
    "                str(i),\n",
    "                org=(orgx, orgy),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1,\n",
    "                color=(14, 173, 238),\n",
    "                thickness=2)\n",
    "    cv2.drawContours(\n",
    "        img,\n",
    "        contours,\n",
    "        contourIdx=i,\n",
    "        color=(127, 127, 127),\n",
    "        thickness=1,\n",
    "    )\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同一片CME区域在前后帧上的形态应当接近，同时所在的位置应当接近，面积也应当相近。因此CostMat不光需要有形状相似项，还应当有位置相似项和面积相似项。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算轮廓线距离函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchPosition(cntA: np.ndarray, cntB: np.ndarray) -> float:\n",
    "    '''计算两个轮廓线之间的距离\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cnaA : np.ndarray\n",
    "        轮廓线A\n",
    "    cntB : np.array\n",
    "        轮廓线B\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        两个轮廓线之间的距离\n",
    "    '''\n",
    "    ma = cv2.moments(cntA)\n",
    "    mb = cv2.moments(cntB)\n",
    "    Gcentera = np.array(\n",
    "        [int(ma['m10'] / ma['m00']),\n",
    "         int(ma['m01'] / ma['m00'])])\n",
    "    Gcenterb = np.array(\n",
    "        [int(mb['m10'] / mb['m00']),\n",
    "         int(mb['m01'] / mb['m00'])])\n",
    "    # 余弦距离\n",
    "    # return np.dot(Gcentera, Gcenterb) / (np.linalg.norm(Gcentera) * np.linalg.norm(Gcenterb))\n",
    "    # 加权欧几里得距离\n",
    "    # X = np.vstack([Gcentera, Gcenterb])\n",
    "    # sigma = np.var(X, axis=0, ddof=1)\n",
    "    # return np.sqrt(((Gcentera - Gcenterb)**2 / sigma).sum())\n",
    "    # 兰氏距离\n",
    "    d = 0\n",
    "    for i in range(len(Gcentera)):\n",
    "        if Gcentera[i] == 0 and Gcenterb[i] == 0:\n",
    "            d += 0\n",
    "        else:\n",
    "            d += abs(Gcentera[i] - Gcenterb[i]) / (abs(Gcentera[i]) +\n",
    "                                                   abs(Gcenterb[i]))\n",
    "    return d\n",
    "\n",
    "\n",
    "def matchArea(cntA: np.ndarray, cntB: np.ndarray) -> float:\n",
    "    '''计算轮廓线之间面积的差异\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    cntB : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        两个轮廓线面积的差值绝对数除以面积之和\n",
    "    '''\n",
    "    areaA = cv2.contourArea(cntA)\n",
    "    areaB = cv2.contourArea(cntB)\n",
    "    return abs(areaA - areaB) / (areaA + areaB)\n",
    "\n",
    "\n",
    "def matchContour(cntA: np.ndarray,\n",
    "                 cntB: np.ndarray,\n",
    "                 alpha: float = 1,\n",
    "                 beta: float = 1,\n",
    "                 gamma: float = 1) -> float:\n",
    "    '''加权计算两个轮廓线之间的相似度，包括形状相似度，位置相似度，面积相似度。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    cntB : np.ndarray\n",
    "        需要计算的轮廓线\n",
    "    alpha : float, optional\n",
    "        形状相似度系数, by default 1\n",
    "    beta : float, optional\n",
    "        位置相似度系数, by default 1\n",
    "    gamma : float, optional\n",
    "        面积相似度系数, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        综合考虑形态、位置、面积相似度的结果\n",
    "    '''\n",
    "    a = alpha * cv2.matchShapes(cntA, cntB, cv2.CONTOURS_MATCH_I3,\n",
    "                                0)  # 计算两帧之间不同CME区域的形态相似度\n",
    "    b = beta * matchPosition(cntA, cntB)  # 计算位置相似度\n",
    "    c = gamma * matchArea(cntA, cntB)  # 计算面积相似度\n",
    "    return a + b + c\n",
    "\n",
    "\n",
    "def measureSimilarity(contoursA: List[np.ndarray],\n",
    "                      contoursB: List[np.ndarray],\n",
    "                      alpha: float = 1,\n",
    "                      beta: float = 1,\n",
    "                      gamma: float = 1) -> np.ndarray:\n",
    "    '''两两成对计算列表中的轮廓线的相似度，返回一个相似度矩阵\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cntA : List[np.ndarray]\n",
    "        包含轮廓线的列表\n",
    "    cntB : List[np.ndarray]\n",
    "        包含轮廓线的列表\n",
    "    alpha : float, optional\n",
    "        形状相似度系数, by default 1\n",
    "    beta : float, optional\n",
    "        位置相似度系数, by default 1\n",
    "    gamma : float, optional\n",
    "        面积相似度系数, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        两个轮廓线列表中的轮廓线成对计算的相似度矩阵，形状为len(contoursA), len(contoursB)\n",
    "    '''\n",
    "    similar = np.zeros((len(contoursA), len(contoursB)))\n",
    "    for i in range(len(contoursA)):\n",
    "        for j in range(len(contoursB)):\n",
    "            similar[i, j] = matchContour(contoursA[i], contoursB[j], alpha,\n",
    "                                         beta, gamma)\n",
    "    return similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour和Frame类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contour:\n",
    "    '''每一个轮廓线所属的类\n",
    "    '''\n",
    "    def __init__(self, contourArray: np.ndarray, binaryProjectMap: np.ndarray,\n",
    "                 index):\n",
    "        '''表示轮廓线的类\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        contourArray : np.ndarray\n",
    "            构成轮廓线的点所组成的数组，形状为(N,2)\n",
    "        binaryProjectMap : np.ndarray\n",
    "            该轮廓线来源的二值共定位图\n",
    "        index: int\n",
    "            该轮廓线在二值共定位图所有的轮廓线中的索引\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        array : np.ndarray\n",
    "            轮廓线的点构成的数组\n",
    "        binaryProjectMap : np.ndarray\n",
    "            该轮廓线所属的二值共定位图\n",
    "        size  : int\n",
    "            轮廓线点的个数\n",
    "        index : int\n",
    "            该轮廓线在二值共定位图所有的轮廓线中的索引\n",
    "        '''\n",
    "        self.array: np.ndarray = contourArray\n",
    "        self.binaryProjectMap = binaryProjectMap\n",
    "        self.size = self.array.shape[0]\n",
    "        self.index = index\n",
    "\n",
    "    def center(self):\n",
    "        return self.array.mean(axis=0).astype('uint')\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmtstr = f'{self.__class__.__name__}(index:{index},Points:{self.size})'\n",
    "        return fmtstr\n",
    "\n",
    "    def toArray(self):\n",
    "        return self.array\n",
    "\n",
    "    def getDistanceTo(self, cnt, alpha=1, beta=1, gamma=1):\n",
    "        return matchContour(self.array, cnt.array, alpha, beta, gamma)\n",
    "\n",
    "    def drawContour(self, color=(255, 127, 127)) -> np.ndarray:\n",
    "        '''在图上画出轮廓线，返回图像数组\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        color : tuple, optional\n",
    "            画出轮廓线的颜色，为RGB元组, by default (255, 127, 127)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            表示图像的数组\n",
    "        '''\n",
    "        img = cv2.merge((self.binaryProjectMap, self.binaryProjectMap,\n",
    "                         self.binaryProjectMap))\n",
    "        cv2.drawContours(\n",
    "            img,\n",
    "            (self.array, ),\n",
    "            contourIdx=0,\n",
    "            color=color,\n",
    "            thickness=cv2.FILLED,\n",
    "        )\n",
    "        return img\n",
    "\n",
    "\n",
    "class Frame:\n",
    "    '''每一个二值图像所属的类，可以生成、管理多个轮廓线Contour对象\n",
    "    '''\n",
    "    def __init__(self, binaryProjectMap: np.ndarray, frameInd: int):\n",
    "        '''初始化一个Frame类\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMap : np.ndarray\n",
    "            二值化的共定位图，形状为HW\n",
    "        frameInd : int\n",
    "            帧序号，表明该帧在原数组中的序号\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        contourList : List[Contour]\n",
    "            轮廓线对象构成的列表\n",
    "        binaryProjectMap : np.ndarry\n",
    "            用以生成轮廓线的二值化的共定位图，形状为HW\n",
    "        size : int\n",
    "            Frame对象所包含的轮廓线的个数\n",
    "        frameInd : int\n",
    "            帧序号，表明该帧在原数组中的序号\n",
    "        '''\n",
    "        self.contourList = self.makeContours(binaryProjectMap)\n",
    "        self.binaryProjectMap = binaryProjectMap\n",
    "        self.size = len(self.contourList)\n",
    "        self.frameInd = frameInd\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        fmtstr = f'{self.__class__.__name__}(Ind {self.frameInd},Contour size:'\n",
    "        for cnt in self.contourList:\n",
    "            fmtstr += str(cnt.size) + ','\n",
    "        fmtstr += ')'\n",
    "        return fmtstr\n",
    "\n",
    "    def __getitem__(self, key) -> Contour:\n",
    "        return self.contourList[key]\n",
    "\n",
    "    def toArray(self):\n",
    "        arrs = []\n",
    "        for cnt in self.contourList:\n",
    "            arrs.append(cnt.array)\n",
    "\n",
    "    @staticmethod\n",
    "    def makeContours(binaryProjectMap: np.ndarray) -> List[Contour]:\n",
    "        '''由二值化的共定位图中得到轮廓线，再进行排序，过滤等步骤生成Contour的列表\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMap : np.ndarray\n",
    "            二值化的共定位图，形状为HW\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Contour]\n",
    "            包含有Contour对象的列表\n",
    "        '''\n",
    "        contours, hierarchy = cv2.findContours(\n",
    "            np.copy(binaryProjectMap).astype('uint8'), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_NONE)\n",
    "        contours = sorted(list(contours),\n",
    "                          key=lambda x: x.shape[0],\n",
    "                          reverse=True)  # 按照轮廓线点的个数进行排序\n",
    "        contoursPointThre = int(contours[0].shape[0] /\n",
    "                                5)  # 过滤阈值为最大contour点数的1/5\n",
    "        contours = list(\n",
    "            filter(lambda x: x.shape[0] > contoursPointThre,\n",
    "                   contours))  # 过滤掉点数小于contoursPointThre的轮廓线\n",
    "        contourList: List[Contour] = []\n",
    "        for i in range(len(contours)):\n",
    "            contourList.append(\n",
    "                Contour(contours[i].squeeze(), binaryProjectMap, i))\n",
    "        return contourList\n",
    "\n",
    "    def linearAssign(self,\n",
    "                     frm,\n",
    "                     alpha: float = 1,\n",
    "                     beta: float = 1,\n",
    "                     gamma: float = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        '''与另一个Frame对象的所有轮廓线进行匹配，解决线性和分配问题\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frm : Frame\n",
    "            另一个包含轮廓线的帧对象\n",
    "        alpha : float, optional\n",
    "            形状相似度系数, by default 1\n",
    "        beta : float, optional\n",
    "            位置相似度系数, by default 1\n",
    "        gamma : float, optional\n",
    "            面积相似度系数, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            给出最佳匹配结果的元组，第一个是行序号构成的数组，第二个是列序号构成的数组\n",
    "        '''\n",
    "        cost = self.getDistanceTo(frm, alpha, beta, gamma)\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        rowInd, colInd = linear_sum_assignment(cost)\n",
    "        return rowInd, colInd\n",
    "\n",
    "    def getDistanceTo(self,\n",
    "                      frm,\n",
    "                      alpha: float = 1,\n",
    "                      beta: float = 1,\n",
    "                      gamma: float = 1) -> np.ndarray:\n",
    "        '''与另一个Frame对象的所有轮廓线两两计算相似度\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frm : Frame\n",
    "            另一个包含轮廓线的帧对象\n",
    "        alpha : float, optional\n",
    "            形状相似度系数, by default 1\n",
    "        beta : float, optional\n",
    "            位置相似度系数, by default 1\n",
    "        gamma : float, optional\n",
    "            面积相似度系数, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            两个Frame对象所包含的轮廓线两两之间的相似度，形状为(self.size, frm.size)\n",
    "        '''\n",
    "        cost = np.zeros((self.size, frm.size))\n",
    "        for i in range(self.size):\n",
    "            for j in range(frm.size):\n",
    "                cost[i, j] = matchContour(self.contourList[i].array,\n",
    "                                          frm.contourList[j].array, alpha,\n",
    "                                          beta, gamma)\n",
    "        return cost\n",
    "\n",
    "    def drawFrame(self, color=(255, 127, 127)) -> np.ndarray:\n",
    "        '''返回一个数组，可用于绘制出该Frame对象中所有的轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        color : tuple, optional\n",
    "            绘制轮廓线的颜色, by default (255, 127, 127)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            可用于直接成图的数组\n",
    "        '''\n",
    "        img = cv2.merge(\n",
    "            (np.copy(self.binaryProjectMap), np.copy(self.binaryProjectMap),\n",
    "             np.copy(self.binaryProjectMap)))\n",
    "        for i in range(len(self.contourList)):\n",
    "            # orgx, orgy = contoursNext[i].squeeze(\n",
    "            # )[0][0], contoursNext[i].squeeze()[0][1]\n",
    "            orgx, orgy = int(self.contourList[i].center()[0]), int(\n",
    "                self.contourList[i].center()[1])\n",
    "            cv2.putText(img,\n",
    "                        str(i),\n",
    "                        org=(orgx, orgy),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=1,\n",
    "                        color=(14, 173, 238),\n",
    "                        thickness=2)\n",
    "            cv2.drawContours(\n",
    "                img,\n",
    "                self.toArray(),\n",
    "                contourIdx=i,\n",
    "                color=color,\n",
    "                thickness=1,\n",
    "            )\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用改进的距离度量方法\n",
    "import pandas as pd\n",
    "\n",
    "CostMat = measureSimilarity(contoursNext, contours)\n",
    "\n",
    "indicies = [f'Next {i}' for i in range(len(contoursNext))]\n",
    "cols = [f'Curr {i}' for i in range(len(contours))]\n",
    "df = pd.DataFrame(CostMat, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(CostMat, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(CostMat)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))\n",
    "\n",
    "# 找出未配对成功的track和detection\n",
    "trackIndicies = np.arange(len(contoursNext))\n",
    "detectionIndicies = np.arange(len(contours))\n",
    "rowInd, colInd = linear_sum_assignment(CostMat)\n",
    "matches, unmatchedTracks, unmatchedDetections = [], [], []\n",
    "for col, detectionIdx in enumerate(detectionIndicies):\n",
    "    if col not in colInd:\n",
    "        unmatchedDetections.append(detectionIdx)\n",
    "for row, trackIdx in enumerate(trackIndicies):\n",
    "    if row not in rowInd:\n",
    "        unmatchedTracks.append(trackIdx)\n",
    "for row, col in zip(rowInd, colInd):\n",
    "    trackIdx = trackIndicies[row]\n",
    "    detectionIdx = detectionIndicies[col]\n",
    "    if CostMat[row, col] > 2:\n",
    "        unmatchedTracks.append(trackIdx)\n",
    "        unmatchedDetections.append(detectionIdx)\n",
    "    else:\n",
    "        matches.append((trackIdx, detectionIdx))\n",
    "print('matches', matches)\n",
    "print('unmatchedTracks', unmatchedTracks)\n",
    "print('unmatchedDetections', unmatchedDetections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用原本的形态相似度量方法\n",
    "import pandas as pd\n",
    "\n",
    "CostMat = np.zeros((len(contoursNext), len(contours)))\n",
    "for i in range(len(contoursNext)):\n",
    "    for j in range(len(contours)):\n",
    "        CostMat[i, j] = cv2.matchShapes(contoursNext[i], contours[j],\n",
    "                                        cv2.CONTOURS_MATCH_I3,\n",
    "                                        0)  # 计算两帧之间不同CME区域的形态相似度\n",
    "# CostMat = measureSimilarity(contoursNext, contours)\n",
    "\n",
    "indicies = [f'Next {i}' for i in range(len(contoursNext))]\n",
    "cols = [f'Curr {i}' for i in range(len(contours))]\n",
    "df = pd.DataFrame(CostMat, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(CostMat, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(CostMat)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame类和Contour类测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "fnext = Frame(np.copy(binaryMap_polar[ind + 1]).astype('uint8'), ind + 1)\n",
    "f = Frame(np.copy(binaryMap_polar[ind]).astype('uint8'), ind)\n",
    "print(list(map(lambda x: x.size, fnext.contourList)))\n",
    "print(list(map(lambda x: x.size, f.contourList)))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f'Next Frame {ind+1} :total {fnext.size} contours')\n",
    "plt.imshow(fnext.drawFrame())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f'Curr Frame {ind} :total {f.size} contours')\n",
    "plt.imshow(f.drawFrame())\n",
    "row, col = fnext.linearAssign(f)\n",
    "for i, j in zip(row, col):\n",
    "    print('Next {} assigned to curr {}'.format(i, j))\n",
    "\n",
    "dis = fnext.getDistanceTo(f)\n",
    "indicies = [f'Next {i}' for i in range(fnext.size)]\n",
    "cols = [f'Curr {i}' for i in range(f.size)]\n",
    "df = pd.DataFrame(dis, index=indicies, columns=cols)\n",
    "df['minLoc'] = np.argmin(dis, axis=1)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轮廓线追踪类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class TrackState(Enum):\n",
    "    confirmed = 1\n",
    "    deleted = 2\n",
    "\n",
    "\n",
    "class Track:\n",
    "    '''表示一个已经被追踪到的连续变化的CME Contour轮廓线\n",
    "    '''\n",
    "    def __init__(self, ID: int, frameIndex: int, contour: Contour,\n",
    "                 maxAge: int) -> None:\n",
    "        '''表示一个已经被追踪到的连续变化的CME Contour轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ID : int\n",
    "            每一个Track独一无二的身份号\n",
    "        frameIndex : int\n",
    "            初始化Track时所使用的轮廓线出现的帧号\n",
    "        contour : Contour\n",
    "            轮廓线\n",
    "        maxAge : int\n",
    "            表示该Track从上次更新轮廓线到现在所允许的最大的帧数，\n",
    "            超过该数字应当标记其state为deleted\n",
    "\n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        ID : int\n",
    "            追踪的身份号，每一个Track的身份号都应该是独一无二的\n",
    "        age : int\n",
    "            表示该Track从第一次出现到现在所经历的帧数\n",
    "        _maxAge : int:\n",
    "            表示该Track从上次更新轮廓线到现在所允许的最大的帧数，\n",
    "            超过该数字应当标记其state为deleted\n",
    "        contourApperenceInd : List[int]\n",
    "            contour中的轮廓线所出现的frame索引\n",
    "        contour : List[Contour]\n",
    "            追踪到的轮廓线\n",
    "        timeSinceLastUpdate : int\n",
    "            自上次更新轮廓线到现在所经历的帧数\n",
    "        state : TrackState\n",
    "            Track的状态，delete或者confirm\n",
    "        '''\n",
    "        self.ID: int = ID\n",
    "        self.age: int = 1\n",
    "        self._maxAge = maxAge\n",
    "        self.contourApperenceInd: List[int] = [frameIndex]\n",
    "        self.contour: List[Contour] = [contour]\n",
    "        self.timeSinceLastUpdate = 0\n",
    "        self.state: TrackState = TrackState.confirmed\n",
    "\n",
    "    def update(self, newContourInd: int, newContour: Contour) -> None:\n",
    "        '''增加新的被追踪到的轮廓线\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        newContourInd : int\n",
    "            新的轮廓线的出现的所在帧号\n",
    "        newContour : Contour\n",
    "            新的轮廓线\n",
    "        '''\n",
    "        self.contour.append(newContour)\n",
    "        self.contourApperenceInd.append(newContourInd)\n",
    "        self.timeSinceLastUpdate = 0\n",
    "        self.age += 1\n",
    "\n",
    "    def markMissed(self) -> None:\n",
    "        self.timeSinceLastUpdate += 1\n",
    "        if self.timeSinceLastUpdate > self._maxAge:\n",
    "            self.state = TrackState.deleted\n",
    "\n",
    "    def isDeleted(self) -> bool:\n",
    "        return self.state == TrackState.deleted\n",
    "\n",
    "    def isConfirm(self) -> bool:\n",
    "        return self.state == TrackState.confirmed\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        fmtstr = f'{self.__class__.__name__}(ID:{self.ID},Age:{self.age},State:{self.state.name},{len(self.contour)} contours,Appear in{self.contourApperenceInd}'\n",
    "        return fmtstr\n",
    "\n",
    "    def getRecordsNum(self):\n",
    "        return len(self.contour)\n",
    "\n",
    "    def drawTrack(self, cols: int = 5, title: Optional[str] = None) -> None:\n",
    "        from math import ceil\n",
    "        contourNums = len(self.contour)\n",
    "        rows = ceil(contourNums / cols)\n",
    "        plt.figure(figsize=(cols * 5, rows * 5))\n",
    "        if not title:\n",
    "            plt.suptitle(title)\n",
    "        else:\n",
    "            plt.suptitle(f'Track ID {self.ID}')\n",
    "        ind = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ax = plt.subplot(rows, cols, ind + 1)\n",
    "                ax.set_title(\n",
    "                    f'Frame {self.contourApperenceInd[ind]} Contour Ind {self.contour[ind].index}'\n",
    "                )\n",
    "                plt.imshow(self.contour[ind].drawContour())\n",
    "                ind += 1\n",
    "                if ind >= contourNums:\n",
    "                    break\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    '''完成CME区域跟踪类\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 binaryProjectMaps: np.ndarray,\n",
    "                 maxDistance: float,\n",
    "                 maxAge: int = 0):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        binaryProjectMaps : np.ndarray\n",
    "            用于进行CME区域跟踪的二值化共定位图，形状为NHW\n",
    "        maxDistance : float\n",
    "            所能允许的轮廓线之间的最大距离，超过该距离将被认为不匹配\n",
    "        maxAge : int, optional\n",
    "            track状态设置为deleted前所能容许最大track不更新的帧数, by default 0\n",
    "            \n",
    "        Attribute:\n",
    "        ----------------------\n",
    "        binaryProjectMaps : np.ndarray\n",
    "            一次CME事件完整的二值化共定位图 形状为NHW\n",
    "        maxDistance : float\n",
    "            两个轮廓线之间相似度的最大允许值，若相似度大于此值则认为不匹配\n",
    "        tracks: List[Track]\n",
    "            跟踪到的Track构成的列表\n",
    "        maxAge : int\n",
    "            若track在超过maxAge个帧后仍未被更新，则会标记为丢失\n",
    "        '''\n",
    "        self.binaryProjectMaps: np.ndarray = binaryProjectMaps\n",
    "        self.maxDistance: float = maxDistance\n",
    "        self.tracks: List[Track] = []\n",
    "        self._nextID = 0\n",
    "        self.maxAge: int = maxAge\n",
    "\n",
    "    def _initiateTrack(self, frameIndex: int, contour: Contour) -> None:\n",
    "        '''利用Contour初始化一个Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frameIndex : int\n",
    "            Contour所在帧的编号\n",
    "        contour : Contour\n",
    "            新的轮廓线\n",
    "        '''\n",
    "        self.tracks.append(\n",
    "            Track(self._nextID, frameIndex, contour, self.maxAge))\n",
    "        self._nextID += 1\n",
    "\n",
    "    def _match(\n",
    "        self, nextFrame: Frame\n",
    "    ) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:\n",
    "        '''与新的一帧进行匹配，并返回匹配的tracks、未匹配的tracks、未匹配的detections的编号\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nextFrame : Frame\n",
    "            表示下一张图片的帧对象\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[Tuple[int, int]], List[int], List[int]]\n",
    "            返回一个包含以下三项的元组\n",
    "            - 包含匹配成功的track和detection的索引的列表\n",
    "            - 包含未匹配成功的tracks的索引列表\n",
    "            - 包含未匹配成功的detection的索引列表\n",
    "        '''\n",
    "        # 只有state为Track.Confirmed的track才能用于匹配\n",
    "        # 将被用于匹配的tracks的索引，将cost矩阵中的索引映射到在self.track中的索引\n",
    "        confirmedTrackIndicies = [\n",
    "            i for i, t in enumerate(self.tracks) if t.isConfirm()\n",
    "        ]\n",
    "        confirmedDetectionIndicies = np.arange(\n",
    "            nextFrame.size)  # 将被用于匹配的detections的索引（所有的detection都将被用于匹配）\n",
    "        cost = np.zeros((len(confirmedTrackIndicies),\n",
    "                         nextFrame.size))  #每一行代表track，每一列代表detection\n",
    "        for i in range(len(confirmedTrackIndicies)):\n",
    "            for j in range(nextFrame.size):\n",
    "                # 使用tracks中最新的那一个contour来计算相似度\n",
    "                cost[i, j] = self.tracks[\n",
    "                    confirmedTrackIndicies[i]].contour[-1].getDistanceTo(\n",
    "                        nextFrame[j])\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        rowInd, colInd = linear_sum_assignment(cost)\n",
    "        # 从rowInd和ColInd中找出匹配成功和未被匹配的track和detection\n",
    "        matches, unmatchedTracks, unmatchedDetections = [], [], []\n",
    "        # * 由于实际用于匹配的tracks并不是所有的tracks\n",
    "        # * 因此cost中的某一个track在cost中的行索引(rowInd)并不等于在self.tracks中的索引(trackIdx)\n",
    "        # * 因此若col不在colInd中，则把它对应的trackIdx加入到未匹配的track中\n",
    "        for col, detectionIdx in enumerate(confirmedDetectionIndicies):\n",
    "            if col not in colInd:\n",
    "                unmatchedDetections.append(detectionIdx)\n",
    "        for row, trackIdx in enumerate(confirmedTrackIndicies):\n",
    "            if row not in rowInd:\n",
    "                unmatchedTracks.append(trackIdx)\n",
    "        for row, col in zip(rowInd, colInd):\n",
    "            trackIdx = confirmedTrackIndicies[row]\n",
    "            detectionIdx = confirmedDetectionIndicies[col]\n",
    "            if cost[row, col] > self.maxDistance:\n",
    "                unmatchedTracks.append(trackIdx)\n",
    "                unmatchedDetections.append(detectionIdx)\n",
    "            else:\n",
    "                matches.append((trackIdx, detectionIdx))\n",
    "        return matches, unmatchedTracks, unmatchedDetections\n",
    "\n",
    "    def update(self, nextFrame: Frame) -> None:\n",
    "        '''将现有的track列表推进更新至下一帧\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nextFrame : Frame\n",
    "            代表下一帧图像的Frame类\n",
    "        '''\n",
    "        if not self.tracks:  # 如果目前没有任何活动跟踪，则将从nextFrame新建\n",
    "            for cnt in nextFrame.contourList:\n",
    "                self._initiateTrack(nextFrame.frameInd, cnt)\n",
    "        else:\n",
    "            matches, unmatchedTracks, unmatchedDetections = self._match(\n",
    "                nextFrame)  # 用当前的track匹配下一帧中的轮廓线\n",
    "            for trackIdx, detectionIdx in matches:\n",
    "                # 对于匹配成功的track，更新状态\n",
    "                self.tracks[trackIdx].update(nextFrame.frameInd,\n",
    "                                             nextFrame[detectionIdx])\n",
    "            for trackIdx in unmatchedTracks:\n",
    "                # 对于匹配不成功的track，标记为丢失\n",
    "                self.tracks[trackIdx].markMissed()\n",
    "            for detectionIdx in unmatchedDetections:\n",
    "                # 对于未被匹配的轮廓线，则从直接新建Track\n",
    "                self._initiateTrack(nextFrame.frameInd,\n",
    "                                    nextFrame[detectionIdx])\n",
    "\n",
    "    def getTrackByID(self, ID: int) -> Track:\n",
    "        '''返回给定ID号的Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ID : int\n",
    "            ID号\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[Track]\n",
    "            符合给定ID号的Track\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            若没有track符合给定ID号则抛出异常\n",
    "        '''\n",
    "        track = None\n",
    "        for t in self.tracks:\n",
    "            if t.ID == ID:\n",
    "                return t\n",
    "        else:\n",
    "            raise ValueError('No tracks match given ID {ID}')\n",
    "\n",
    "    def getTrackByIndex(self, index: int) -> Track:\n",
    "        '''返回对应于tracks列表中索引的Track\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            索引，对应于tracks列表中的索引\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Track\n",
    "            在tracks列表中索引为index的Track\n",
    "        '''\n",
    "        return self.tracks[index]\n",
    "\n",
    "    def drawImg(self, cols=5) -> None:\n",
    "        '''绘制出tracker追踪的轮廓线所属的二值图像\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : int, optional\n",
    "            绘制的图像中每一列子图个数, by default 5\n",
    "        '''\n",
    "        from math import ceil\n",
    "        nums = self.binaryProjectMaps.shape[0]\n",
    "        rows = ceil(nums / cols)\n",
    "        plt.figure(figsize=(cols * 5, rows * 5))\n",
    "        ind = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ax = plt.subplot(rows, cols, ind + 1)\n",
    "                ax.set_title(f'Frame {ind}')\n",
    "                # plt.imshow(self.binaryPeojectMaps[ind], cmap='gray')\n",
    "                plt.imshow(Frame(self.binaryProjectMaps[ind], ind).drawFrame())\n",
    "                ind += 1\n",
    "                if ind >= nums:\n",
    "                    break\n",
    "\n",
    "    def run(self, isSort: bool = True) -> None:\n",
    "        for i, binartProjectMap in enumerate(self.binaryProjectMaps):\n",
    "            nextFrm = Frame(binartProjectMap, i)\n",
    "            self.update(nextFrm)\n",
    "        if isSort:\n",
    "            self.tracks.sort(key=lambda x: x.getRecordsNum(), reverse=True)\n",
    "\n",
    "\n",
    "tracker = Tracker(binaryMap_polar, 2.2, 0)\n",
    "tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.drawImg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.getTrackByIndex(2).drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.getTrackByIndex(9).drawTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "frame = [Frame(binaryMap_polar[i], i) for i in range(binaryMap_polar.shape[0])]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(frame[5].drawFrame())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(frame[6].drawFrame())\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(frame[6][0].drawContour())\n",
    "# pd.DataFrame(frame[1].getDistanceTo(frame[2]))\n",
    "# plt.imshow(frame[0][0].show())\n",
    "linear_sum_assignment(frame[5].getDistanceTo(frame[6]))\n",
    "frame[5].getDistanceTo(frame[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.drawImageArrays(otsuprojectMap_new[:, :, :, None],\n",
    "#                       otsuprojectMap_new_polar[:, :, :, None])\n",
    "\n",
    "# utils.drawImageArrays(outputImage_new, otsuCut,\n",
    "#                       warpCartToPolar(otsuCut, (360, 360), (258, 243), 245),\n",
    "#                       otsuprojectMap_new_polar[:, :, :, None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对Lasco C3的尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import applyMaskOnImage\n",
    "from kornia.filters import box_blur\n",
    "\n",
    "PathCMEC3 = r'CME_data\\LascoC3\\20140602'\n",
    "\n",
    "trans = torchvision.transforms.Compose(\n",
    "    [divide255,\n",
    "     utils.CenterCrop('NCHW', circlePoint=(244, 256), radius=37)])\n",
    "cropimageArray_C3 = box_blur(\n",
    "    torch.from_numpy(utils.loadImageFolder(PathCMEC3, trans)), (3, 3)).numpy()\n",
    "cropnet_224B = model.model_defination.LeNet5(size=224)\n",
    "cropnet_224B.load_param(\n",
    "    r'trainIssueLog\\2022_10_21_21_13_13_224_blur\\parameters.pkl')\n",
    "projectedMapC3 = DDT.DDTThirdParty(cropimageArray_C3, cropnet_224B)\n",
    "colormapC3 = DDT.toColorMap(projectedMapC3)\n",
    "# 255*cropimageArray20140106_1_new.astype('uint8')=255*(cropimageArray20140106_1_new.astype('uint8'))\n",
    "outputImageC3 = DDT.colorMapImage((cropimageArray_C3 * 255).astype('uint8'),\n",
    "                                  projectedMapC3)\n",
    "largestcompC3 = getLargestComponent(projectedMapC3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskedprojectedmapC3 = applyMaskOnImage(projectedMapC3.squeeze(),\n",
    "                                        largestcompC3)\n",
    "outputImageC3 = DDT.colorMapImage((cropimageArray_C3 * 255).astype('uint8'),\n",
    "                                  projectedMapC3)\n",
    "maskedoutputImageC3 = DDT.colorMapImage(\n",
    "    (cropimageArray_C3 * 255).astype('uint8'), maskedprojectedmapC3)\n",
    "threC3, otsumaskedprojectedmapC3 = otsuThreshold(\n",
    "    maskedprojectedmapC3.squeeze().astype('uint8'))\n",
    "ttle = [\n",
    "    'Origin', 'LeNet224 OutputMap', 'LeNet224 ProjectMap Colored',\n",
    "    'LargestComp', 'LeNet224 OutputMap with Mask'\n",
    "]\n",
    "otsuCutC3 = maskImage(  # 共定位图使用otsu算法后的效果\n",
    "    utils.NCHWtoNHWC(255 * cropimageArray_C3).astype('uint8'),\n",
    "    otsumaskedprojectedmapC3.astype('uint8'))\n",
    "utils.drawImageArrays(\n",
    "    cropimageArray_C3.transpose(0, 2, 3, 1),\n",
    "    outputImageC3,\n",
    "    colormapC3,\n",
    "    largestcompC3[:, :, :, None],\n",
    "    maskedoutputImageC3,\n",
    "    #   title=ttle)\n",
    "    otsuCutC3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "maskedoutputImage_new = DDT.colorMapImage(  # 遮罩后的共定位图使用otsu算法后的效果\n",
    "    (255 * cropimageArray20140106_1_new).astype('uint8'),\n",
    "    maskedprojectedMap_new)\n",
    "concated = np.concatenate(\n",
    "    (np.tile(255 * cropimageArray20140106_1_new, reps=(1, 3, 1, 1)).transpose(\n",
    "        0, 2, 3, 1), maskedoutputImage_new),\n",
    "    axis=2)\n",
    "for i in range(cropimageArray20140106_1_new.shape[0]):\n",
    "    Image.fromarray((concated[i]).astype('uint8')).save(\n",
    "        r'C:\\Programing\\projectmap图像\\最大连接分量遮罩20140602{}.png'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df = pd.read_excel(\n",
    "    r'C:\\Programing\\CMEclassfication\\trainIssueLog\\2022_10_13_22_40_16\\epoch_info.xlsx'\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['epoch'],\n",
    "               y=df['test_acc'],\n",
    "               mode='lines+markers',\n",
    "               name='Test Accuracy'))\n",
    "annotations = []\n",
    "annotations.append(\n",
    "    dict(xref='paper',\n",
    "         x=0.955,\n",
    "         y=df['test_acc'].to_numpy()[-1],\n",
    "         xanchor='left',\n",
    "         yanchor='middle',\n",
    "         text='{}%'.format(\n",
    "             np.round(df['test_acc'].to_numpy()[-1], 4) * 100, 4),\n",
    "         font=dict(family='Arial', size=16),\n",
    "         showarrow=False))\n",
    "# annotations.append(\n",
    "#     dict(xref='paper',\n",
    "#          yref='paper',\n",
    "#          x=0.4,\n",
    "#          y=1.05,\n",
    "#          xanchor='left',\n",
    "#          yanchor='bottom',\n",
    "#          text='Test accuracy growth in Training',\n",
    "#          font=dict(family='Arial', size=30, color='rgb(37,37,37)'),\n",
    "#          showarrow=False))\n",
    "fig.update_layout(annotations=annotations)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.NCHWtoNHWC(maskedprojectedMap_new[:, None, :, :][ind]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = np.array([2, 4, 7])\n",
    "# utils.drawImageArrays(\n",
    "#     utils.NCHWtoNHWC(projectedMap_new),\n",
    "#     largestcomp_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "#     otsuprojectMap_new[:, None, :, :].transpose(0, 2, 3, 1), outputImage_new,\n",
    "#     otsuCut, maskedprojectedMap_new[:, None, :, :].transpose(0, 2, 3, 1),\n",
    "#     otsumaskedprojectedMap_new[:, None, :, :].transpose(0, 2, 3,\n",
    "#                                                         1), maskedotsuCut)\n",
    "\n",
    "# DDT.colorMapImage((cropimageArray20140106_1_new * 255).astype('uint8'),\n",
    "#                   maskedprojectedMap_new)\n",
    "# arrays = [\n",
    "#     utils.NCHWtoNHWC(cropimageArray20140106_1_new[ind]),\n",
    "#     utils.NCHWtoNHWC(maskedprojectedMap_new[:, None, :, :][ind]),\n",
    "#     DDT.colorMapImage((cropimageArray20140106_1_new * 255).astype('uint8'),\n",
    "#                       maskedprojectedMap_new)[ind], maskedotsuCut[ind]\n",
    "# ]\n",
    "ind = np.array([2, 5, 7])\n",
    "arrays = [\n",
    "    utils.NCHWtoNHWC(cropimageArray_C3[ind]),\n",
    "    utils.NCHWtoNHWC(maskedprojectedmapC3[:, None, :, :][ind]),\n",
    "    DDT.colorMapImage((cropimageArray_C3 * 255).astype('uint8'),\n",
    "                      maskedprojectedmapC3)[ind], otsuCutC3[ind]\n",
    "]\n",
    "column = len(arrays)\n",
    "row = arrays[0].shape[0]\n",
    "# utils.drawImageArrays(\n",
    "#     utils.NCHWtoNHWC(cropimageArray20140106_1_new[ind]),\n",
    "#     utils.NCHWtoNHWC(maskedprojectedMap_new[:, None, :, :][ind]),\n",
    "#     DDT.colorMapImage((cropimageArray20140106_1_new * 255).astype('uint8'),\n",
    "#                       maskedprojectedMap_new)[ind], maskedotsuCut[ind])\n",
    "ttle = ['Origin', 'Co-Localization', 'Colored', 'Otsu']\n",
    "plt.figure(figsize=(5 * column, 5 * row))\n",
    "for j in range(column):\n",
    "    for i in range(row):\n",
    "        plt.subplot(row, column, i * column + j + 1)\n",
    "        if i == 0:\n",
    "            plt.title(ttle[j], fontsize=15, color='black')\n",
    "        if arrays[j][i].shape[2] == 1:\n",
    "            plt.imshow(arrays[j][i], cmap='gray')  # 图片是灰度图的情况\n",
    "        elif arrays[j][i].shape[2] == 3:\n",
    "            plt.imshow(arrays[j][i])  # 图片是RGB的情况\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb6408f9acc94ef862c1000d2f7ed73b15d48594c9d0937a8093d9903abff190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
